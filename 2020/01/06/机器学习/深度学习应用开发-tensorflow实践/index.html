<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="第一章 深度学习简介及开发环境搭建">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习应用开发-tensorflow实践">
<meta property="og:url" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/index.html">
<meta property="og:site_name" content="移动城堡">
<meta property="og:description" content="第一章 深度学习简介及开发环境搭建">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter1/%E5%9B%BE1-1.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter1/%E5%9B%BE1-2.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter1/%E5%9B%BE1-3.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter1/%E5%9B%BE1-4.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter1/%E5%9B%BE1-5.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter1/%E5%9B%BE1-6.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter1/%E5%9B%BE1-7.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter1/%E5%9B%BE1-8.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter2/%E5%9B%BE2-1.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter2/%E5%9B%BE2-2.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter2/%E5%9B%BE2-3.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter2/%E5%9B%BE2-4.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter2/%E5%9B%BE2-5.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter3/%E5%9B%BE3-1.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter3/%E5%9B%BE3-2.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter3/%E5%9B%BE3-3.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter3/%E5%9B%BE3-4.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter3/%E5%9B%BE3-5.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter3/%E5%9B%BE3-6.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter3/%E5%9B%BE3-7.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter3/%E5%9B%BE3-8.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter3/%E5%9B%BE3-9.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter3/%E5%9B%BE3-10.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter4/%E5%9B%BE4-1.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter4/%E5%9B%BE4-2.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter4/%E5%9B%BE4-3.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter4/%E5%9B%BE4-4.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter4/%E5%9B%BE4-5.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter4/%E5%9B%BE4-6.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter4/%E5%9B%BE4-7.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter4/%E5%9B%BE4-8.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter4/%E5%9B%BE4-9.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter4/%E5%9B%BE4-10.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter4/%E5%9B%BE4-11.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter4/%E5%9B%BE4-12.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-1.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-2.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-3.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-4.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-5.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-6.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-7.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-8.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-9.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-10.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-11.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-12.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-13.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-14.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-15.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-16.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-17.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-18.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E8%A1%A5%E5%85%85.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-19.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-20.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter6/%E5%9B%BE6-1.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter6/%E5%9B%BE6-2.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter6/%E5%9B%BE6-3.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter6/%E5%9B%BE6-4.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter6/%E5%9B%BE6-5.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter6/%E5%9B%BE6-6.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter6/%E5%9B%BE6-7.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-1.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-2.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-3.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-4.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-5.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-6.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-7.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-8.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-9.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-10.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-11.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-12.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-13.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-14.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-15.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-16.png">
<meta property="og:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-17.png">
<meta property="article:published_time" content="2020-01-06T09:24:52.000Z">
<meta property="article:modified_time" content="2023-07-17T01:59:12.809Z">
<meta property="article:author" content="李澳">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="tensorflow">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter1/%E5%9B%BE1-1.png">

<link rel="canonical" href="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>深度学习应用开发-tensorflow实践 | 移动城堡</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">移动城堡</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">欢迎来到我的博客</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="李澳">
      <meta itemprop="description" content="自娱自乐">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="移动城堡">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习应用开发-tensorflow实践
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-01-06 17:24:52" itemprop="dateCreated datePublished" datetime="2020-01-06T17:24:52+08:00">2020-01-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-07-17 09:59:12" itemprop="dateModified" datetime="2023-07-17T09:59:12+08:00">2023-07-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>29k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1:38</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="第一章-深度学习简介及开发环境搭建"><a href="#第一章-深度学习简介及开发环境搭建" class="headerlink" title="第一章 深度学习简介及开发环境搭建"></a>第一章 深度学习简介及开发环境搭建</h1><span id="more"></span>

<h2 id="1-1-人工智能与机器学习"><a href="#1-1-人工智能与机器学习" class="headerlink" title="1.1 人工智能与机器学习"></a>1.1 人工智能与机器学习</h2><p><strong>人工智能</strong>是要让机器的行为看起来像人所表现出的智能行为一样.<br>人工智能包括<strong>计算智能</strong>、<strong>感知智能</strong>和<strong>认知智能</strong>等层次,目前人工智能还介于前两者之间.<br>目前人工智能所处的阶段还在**“弱人工智能”(Narrow AI)<strong>阶段,距离</strong>“强人工智能”(General AI)**还有较长的路要走.</p>
<p><strong>机器学习</strong>是人工智能的一个分支,它是实现人工智能的一个核心技术,即以机器学习为手段解决人工智能中的问题.<br><strong>机器学习</strong>是通过一些让计算机可以自动”学习”的算法并从数据中分析获得规律,然后利用规律对新样本进行预测.<br><strong>机器学习</strong>的形式化的描述:对于某类任务<strong>T</strong>和性能度量<strong>P</strong>,如果一个计算机程序在<strong>T</strong>上一<strong>P</strong>衡量的性能随着经验<strong>E</strong>而自我完善,那么就称这个计算机程序在从经验<strong>E</strong>学习.<br>其实机器学习的过程与人类学习很相似,如下图:<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter1/%E5%9B%BE1-1.png"></p>
<h2 id="1-2-机器学习分类与算法简介"><a href="#1-2-机器学习分类与算法简介" class="headerlink" title="1.2 机器学习分类与算法简介"></a>1.2 机器学习分类与算法简介</h2><p>机器学习可分为如下几类:</p>
<ul>
<li>有监督学习</li>
<li>无监督学习</li>
<li>半监督学习</li>
<li>强化学习</li>
</ul>
<p>下面对其一一进行介绍:<br><strong>有监督学习(supervised learning)</strong>:指的是事先需要准备好输入与正确的输出(区分方法)相配套的训练数据,让计算机进行学习,以便当它被输入某个数据时能够得到正确的输出(区分方法).</p>
<p>**无监督学习(unsupervised learning)**的目的是让计算机自己去学习怎样去做一些事情,所有数据只有特征而没有标记.<br><strong>无监督学习</strong>被运用于仅提供输入用数据、需要计算机自己找出数据内在结构的场合.其目的是让计算机从数据中抽取其中所包含的模式和规则.</p>
<p>二者的中间地带是<strong>半监督学习(semi-supervised learning)</strong>.<br>对于半监督学习,其训练数据一部分有标记,另一部分沒有标记,而沒有标记数据的数量常常极大于有标记数据的数量.<br>它的基本规律是:数据的分布必然不是完全随机的,通过结合有标记数据的局部特征,以及大量没标记数据的整体分布,可以得到比较好的分类结果.</p>
<p><strong>强化学习(reinforcement learning,RL)</strong>,是解决计算机从感知到决策控制的问题,从而实现通用人工智能.<br>强化学习是目标导向的,从白纸一张的状态开始,经由许多个步骤来实现某一维度上的目标最大化。最简单的理解是在训练过程中,不断去尝试,错误就惩罚,正确就奖励,由此训练得到的模型在各个状态环境中都最好.<br>对强化学习来说,它虽然没有标记,但有一个延迟奖励与训练相关,通过学习过程中的某种激励函数获得某种从状态到行动的映射.强化学习强调如何基于环境而行动,以取得最大化的预期利益.<br>强化学习一般应用于游戏、下棋等需要连续决策的领域.<br>无监督学习的典型应用模式有<strong>聚类</strong>.如下图:<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter1/%E5%9B%BE1-2.png"><br>有监督学习的典型应用模式有<strong>预测(针对连续数据)<strong>和</strong>分类(针对离散数据)</strong>,如下图:<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter1/%E5%9B%BE1-3.png"><br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter1/%E5%9B%BE1-4.png"></p>
<h2 id="1-3-神经网络与深度学习"><a href="#1-3-神经网络与深度学习" class="headerlink" title="1.3 神经网络与深度学习"></a>1.3 神经网络与深度学习</h2><p>要介绍神经网络,首先要介绍神经元,生物神经元与神经元的数学模型对比如下图:<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter1/%E5%9B%BE1-5.png"><br>从上图来看,神经元的数学模型算是比较简单的,其有多个带权的输入和一个偏置值,对其进行加权求和后,再进行激活函数的运算,最后输出.<br>神经网络是由多个神经元相互连接而成,由一些神经元的输出作为另一些神经元的输入,如下图:<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter1/%E5%9B%BE1-6.png"><br>神经网络可分为<strong>输入层</strong>、<strong>隐藏层</strong>和<strong>输出层</strong>,<strong>输入层</strong>是接受输入数据的层,最后输出结果的层叫做<strong>输出层</strong>,中间的神经元组成的中间层叫做<strong>隐藏层</strong>.<br>我们通常所说的几层的神经网络通常是指的<strong>隐藏层</strong>的数目,比如上面的神经网络即为单层神经网络.<br>在大多数情况下,在设计神经网络时,根据具体要解决的问题,<strong>输入层</strong>和<strong>输出层</strong>往往是固定的,中间的<strong>隐藏层</strong>是需要重点设计的.<br>上图所示的神经网络为<strong>全连接网络</strong>,即后面的节点与前面所有节点均相连.<br>上图中神经网络的计算如下:</p>
<blockquote>
<p>z<sub>1</sub> &#x3D; w<sub>11</sub>*x<sub>1</sub> + w<sub>12</sub>*x<sub>2</sub> + w<sub>13</sub>*x<sub>3</sub> + b<sub>1</sub><br>z<sub>2</sub> &#x3D; w<sub>21</sub>*x<sub>1</sub> + w<sub>22</sub>*x<sub>2</sub> + w<sub>23</sub>*x<sub>3</sub> + b<sub>2</sub><br>z<sub>2</sub> &#x3D; w<sub>21</sub>*x<sub>1</sub> + w<sub>22</sub>*x<sub>2</sub> + w<sub>33</sub>*x<sub>3</sub> + b<sub>3</sub><br>a<sub>1</sub> &#x3D; <em>f</em>(z<sub>1</sub>)<br>a<sub>2</sub> &#x3D; <em>f</em>(z<sub>2</sub>)<br>a<sub>3</sub> &#x3D; <em>f</em>(z<sub>3</sub>)</p>
</blockquote>
<p>上面公式中的b实际上就是偏置项,即图中最下方的标注着+1的节点.函数<em>f</em>(x)为激活函数,是非线性的.由激活函数得到一个<strong>介于0到1之间</strong>的<strong>激活值</strong>,以激活值作为神经元的输出.<br>下面介绍两种常见的激活函数:<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter1/%E5%9B%BE1-7.png"><br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter1/%E5%9B%BE1-8.png"></p>
<h2 id="1-4-Anaconda和TensorFlow开发环境搭建"><a href="#1-4-Anaconda和TensorFlow开发环境搭建" class="headerlink" title="1.4 Anaconda和TensorFlow开发环境搭建"></a>1.4 Anaconda和TensorFlow开发环境搭建</h2><p>环境的安装选择Anaconda,在这里不多进行介绍.<br>conda命令是Anaconda中管理包与环境的命令,tensorflow采用conda命令来安装,如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install tensorflow</span><br></pre></td></tr></table></figure>
<p>这个命令将默认安装tensorflow2.0,并且将其安装到默认的base环境中.<br>但是tensorflow2.0与tensorflow1.x是不兼容的,而本次学习使用的是tensorflow1.x,所以我又安装了tensorflow1.15.0,不过这个放在了另一个环境中,如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">conda env list	#列出所有环境</span><br><span class="line">conda create -n tensorflow1 python=3.7	#新建一个名为tensorflow1的环境,并且指明python版本为3.7</span><br><span class="line">conda activate tensorflow1	#激活环境</span><br><span class="line">conda search tensorflow	#在该环境下查找并显示所有tensorflow包的信息</span><br><span class="line">conda install tensorflow==1.15.0	#在该环境下安装1.15.0版本的tensorflow</span><br><span class="line">conda deactivate	#退出该环境</span><br></pre></td></tr></table></figure>
<p>下面再列出几个比较有用的conda命令:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">conda env remove -n env_name	#删除名为env_name的环境</span><br><span class="line">conda list -n env_name	#查看名为env_name的环境中的所有包,也可以直接在该环境中使用conda list命令</span><br><span class="line">conda remove -n env_name package_name	#删除名为env_name的环境中的名为package_name的包,也可以在该环境中直接使用conda remove package_name命令</span><br><span class="line">conda update conda	#更新conda</span><br><span class="line">conda update anaconda	#更新anaconda</span><br><span class="line">conda update package_name	#更新名为package_name的包</span><br><span class="line">conda update --all	#更新所有库</span><br></pre></td></tr></table></figure>

<h1 id="第二章-TensorFlow编程基础"><a href="#第二章-TensorFlow编程基础" class="headerlink" title="第二章 TensorFlow编程基础"></a>第二章 TensorFlow编程基础</h1><p>再次说明,下面所讲的都是基于TensorFlow1.x的,并不是TensorFlow2.0,下面正式开始.<br>先由一个Hello World示例开始:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个常数运算,将作为一个节点加入到默认计算图中</span></span><br><span class="line">hello = tf.constant(<span class="string">&quot;Hello,World&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个TF会话</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行并获得结果</span></span><br><span class="line"><span class="built_in">print</span>(sess.run(hello))</span><br></pre></td></tr></table></figure>
<p>输出为:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">b&#x27;Hello,World&#x27;</span></span><br></pre></td></tr></table></figure>
<p>输出前面的’b’表示Bytes literals(字节文字)</p>
<h2 id="2-1-TensorFlow的基本概念"><a href="#2-1-TensorFlow的基本概念" class="headerlink" title="2.1 TensorFlow的基本概念"></a>2.1 TensorFlow的基本概念</h2><h3 id="2-1-1-TensorFlow的计算模型-计算图"><a href="#2-1-1-TensorFlow的计算模型-计算图" class="headerlink" title="2.1.1 TensorFlow的计算模型:计算图"></a>2.1.1 TensorFlow的计算模型:计算图</h3><p>TensorFlow &#x3D; Tensor + Flow<br>Tensor	张量</p>
<ul>
<li>数据结构:多维数组</li>
</ul>
<p>Flow	流</p>
<ul>
<li>计算模型:张量之间通过计算而转换的过程</li>
</ul>
<p>TensorFlow是一个通过<strong>计算图</strong>的形式表述计算的编程系统,每一个计算都是计算图上的一个节点,节点之间的边描述了计算之间的关系.<br><strong>计算图</strong>是一种有向图,由以下内容构成:</p>
<ul>
<li>一组节点,每个<strong>节点</strong>都代表一个<strong>操作</strong>,是一种<strong>运算</strong>.</li>
<li>一组有向边,每条<strong>边</strong>代表节点之间的<strong>关系</strong>(<strong>数据传递</strong>和<strong>数据依赖</strong>.</li>
</ul>
<p>TensorFlow有两种边:</p>
<ul>
<li><strong>常规边(实线)</strong>:代表数据依赖关系.一个节点的运算输出成另一个节点的输入,两个节点之间有tensor流动**(值传递)**</li>
<li><strong>特殊边(虚线)</strong>:不携带值,表示两个节点之间的<strong>控制</strong>相关性.比如,happens-before关系,源节点必须在目的节点执行完前完成执行</li>
</ul>
<p>计算图的一个实例如下:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#一个简单计算图</span></span><br><span class="line">node1 = tf.constant(<span class="number">3.0</span>,tf.float32,name=<span class="string">&quot;node1&quot;</span>)</span><br><span class="line">node2 = tf.constant(<span class="number">4.0</span>,tf.float32,name=<span class="string">&quot;node2&quot;</span>)</span><br><span class="line">node3 = tf.add(node1,node2);</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(node3)</span><br></pre></td></tr></table></figure>
<p>输出为:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor(<span class="string">&quot;Add:0&quot;</span>, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>输出的结果并不是一个数字,而是一个张量的结构.<br>上述代码的计算图在TensorBoard中的图形如下图:<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter2/%E5%9B%BE2-1.png"><br>上例中输出node3并没有得到预想中的7.0,而是输出了一个张量的结构,这是因为<strong>创建静态图只是建立静态计算模型,执行会话才能提供数据并获得结果</strong>.如下:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建立对话并显示运行结果</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;运行sess.run(node1)的结果: &quot;</span>,sess.run(node1))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;运行sess.run(node3)的结果: &quot;</span>,sess.run(node3))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭会话</span></span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<p>运行结果为:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">运行sess.run(node1)的结果:  <span class="number">3.0</span></span><br><span class="line">运行sess.run(node3)的结果:  <span class="number">7.0</span></span><br></pre></td></tr></table></figure>
<p>在Python3中print函数是自动换行的,如果想要其不换行,应写成如下形式:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置分隔符参数 end</span></span><br><span class="line"><span class="built_in">print</span>(i, end = <span class="string">&#x27;&#x27;</span> )</span><br></pre></td></tr></table></figure>

<h3 id="2-1-2-张量"><a href="#2-1-2-张量" class="headerlink" title="2.1.2 张量"></a>2.1.2 张量</h3><p>在TensorFlow中,所有的数据都通过张量的形式来表示.上一节计算图的实例中所输出的就是张量.<br>从功能的角度,张量可以简单理解为多维数组:</p>
<ul>
<li><strong>零阶张量</strong>表示<strong>标量(scalar)</strong>,也就是<strong>一个数</strong></li>
<li><strong>一阶张量</strong>为<strong>向量(vector)</strong>,也就是<strong>一维数组</strong></li>
<li><strong>n阶张量</strong>可以理解为一个<strong>n维数组</strong></li>
</ul>
<p>张量并没有真正保存数字,它<strong>保存的是计算过程</strong>.<br>张量具有如下属性:<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter2/%E5%9B%BE2-2.png"><br>有三个术语描述张量的维度:<strong>阶</strong>(rank)、<strong>形状</strong>(shape)、<strong>维数</strong>(dimension number).如下:<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter2/%E5%9B%BE2-3.png"><br>上面的形状属性括号内的D的含义为当前维度的元素个数.<br>下面是张量形状的代码示例:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tens1 = tf.constant([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>]],</span><br><span class="line">                    [[<span class="number">3</span>,<span class="number">3</span>,<span class="number">6</span>],[<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>]],</span><br><span class="line">                    [[<span class="number">7</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">9</span>,<span class="number">1</span>,<span class="number">9</span>]],</span><br><span class="line">                    [[<span class="number">11</span>,<span class="number">12</span>,<span class="number">7</span>],[<span class="number">1</span>,<span class="number">3</span>,<span class="number">14</span>]]],name=<span class="string">&quot;tens1&quot;</span>)</span><br><span class="line"><span class="comment"># 语句中包含[],&#123;&#125;或()括号中间换行的就不需要使用多行连接符</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tens1)</span><br></pre></td></tr></table></figure>
<p>输出为:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor(<span class="string">&quot;tens1:0&quot;</span>, shape=(<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>), dtype=int32)</span><br></pre></td></tr></table></figure>
<p>该张量的形状为(4,2,3),即第一维(即最外层维度)有4个元素,第二维有两个元素,第三维(即最内层的维度)有3个元素.<br>下面是另一段代码:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scalar = tf.constant(<span class="number">100</span>)</span><br><span class="line">vector = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">matrix = tf.constant([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">cube_matrix = tf.constant([[[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]],[[<span class="number">4</span>],[<span class="number">5</span>],[<span class="number">6</span>]],[[<span class="number">7</span>],[<span class="number">8</span>],[<span class="number">9</span>]]])</span><br><span class="line"></span><br><span class="line"><span class="comment">#get_shape()函数,获取张量的形状</span></span><br><span class="line"><span class="built_in">print</span>(scalar.get_shape())</span><br><span class="line"><span class="built_in">print</span>(vector.get_shape())</span><br><span class="line"><span class="built_in">print</span>(matrix.get_shape())</span><br><span class="line"><span class="built_in">print</span>(cube_matrix.get_shape())</span><br></pre></td></tr></table></figure>
<p>运行结果:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">()</span><br><span class="line">(<span class="number">5</span>,)</span><br><span class="line">(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">(<span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>张量元素的获取也是非常简单的,下面对其进行介绍.<br>阶为1的张量等价于向量,通过**t[i]<strong>获取元素.<br>阶为2的张量等价于矩阵,通过</strong>t[i,j]<strong>获取元素.<br>阶为3的张量,通过</strong>t[i,j,k]**获取元素.<br>示例如下:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tens1 = tf.constant([[[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">3</span>]],[[<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>]]])</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="built_in">print</span>(sess.run(tens1)[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<p>运行结果为5.<br>需要注意的一点是访问的是张量的值,因此也要先运行.</p>
<p>TensorFlow支持14种不同的类型(下面只是列出了比较常用的一些,并未全部列出):</p>
<ul>
<li>浮点数: tf.float32、tf.float64</li>
<li>整数: tf.int8、tf.int16、tf.int32、tf.int64、tf.uint8</li>
<li>布尔: tf.bool</li>
<li>复数: tf.complex64、tf.complex128</li>
<li>字符串: tf.string</li>
</ul>
<p><strong>默认类型</strong>:不带小数点的数会被默认为int32,带小数点的数会被默认为float32<br><strong>TensorFlow会对参与运算的所有张量进行类型的检查,发现类型不匹配是时会报错</strong>.<br>注意!只要不是一个类型就会报错,哪怕是int32与int64之间进行运算也会报错.</p>
<h3 id="2-2-3-操作"><a href="#2-2-3-操作" class="headerlink" title="2.2.3 操作"></a>2.2.3 操作</h3><p>计算图中的<strong>节点</strong>就是<strong>操作</strong>(Operation)</p>
<ul>
<li>一次加法也是一个操作</li>
<li>一次乘法也是一个操作</li>
<li>构建一些变量的初始值也是一个操作</li>
</ul>
<p>每个运算操作都有<strong>属性</strong>,它在构建图的时候需要确定下来.<br>操作可以和计算<strong>设备绑定</strong>,指定操作在某个设备上执行.比如让CPU运行某些操作,让GPU运行另一些操作.<br>操作之间存在顺序关系,这些操作之间的的<strong>依赖</strong>就是**”边”**.<br>如果操作A的输入是操作B执行的结果,那么这个操作就依赖于操作B.<br>代码示例如下:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义变量a</span></span><br><span class="line">a = tf.Variable(<span class="number">1</span>,name=<span class="string">&quot;a&quot;</span>)</span><br><span class="line"><span class="comment"># 定义操作b为a+1</span></span><br><span class="line">b = tf.add(a,<span class="number">1</span>,name=<span class="string">&quot;b&quot;</span>)</span><br><span class="line"><span class="comment">#定义操作c为b*4</span></span><br><span class="line">c = tf.multiply(b,<span class="number">4</span>,name=<span class="string">&quot;c&quot;</span>)</span><br><span class="line"><span class="comment">#定义d为c-b</span></span><br><span class="line">d = tf.subtract(c,b,name=<span class="string">&quot;d&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>其在TensorBoard中的计算图如下:<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter2/%E5%9B%BE2-4.png"></p>
<p>TensorFlow中大多数运算符都进行了重载操作，使我们可以快速使用 (+ - * &#x2F;) 等，但是有一点不好的是使用重载操作符后就不能为每个操作命名了。下面一些TensorFlow中具体的运算方法：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 算术操作符：+ - * / % </span></span><br><span class="line">tf.add(x, y, name=<span class="literal">None</span>)        <span class="comment"># 加法(支持 broadcasting)</span></span><br><span class="line">tf.subtract(x, y, name=<span class="literal">None</span>)   <span class="comment"># 减法</span></span><br><span class="line">tf.multiply(x, y, name=<span class="literal">None</span>)   <span class="comment"># 乘法</span></span><br><span class="line">tf.divide(x, y, name=<span class="literal">None</span>)     <span class="comment"># 浮点除法, 返回浮点数(python3 除法)</span></span><br><span class="line">tf.mod(x, y, name=<span class="literal">None</span>)        <span class="comment"># 取余</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 幂指对数操作符：^ ^2 ^0.5 e^ ln </span></span><br><span class="line">tf.<span class="built_in">pow</span>(x, y, name=<span class="literal">None</span>)        <span class="comment"># 幂次方</span></span><br><span class="line">tf.square(x, name=<span class="literal">None</span>)        <span class="comment"># 平方</span></span><br><span class="line">tf.sqrt(x, name=<span class="literal">None</span>)          <span class="comment"># 开根号，必须传入浮点数或复数</span></span><br><span class="line">tf.exp(x, name=<span class="literal">None</span>)           <span class="comment"># 计算 e 的次方</span></span><br><span class="line">tf.log(x, name=<span class="literal">None</span>)           <span class="comment"># 以 e 为底，必须传入浮点数或复数</span></span><br></pre></td></tr></table></figure>

<h2 id="2-2-TensorFlow的基本的运算"><a href="#2-2-TensorFlow的基本的运算" class="headerlink" title="2.2 TensorFlow的基本的运算"></a>2.2 TensorFlow的基本的运算</h2><h3 id="2-2-1-会话"><a href="#2-2-1-会话" class="headerlink" title="2.2.1 会话"></a>2.2.1 会话</h3><p><strong>会话</strong>拥有并管理TensorFlow程序运行时的所有<strong>资源</strong>,当所有计算完成之后需要<strong>关闭会话</strong>帮助系统<strong>回收资源</strong>.<br>示例如下:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义计算图</span></span><br><span class="line">tens1 = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个会话</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用这个创建好的会话来得到关心的运算的结果,比如可以调用sess.run(result)</span></span><br><span class="line"><span class="comment"># 来得到张量result的取值</span></span><br><span class="line"><span class="built_in">print</span>(sess.run(tens1))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭会话使得本次运行中使用到的资源可以被释放</span></span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<p>需要注意的一点,创建会话的tf.Session()函数第一个S需要大写.<br>需要明确调用 Session.close()函数来关闭会话并释放资源,在上述程序中,当程序因为异常退出时,关闭会话函数可能就不会被执行从而导致资源泄漏.<br>因此可以通过下面两种模式来确保会话的关闭.</p>
<p>会话的模式1:使用Python中的finally语句.示例代码如下:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义计算图</span></span><br><span class="line">tens1 = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个会话</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="built_in">print</span>(sess.run(tens1))</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Exception!&quot;</span>)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    <span class="comment">#确保能关闭会话使得本次运行中使用到的资源可以被释放</span></span><br><span class="line">    sess.close()</span><br></pre></td></tr></table></figure>

<p>会话的模式2:使用Python中的with语句.示例代码如下:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">node1 = tf.constant(<span class="number">3.0</span>,tf.float32,name=<span class="string">&quot;node1&quot;</span>)</span><br><span class="line">node2 = tf.constant(<span class="number">4.0</span>,tf.float32,name=<span class="string">&quot;node2&quot;</span>)</span><br><span class="line">result = tf.add(node1,node2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个会话,并通过Python中的上下文管理器来管理这个会话</span></span><br><span class="line"><span class="keyword">with</span>  tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 使用创建好的会话来计算关系的结果</span></span><br><span class="line">    <span class="built_in">print</span>(sess.run(result))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不需要再调用Session.close()函数来关闭会话</span></span><br><span class="line"><span class="comment"># 当上下文退出时会话关闭和资源释放也自动完成了</span></span><br></pre></td></tr></table></figure>

<p>TensorFlow不会自动生成默认的会话,需要手动指定.<br>当默认的会话被指定之后就可以通过<strong>tf.Tensor.eval()函数</strong>来计算一个张量的取值.<br>代码示例如下:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">node1 = tf.constant(<span class="number">3.0</span>,tf.float32,name=<span class="string">&quot;node1&quot;</span>)</span><br><span class="line">node2 = tf.constant(<span class="number">4.0</span>,tf.float32,name=<span class="string">&quot;node2&quot;</span>)</span><br><span class="line">result = tf.add(node1,node2)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default():</span><br><span class="line">    <span class="built_in">print</span>(result.<span class="built_in">eval</span>())</span><br></pre></td></tr></table></figure>
<p>或者用如下方式也是可以的:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">node1 = tf.constant(<span class="number">3.0</span>,tf.float32,name=<span class="string">&quot;node1&quot;</span>)</span><br><span class="line">node2 = tf.constant(<span class="number">4.0</span>,tf.float32,name=<span class="string">&quot;node2&quot;</span>)</span><br><span class="line">result = tf.add(node1,node2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="built_in">print</span>(result.<span class="built_in">eval</span>(session=sess))</span><br></pre></td></tr></table></figure>
<p>我在这里还发现了一个比较有意思的现象,如果像下面这样写的话,运行会报错:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">sess.as_default()</span><br><span class="line"><span class="built_in">print</span>(result.<span class="built_in">eval</span>())</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<p>但是改成下面这样就可以正确运行了.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.as_default()</span><br><span class="line">    <span class="built_in">print</span>(result.<span class="built_in">eval</span>())</span><br></pre></td></tr></table></figure>
<p>刚开始的那个写法是不正确的 ,但为什么不正确我也不太清楚.vscode给出的提示如下:Use <code>with sess.as_default()</code> or pass an explicit session to <code>eval(session=sess)</code>.以后还是用这两种形式指定默认会话吧.<br>在交互式环境下,Python脚本或者Jupyter编辑器下,通过设置默认会话来获取张量的取值更加方便.<br><strong>tf.InteractiveSession()</strong>,这个函数会生成一个会话并自动将生成的会话注册为默认会话.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result.<span class="built_in">eval</span>())</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>

<h3 id="2-2-2-常量和变量"><a href="#2-2-2-常量和变量" class="headerlink" title="2.2.2 常量和变量"></a>2.2.2 常量和变量</h3><p>**常量(constant)**是在运行过程中不会改变的单元,在TensorFlow中无须进行初始化操作.函数原型如下:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.constant(</span><br><span class="line">    value,</span><br><span class="line">    dtype=<span class="literal">None</span>,</span><br><span class="line">    shape=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="string">&#x27;Const&#x27;</span>,</span><br><span class="line">    verify_shape=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>value</strong>:是一个必须的值,可以是一个数值或字符串,也可以是一个列表.可以是一维的,也可以是多维的.<br><strong>dtype</strong>:数据类型.<br><strong>shape</strong>:张量的”形状”,即维数及每一维的大小.<br><strong>name</strong>:张量的名字,字符串类型.<br><strong>verify_shape</strong>:默认为False,如果修改为True的话表示检查value的形状与shape是否相符,如果不符会报错.<br>示例代码如下:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant(<span class="number">1.0</span>,dtype=tf.float32,name=<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">b = tf.constant(<span class="number">2.5</span>,dtype=tf.float32,name=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">c = tf.add(a,b,name=<span class="string">&#x27;c&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    c_value = sess.run(c)</span><br><span class="line">    <span class="built_in">print</span>(c_value)</span><br></pre></td></tr></table></figure>
<p>输出结果为3.5.</p>
<p><strong>变量(Variable)<strong>是在运行过程中会改变的单元,在TensorFlow中须进行</strong>初始化操作</strong>.<br>变量的定义与常量很相似,虽然函数有很多参数,但最常用的还是初始值,数据类型,名字这三个.函数原型如下:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># V是大写!</span></span><br><span class="line">tf.Variable(</span><br><span class="line">    initial_value=<span class="literal">None</span>,</span><br><span class="line">    trainable=<span class="literal">True</span>,</span><br><span class="line">    collections=<span class="literal">None</span>,</span><br><span class="line">    validate_shape=<span class="literal">True</span>,</span><br><span class="line">    caching_device=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    variable_def=<span class="literal">None</span>,</span><br><span class="line">    dtype=<span class="literal">None</span>,</span><br><span class="line">    expected_shape=<span class="literal">None</span>,</span><br><span class="line">    import_scope=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>变量</strong>须进行<strong>初始化操作</strong>,进行初始化的语句如下:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 个别变量初始化,这里的name_variable为你自己定义的变量名</span></span><br><span class="line">init_op = name_variable.initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有变量初始化</span></span><br><span class="line">init_op = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure>
<p>示例代码如下:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">node1 = tf.Variable(<span class="number">3.0</span>,dtype=tf.float32,name=<span class="string">&#x27;node1&#x27;</span>)</span><br><span class="line">node2 = tf.Variable(<span class="number">4.0</span>,dtype=tf.float32,name=<span class="string">&#x27;node2&#x27;</span>)</span><br><span class="line">result = tf.add(node1,node2,name=<span class="string">&#x27;add&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment">#变量初始化</span></span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(sess.run(result))</span><br></pre></td></tr></table></figure>
<p>输出结果为7.0<br>上面的代码增加了一个init初始化变量,并<strong>调用会话的run命令对参数进行初始化</strong>.这里一定要注意!<br>上述代码中调用tf.global_variables_initializer()函数只是进行了一个定义,并没有对变量进行初始化,必须要使用run命令对参数进行初始化.如果吧上面代码中的sess.run(init)删除后再运行的话,会发生错误.</p>
<h3 id="2-2-3-变量的赋值"><a href="#2-2-3-变量的赋值" class="headerlink" title="2.2.3 变量的赋值"></a>2.2.3 变量的赋值</h3><p>与传统编程语言不同,TensorFlow中的变量定义后,一般无需人工赋值,系统会根据算法模型,训练优化过程中<strong>自动调整变量对应的数值</strong>.<br>比如权重Weight变量w,经过多次迭代,会自动调整.如果不想让其自动调整,可以在定义变量时将trainable属性置为False.如下:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">epoch = tf.Variable(<span class="number">0</span>,name=<span class="string">&#x27;epoch&#x27;</span>,trainable=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>特殊情况需要人工更新的,可用tf.assign()进行变量赋值,函数参数如下:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.assign(</span><br><span class="line">    ref,</span><br><span class="line">    value,</span><br><span class="line">    validate_shape=<span class="literal">None</span>,</span><br><span class="line">    use_locking=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>函数将value的值赋给ref.其中,<strong>ref必须是tf.Variable创建的Tensor</strong>,我看文档里说value也是Tensor,但是在调用函数时将数值或者列表传递给value时也可以运行,可能是被包装成Tensor了吧.此外,默认情况下ref的shape和value的shape是相同的.<br>代码示例如下:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">value = tf.Variable(<span class="number">0</span>,name=<span class="string">&#x27;value&#x27;</span>)</span><br><span class="line">one = tf.constant(<span class="number">1</span>)</span><br><span class="line">new_value = tf.add(value,one)</span><br><span class="line"><span class="comment"># 将new_value的值赋给value</span></span><br><span class="line">update_value = tf.assign(value,new_value)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    	<span class="comment"># 执行赋值操作</span></span><br><span class="line">        sess.run(update_value)</span><br><span class="line">        <span class="built_in">print</span>(sess.run(value),end=<span class="string">&#x27; &#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>输出结果为:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span> <span class="number">10</span> </span><br></pre></td></tr></table></figure>
<p>同样的,上面的代码调用assign函数只是定义了一个赋值操作,要想执行该操作,必须要再调用run命令.<br>下面再贴一段计算1+2+3+…+10的代码:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">sum_value = tf.Variable(<span class="number">0</span>,name=<span class="string">&#x27;sum_value&#x27;</span>)</span><br><span class="line">number = tf.Variable(<span class="number">1</span>,name=<span class="string">&#x27;number&#x27;</span>)</span><br><span class="line">one = tf.constant(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">new_sum_value = tf.add(sum_value,number)</span><br><span class="line">new_number = tf.add(number,one)</span><br><span class="line"></span><br><span class="line">update_op1 = tf.assign(sum_value,new_sum_value)</span><br><span class="line">update_op2 = tf.assign(number,new_number)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        sess.run(update_op1)</span><br><span class="line">        sess.run(update_op2)</span><br><span class="line">    <span class="built_in">print</span>(sess.run(sum_value)) </span><br></pre></td></tr></table></figure>
<p>输出结果为55.</p>
<h3 id="2-2-4-占位符、Feed数据填充和Fetch数据获取"><a href="#2-2-4-占位符、Feed数据填充和Fetch数据获取" class="headerlink" title="2.2.4 占位符、Feed数据填充和Fetch数据获取"></a>2.2.4 占位符、Feed数据填充和Fetch数据获取</h3><p>TensorFlow中的<strong>Variable变量类型</strong>,在定义时需要初始化,但有些变量<strong>定义时并不知道其数值</strong>,只有当真正开始运行程序时,才由外部输入,比如训练数据,这时候需要用到<strong>占位符</strong>.<br>tf.placeholder<strong>占位符</strong>,是TensorFlow中特有的一种数据结构,类似动态变量,函数的参数、或者C语言或者Python语言中格式化输出时的”%”占位符.<br>占位符Placeholder的函数参数如下:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.placeholder(</span><br><span class="line">    dtype,</span><br><span class="line">    shape=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这里的shape参数可以是元组，也可以是列表。<br>示例如下:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此代码生成一个2*3的二维数组,矩阵中每个元素的类型都是tf.float32,</span></span><br><span class="line"><span class="comment"># 内部对应的符号名称是tx</span></span><br><span class="line">x = tf.placeholder(tf.float32,[<span class="number">2</span>,<span class="number">3</span>],name=<span class="string">&#x27;tx&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只指定列数为3，不指定行数，None表示行的数量未知</span></span><br><span class="line">y = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">3</span>],name=<span class="string">&#x27;ty&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>如果构建了一个包含placeholder操作的计算图,当在session中调用run方法时,placeholder占用的变量必须通过<strong>feed_dict参数</strong>传递进去,否则会报错.<br>传值时须按字典格式,以占位符的变量名为键,以所要传递的数据为值.<br>示例代码如下:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = tf.placeholder(tf.float32,name=<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">b = tf.placeholder(tf.float32,name=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">c = tf.multiply(a,b,name=<span class="string">&#x27;c&#x27;</span>)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="comment"># 通过feed_dict的参数传值,按字典格式</span></span><br><span class="line">    result = sess.run(c,feed_dict=&#123;a:<span class="number">8.0</span>,b:<span class="number">3.5</span>&#125;)</span><br><span class="line">    <span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>
<p>输出结果为28.0</p>
<p><strong>多次操作可以通过一次Feed完成执行</strong>:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a = tf.placeholder(tf.float32,name=<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">b = tf.placeholder(tf.float32,name=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">c = tf.multiply(a,b,name=<span class="string">&#x27;c&#x27;</span>)</span><br><span class="line">d = tf.subtract(a,b,name=<span class="string">&#x27;d&#x27;</span>)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    </span><br><span class="line">    result = sess.run([c,d],feed_dict=&#123;a:[<span class="number">8.0</span>,<span class="number">2.0</span>,<span class="number">3.5</span>],b:[<span class="number">1.5</span>,<span class="number">2.0</span>,<span class="number">4.0</span>]&#125;)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(result)</span><br><span class="line">    <span class="comment"># 取结果中的第一个</span></span><br><span class="line">    <span class="built_in">print</span>(result[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>输出为:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[array([<span class="number">12.</span>,  <span class="number">4.</span>, <span class="number">14.</span>], dtype=float32), array([ <span class="number">6.5</span>,  <span class="number">0.</span> , -<span class="number">0.5</span>], dtype=float32)]</span><br><span class="line">[<span class="number">12.</span>  <span class="number">4.</span> <span class="number">14.</span>]</span><br></pre></td></tr></table></figure>
<p>话说这里的输出结果不就是numpy中的数组类型吗,而且计算也跟numpy中的一样.</p>
<p>也可以<strong>一次返回多个值分别赋给多个变量</strong>,Python中是支持这种用法的.<br>示例代码如下:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">a = tf.placeholder(tf.float32,name=<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">b = tf.placeholder(tf.float32,name=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">c = tf.multiply(a,b,name=<span class="string">&#x27;c&#x27;</span>)</span><br><span class="line">d = tf.subtract(a,b,name=<span class="string">&#x27;d&#x27;</span>)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#返回的两个值分别赋给两个变量</span></span><br><span class="line">    rc,rd = sess.run([c,d],feed_dict=&#123;a:[<span class="number">8.0</span>,<span class="number">2.0</span>,<span class="number">3.5</span>],b:[<span class="number">1.5</span>,<span class="number">2.0</span>,<span class="number">4.0</span>]&#125;)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;value of c = &#x27;</span>,rc,<span class="string">&#x27;value of d = &#x27;</span>,rd)</span><br></pre></td></tr></table></figure>
<p>输出为:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">value of c =  [<span class="number">12.</span>  <span class="number">4.</span> <span class="number">14.</span>] value of d =  [ <span class="number">6.5</span>  <span class="number">0.</span>  -<span class="number">0.5</span>]</span><br></pre></td></tr></table></figure>

<h2 id="2-3-TensorBoard可视化初步"><a href="#2-3-TensorBoard可视化初步" class="headerlink" title="2.3 TensorBoard可视化初步"></a>2.3 TensorBoard可视化初步</h2><p>TensorBoard是TensorFlow的可视化工具,通过TensorFlow程序运行过程中输出的日志文件可视化TensorFlow程序的运行状态.<br>TensorBoard和TensorFlow程序跑在不同的进程中.<br>示例代码如下:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 清除default graph和不断增加的节点</span></span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="comment"># logdir改为自己机器上的合适路径,我这里用的是相对路径</span></span><br><span class="line">logdir = <span class="string">&#x27;log/&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个简单的计算图,实现向量加法的操作</span></span><br><span class="line">input1 = tf.constant([<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>],name=<span class="string">&#x27;input1&#x27;</span>)</span><br><span class="line">input2 = tf.Variable(tf.random_uniform([<span class="number">3</span>]),name=<span class="string">&#x27;input2&#x27;</span>)</span><br><span class="line">output = tf.add_n([input1,input2],name=<span class="string">&#x27;add&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一个写日志文件的Writer,并将当前的TensorFlow计算图写入日志</span></span><br><span class="line">writer = tf.summary.FileWriter(logdir,tf.get_default_graph())</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p>然后在终端中运行如下命令:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># /path/log改为你自己机器上日志文件的存放位置</span><br><span class="line">tensorboard --logdir=/path/log</span><br></pre></td></tr></table></figure>
<p>这样就会运行TensorBoard了,启动服务的端口默认为6006,使用–port参数可以改变启动服务的端口.<br>在浏览器访问<a href="http://localhost:6006即可打开TensorBoard">http://localhost:6006即可打开TensorBoard</a>.<br>下面是TensorBoard中的常用API:<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter2/%E5%9B%BE2-5.png"></p>
<h1 id="第三章-单变量线性回归-TensorFlow实战"><a href="#第三章-单变量线性回归-TensorFlow实战" class="headerlink" title="第三章 单变量线性回归:TensorFlow实战"></a>第三章 单变量线性回归:TensorFlow实战</h1><h2 id="3-1-监督式机器学习的基本术语"><a href="#3-1-监督式机器学习的基本术语" class="headerlink" title="3.1 监督式机器学习的基本术语"></a>3.1 监督式机器学习的基本术语</h2><h3 id="3-1-1-样本、特征、标签与模型"><a href="#3-1-1-样本、特征、标签与模型" class="headerlink" title="3.1.1 样本、特征、标签与模型"></a>3.1.1 样本、特征、标签与模型</h3><p><strong>机器学习系统</strong>：通过学习如何组合输入信息，来对未见过的数据做出有用的预测。<br>下面通过一个简单的线性回归案例来对一些基本的术语进行介绍。<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter3/%E5%9B%BE3-1.png"><br>可以看到图中的原始数据点的分布符合某种规律，可以画出一条直线，使这些点相对均匀的分布在直线两侧。假设该直线方程为y &#x3D; w*x + b，该方程即为线性回归方程。<br><strong>标签</strong>是我们要预测的事物，在简单线性回归中为y变量。<br><strong>特征</strong>是指用于描述数据的输入变量，在简单线性回归中为x变量。简单的机器学习项目可能会使用单个特征，而比较复杂的机器学习项目可能会使用数百万个特征，按如下方式指定：{x<sub>1</sub>,x<sub>2</sub>,…,x<sub>n</sub>}变量。<br><strong>样本</strong>是指数据的特定实例。可分为如下两类：</p>
<ul>
<li>有标签样本</li>
<li>无标签样本</li>
</ul>
<p><strong>有标签样本</strong>同时包含特征和标签。用于训练模型。在简单线性回归中为{x,y}。<br><strong>无标签样本</strong>包含特征，但不包含标签。用于对新数据进行预测。在简单线性回归中为{x,?}。<br><strong>模型</strong>可将无标签样本映射到预测标签y’。y’由模型内部参数定义，这些内部参数值是通过学习得到的。</p>
<h3 id="3-1-2-训练与损失"><a href="#3-1-2-训练与损失" class="headerlink" title="3.1.2 训练与损失"></a>3.1.2 训练与损失</h3><p><strong>训练</strong>模型表示通过<strong>有标签样本</strong>来学习所有<strong>权重</strong>和<strong>偏差</strong>的理想值。<br>在监督式学习中，机器学习算法通过以下方式构建模型：检查多个样本并尝试找出可最大限度地<strong>减少损失</strong>的模型。这一过程称为<strong>经验风险最小化</strong>。</p>
<p>损失是对糟糕预测的惩罚：<strong>损失（loss）</strong>是一个数值，表示对于<strong>单个样本</strong>而言模型预测的准确程度。<strong>代价（cost）</strong>是指<strong>整个训练集</strong>上所有样本误差的平均。不过现在很多都把损失与代价混着用了。<br>如果模型的预测完全准确，则损失为零，否则损失会较大。<br><strong>训练模型的目标</strong>是从所有样本中找到一组<strong>平均损失“较小”</strong>的权重和偏差。<br>下面举个损失的例子：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter3/%E5%9B%BE3-2.png"><br>下面介绍一下常用的损失函数：<br><strong>L<sub>1</sub>损失</strong>：基于模型预测的值与标签的实际值之差的绝对值。<br><strong>平方损失</strong>：一种常见的损失函数，又称<strong>L<sub>2</sub>损失</strong>，其为L<sub>1</sub>损失的平方。<br><strong>均方误差（MSE)<strong>：指的是每个样本的平均平方损失。计算公式如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter3/%E5%9B%BE3-3.png"><br>上面公式中的prediction(x)为y的预测值。<br>均方误差是常用的</strong>代价函数</strong>。但是也有很多地方代价函数与损失函数这两种说法混用。</p>
<h3 id="3-1-3-模型训练与降低损失"><a href="#3-1-3-模型训练与降低损失" class="headerlink" title="3.1.3 模型训练与降低损失"></a>3.1.3 模型训练与降低损失</h3><p>训练模型的迭代过程如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter3/%E5%9B%BE3-4.png"><br>模型训练要点：首先对权重w和偏差b进行初始猜测，然后反复调整这些猜测，直到获得损失可能最低的权重和偏差为止。</p>
<p>在学习优化过程中，机器学习系统将根据所有标签去重新评估所有特征，为损失函数生成一个新值，而该值又产生新的参数值。<br>通常，你可以不断迭代，直到总体损失不再变化或至少变化及其缓慢为止。这时候，我们可以说该模型已<strong>收敛</strong>。</p>
<p>如果采用均方差作为该线性回归模型的代价函数，则其为<strong>凸函数</strong>。如下图：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter3/%E5%9B%BE3-5.png"><br>上图中的纵坐标严格来说应该是<strong>代价</strong>，而不是<strong>损失</strong>。凸形问题<strong>只有一个最低点</strong>；即只存在一个<strong>斜率正好为0</strong>的位置，这个最小值就是损失函数收敛之处。<br>值得一提的是，这里的凸函数与统计高数教材上的定义恰恰相反，这里的凹凸均是指的向下凹或向下凸。国外大多采用的这种定义。下面详细说一下这里的凸函数的定义。<br>对区间[a,b]上定义的函数f，若它对区间中任意两点x1和x2，均有f((x1+x2)&#x2F;2)&lt;&#x3D;(f(x1)+f(x2))&#x2F;2，则称f为区间[a,b]上的凸函数。对实数集上的函数，可通过求二阶导数来判别。若二阶导数在区间上非负，则称为凸函数，若二阶导数恒大于0，则称为严格凸函数。<br>显然这里的代价函数为凸函数，因为其二阶导数恒大于等于零。</p>
<h3 id="3-1-4-梯度下降法与超参数"><a href="#3-1-4-梯度下降法与超参数" class="headerlink" title="3.1.4 梯度下降法与超参数"></a>3.1.4 梯度下降法与超参数</h3><p><strong>梯度</strong>：一个向量（矢量），表示某一函数在该点的<strong>方向导数</strong>沿着该方向取得<strong>最大值</strong>，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大。<br><strong>梯度</strong>是矢量，具有<strong>方向</strong>和<strong>大小</strong>。<br><strong>梯度下降法</strong>即沿着<strong>负梯度方向</strong>进行下一步探索，最终会到达一个<strong>局部最小值点</strong>。关于<strong>梯度下降</strong>和<strong>反向传播算法</strong>的详细内容可以去B站上看一下3b1b的视频，讲的挺不错的。<br>但是沿着负梯度方向进行下一步探索，<strong>前进多少合适？</strong><br>用<strong>负梯度</strong>乘以一个称为<strong>学习速率</strong>（有时也称为<strong>步长</strong>）的标量，以确定下一个点的位置。<br>因此<strong>反向传播算法</strong>中跟新参数时，新参数等于旧参数加上负梯度乘学习率。<br>不过学习率要选的合适才会有比较好的效果，至于多少合适，也没有明确的方法指导，只有自己看着设置了。学习率设置不当的后果如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter3/%E5%9B%BE3-6.png"></p>
<p>在机器学习中，<strong>超参数</strong>是在开始学习过程<strong>之前</strong>设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，选择一组好的超参数，可以提高学习的性能和效果。<br><strong>超参数</strong>是编程人员在机器学习算法中用于调整的旋钮。（原来传说中的调参侠是这么来的……）<br>典型超参数：学习率、神经网络的隐含层数量……</p>
<h2 id="3-2-线性回归问题TensorFlow实战"><a href="#3-2-线性回归问题TensorFlow实战" class="headerlink" title="3.2 线性回归问题TensorFlow实战"></a>3.2 线性回归问题TensorFlow实战</h2><h3 id="3-2-1-基本步骤"><a href="#3-2-1-基本步骤" class="headerlink" title="3.2.1 基本步骤"></a>3.2.1 基本步骤</h3><p>使用TensorFlow进行算法设计与训练的核心步骤如下：</p>
<ol>
<li>准备模型</li>
<li>构建模型</li>
<li>训练模型</li>
<li>进行预测</li>
</ol>
<p>上述步骤是我们使用TensorFlow进行算法设计与训练的核心步骤，贯穿于后面介绍的具体实战中，本章用一个简单的例子来讲解这几个步骤。<br>单变量的线性方程可以表示为：y &#x3D; w*x + b。本例通过生成人工数据集，随机生成一个近似采样随机分布，使得w&#x3D;2.0,b&#x3D;1,并加入一个噪声，噪声的最大振幅为0.4。<br>部分代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在Jupyter中，使用matplotlib显示图像需要设置为inline模式，否则不会显示图像</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置随机数种子</span></span><br><span class="line">np.random.seed(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接采用np生成等差数列的方法，生成100个点，每个点的取值在-1～1之间</span></span><br><span class="line">x_data = np.linspace(-<span class="number">1</span>,<span class="number">1</span>,<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># y = 2*x + 1 + 噪声，其中，噪声的维度与x_data一样</span></span><br><span class="line"><span class="comment"># randn生成的随机数为标准正态分布</span></span><br><span class="line">y_data = <span class="number">2</span>*x_data + <span class="number">1.0</span> + np.random.randn(*x_data.shape) * <span class="number">0.4</span></span><br></pre></td></tr></table></figure>
<p>上面的x_data与y_data均为一维的shape为（100,）的ndarray数组。<br>其中，关于numpy和matplotlib的用法我之前的博客。<br>上面代码中最后一句中的*的作用具体见下面这篇博客：<a target="_blank" rel="noopener" href="https://blog.csdn.net/yilovexing/article/details/80577510">https://blog.csdn.net/yilovexing/article/details/80577510</a> </p>
<p>利用matplotlib画出生成结果：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 画出随机生成数据的散点图</span></span><br><span class="line">plt.scatter(x_data,y_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出我们想要学习到的线性函数y = 2*x + 1</span></span><br><span class="line">plt.plot(x_data,<span class="number">2</span>*x_data + <span class="number">1.0</span>,color=<span class="string">&#x27;red&#x27;</span>,linewidth=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>所绘制图形如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter3/%E5%9B%BE3-7.png"></p>
<p>构建模型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义训练数据的占位符，x是特征值，y是标签值</span></span><br><span class="line">x = tf.placeholder(np.float32,name=<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">y = tf.placeholder(np.float32,name=<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model</span>(<span class="params">x,w,b</span>):</span><br><span class="line">    <span class="keyword">return</span> tf.multiply(x,w) + b</span><br></pre></td></tr></table></figure>

<p>定义模型结构：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建线性函数的斜率，变量w</span></span><br><span class="line">w = tf.Variable(<span class="number">1.0</span>,name=<span class="string">&#x27;w0&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#构建线性函数的截距，变量b</span></span><br><span class="line">b = tf.Variable(<span class="number">0.0</span>,name=<span class="string">&#x27;b0&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># pred是预测值，前向计算</span></span><br><span class="line">pred = model(x,w,b)</span><br></pre></td></tr></table></figure>

<p>设置训练参数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 迭代次数</span></span><br><span class="line">train_epochs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 学习率</span></span><br><span class="line">learning_rate = <span class="number">0.05</span></span><br></pre></td></tr></table></figure>

<p>定义损失函数：</p>
<ul>
<li>损失函数用于描述预测值与真实值之间的误差，从而指导模型收敛方向。</li>
<li>常见损失函数：均方差(Mean Square Error, MSE)和交叉熵(cross-entropy)。<br>交叉商后面再说，这里我们用L<sub>2</sub>损失函数（均方差）。代码如下：<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 采用均方差作为损失函数</span></span><br><span class="line">loss_function = tf.reduce_mean(tf.square(y - pred))</span><br></pre></td></tr></table></figure></li>
</ul>
<p>定义优化器：<br>定义优化器Optimizer，初始化一个GradientDescentOptimizer。<br>设置学习率，设置优化目标：最小化损失。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#梯度下降优化器</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_function)</span><br></pre></td></tr></table></figure>

<p>初始化及开始训练：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 声明会话</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 变量初始化</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始迭代训练，轮数为epoch，采用SGD随机下降优化方法</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(train_epochs):</span><br><span class="line">    <span class="keyword">for</span> xs,ys <span class="keyword">in</span> <span class="built_in">zip</span>(x_data,y_data):</span><br><span class="line">        _,loss = sess.run([optimizer,loss_function],feed_dict = &#123;x:xs,y:ys&#125;)</span><br><span class="line">    b0temp = b.<span class="built_in">eval</span>(session=sess)</span><br><span class="line">    w0temp = w.<span class="built_in">eval</span>(session=sess)</span><br><span class="line">    plt.plot(x_data,w0temp * x_data + b0temp)    <span class="comment"># 在一个绘图区域内绘制出迭代训练结果图形</span></span><br></pre></td></tr></table></figure>
<p>图形如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter3/%E5%9B%BE3-8.png"><br>从上图可以看出,本案例所拟合的模型较简单，训练3次之后已经接近收敛，对于复杂模型，需要更多次训练才能收敛。</p>
<p>关于上面代码中的<strong>zip()函数</strong>，由于用的比较多，我在这里给出一些说明。<br>zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的<strong>zip对象</strong>。每个元组依次取各个可迭代对象的一个元素所组合成的。<br>我们可以使用list()转换来输出列表。<br>当传入参数的长度不同时，zip能自动以最短序列长度为准进行截取。<br>此外，<strong>*号操作符</strong>可以实现zip()函数的逆过程，将zip对象变成原先组合前的数据。<br>zip语法：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 其参数为一个或多个迭代器，可以是元组、列表、字典等类型。</span></span><br><span class="line"><span class="built_in">zip</span>([iterable, ...])</span><br></pre></td></tr></table></figure>
<p>下面是示例代码：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: b = [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: c = [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: zipped1 = <span class="built_in">zip</span>(a)	<span class="comment"># 一个参数时</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: zipped1	<span class="comment"># zip()函数返回值为zip对象</span></span><br><span class="line">Out[<span class="number">5</span>]: &lt;<span class="built_in">zip</span> at <span class="number">0x7f8f08027f88</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: <span class="built_in">list</span>(zipped1)	<span class="comment"># 使用list()函数将其转化为列表</span></span><br><span class="line">Out[<span class="number">6</span>]: [(<span class="number">1</span>,), (<span class="number">2</span>,), (<span class="number">3</span>,)]</span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: zipped2 = <span class="built_in">zip</span>(a,b)	<span class="comment"># 两个参数时</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: <span class="built_in">list</span>(zipped2)</span><br><span class="line">Out[<span class="number">8</span>]: [(<span class="number">1</span>, <span class="number">4</span>), (<span class="number">2</span>, <span class="number">5</span>), (<span class="number">3</span>, <span class="number">6</span>)]</span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: zipped3 = <span class="built_in">zip</span>(b,c)	<span class="comment"># 不同长度</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">10</span>]: <span class="built_in">list</span>(zipped3)	<span class="comment"># 以最短序列长度为准</span></span><br><span class="line">Out[<span class="number">10</span>]: [(<span class="number">4</span>, <span class="number">4</span>), (<span class="number">5</span>, <span class="number">5</span>), (<span class="number">6</span>, <span class="number">6</span>)]</span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: zipped = <span class="built_in">zip</span>(a,b)</span><br><span class="line"></span><br><span class="line">In [<span class="number">12</span>]: unzipped = <span class="built_in">zip</span>(*zipped)	<span class="comment"># 与zip()相反，zip(*)可理解为解压，将zip对象变成原先组合前的数据</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">13</span>]: <span class="built_in">list</span>(unzipped)</span><br><span class="line">Out[<span class="number">13</span>]: [(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), (<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)]</span><br></pre></td></tr></table></figure>
<p>需要注意的是，这里有一个坑！看下面的代码：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: b = [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: zipped = <span class="built_in">zip</span>(a,b)</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: <span class="built_in">list</span>(zipped)</span><br><span class="line">Out[<span class="number">4</span>]: [(<span class="number">1</span>, <span class="number">4</span>), (<span class="number">2</span>, <span class="number">5</span>), (<span class="number">3</span>, <span class="number">6</span>)]</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: <span class="built_in">list</span>(zipped)	<span class="comment"># 这里居然输出了一个空列表！</span></span><br><span class="line">Out[<span class="number">5</span>]: []</span><br></pre></td></tr></table></figure>
<p>为什么会出现上面这种情况呢，主要是因为zip对象是一个<strong>迭代器</strong>。<strong>迭代器只能前进，不能后退</strong>。<br>拿上面的代码来说，list()函数执行完后，迭代器的内部指针已经指向的内部的最后一个元组，然后到了下一个list()函数的时候，迭代器只能前进不能后退，所以指针没有被重置，此时迭代器已经没有元组可返回了，所以结果为空list。<br>zip()函数的介绍告一段落，下面接着开始正题。</p>
<p>当训练完成后，打印查看参数。代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w: &#x27;</span>,sess.run(w))    <span class="comment"># w的值应该在2附近</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b: &#x27;</span>,sess.run(b))    <span class="comment"># b的值应该在1附近</span></span><br></pre></td></tr></table></figure>
<p>输出为：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w:  <span class="number">1.9822965</span></span><br><span class="line">b:  <span class="number">1.0420128</span></span><br></pre></td></tr></table></figure>

<p>结果可视化：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(x_data,y_data,label=<span class="string">&#x27;Original&#x27;</span>)</span><br><span class="line">plt.plot(x_data,x_data * sess.run(w) + sess.run(b),</span><br><span class="line">        label=<span class="string">&#x27;Fitted line&#x27;</span>,color=<span class="string">&#x27;r&#x27;</span>,linewidth=<span class="number">3</span>)</span><br><span class="line">plt.legend()    <span class="comment"># 显示图例</span></span><br></pre></td></tr></table></figure>
<p>绘制图形如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter3/%E5%9B%BE3-9.png"></p>
<p>利用模型进行预测：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x_test = <span class="number">3.21</span></span><br><span class="line">predict = sess.run(pred,feed_dict=&#123;x:x_test&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;预测值：%f&#x27;</span> % predict)</span><br><span class="line"></span><br><span class="line">target = <span class="number">2</span> * x_test + <span class="number">1.0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;目标值：%f&#x27;</span> % target)</span><br></pre></td></tr></table></figure>
<p>输出为：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">预测值：<span class="number">7.405184</span></span><br><span class="line">目标值：<span class="number">7.420000</span></span><br></pre></td></tr></table></figure>

<p>以上通过一个线性回归的简单例子介绍了利用TensorFlow实现机器学习的思路，重点讲解了下述步骤：</p>
<ol>
<li>生成人工数据集及其可视化</li>
<li>构建线性模型</li>
<li>定义损失函数</li>
<li>定义优化器、最小化损失函数</li>
<li>训练结果的可视化</li>
<li>利用学习完成的模型进行预测</li>
</ol>
<h3 id="3-2-2-显示损失Loss"><a href="#3-2-2-显示损失Loss" class="headerlink" title="3.2.2 显示损失Loss"></a>3.2.2 显示损失Loss</h3><p>对之前进行训练的代码进行一些修改，使其能够显示损失值：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开始训练，轮数为epoch，采用SGD随机下降优化方法</span></span><br><span class="line">step = <span class="number">0</span>    <span class="comment"># 记录训练步数</span></span><br><span class="line">loss_list = []    <span class="comment">#用于保存loss值的列表</span></span><br><span class="line">display_step = <span class="number">10</span>    <span class="comment">#控制报告的粒度</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(train_epochs):</span><br><span class="line">    <span class="keyword">for</span> xs,ys <span class="keyword">in</span> <span class="built_in">zip</span>(x_data,y_data):</span><br><span class="line">        _,loss = sess.run([optimizer,loss_function],feed_dict = &#123;x:xs,y:ys&#125;)</span><br><span class="line">        <span class="comment"># 显示损失值loss</span></span><br><span class="line">        loss_list.append(loss)</span><br><span class="line">        step = step + <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> step % display_step == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Train Epoch: %02d   Step: %03d   loss= %.9f&#x27;</span></span><br><span class="line">                 % (epoch + <span class="number">1</span>,step,loss))</span><br><span class="line">            </span><br><span class="line">    b0temp = b.<span class="built_in">eval</span>(session=sess)</span><br><span class="line">    w0temp = w.<span class="built_in">eval</span>(session=sess)</span><br><span class="line">    plt.plot(x_data,w0temp * x_data + b0temp)    <span class="comment">#在一个绘图区域内绘制出训练图形</span></span><br></pre></td></tr></table></figure>
<p>上述代码中定义了一个display_step变量来控制报告的粒度。例如，如果display_step设为10，则将每训练10个样本就输出一次损失值。与超参数不同，修改diaplay_step不会更改模型学习的规律。<br>还可以图形化显示损失值。如下图：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter3/%E5%9B%BE3-10.png"></p>
<p>在梯度下降法中，<strong>批量</strong>指的是用于在单次迭代中计算梯度的样本总数。<br>假定批量是指整个数据集，数据集通常包含很大样本（数万甚至数千亿），此外，数据集通常包含多个特征。因此，一个批量可能相当巨大。如果是超大批量，则单次迭代就可能要花费很长时间进行计算。<br>介绍完了批量的概念，下面介绍一下梯度下降法的几种不同形式，其形式共有以下三种：</p>
<ul>
<li><strong>批量梯度下降（Batch Gradient Descent，BSD）</strong></li>
<li><strong>随机梯度下降（Stochastic Gradient Descent，SGD）</strong></li>
<li><strong>小批量梯度下降（Mini-Batch Gradient Descent，MBGD）</strong></li>
</ul>
<p><strong>批量梯度下降法</strong>是最原始的形式，它是指在<strong>每一次迭代时</strong>使用<strong>所有样本</strong>来进行梯度的更新。<br><strong>随机梯度下降法</strong>不同于批量梯度下降，随机梯度下降是<strong>每次迭代</strong>使用<strong>一个样本</strong>来对参数进行更新。使得训练速度加快。”随机“这一术语表示构成各个批量的一个样本都是随机选择的。<br><strong>小批量梯度下降</strong>是对批量梯度下降以及随机梯度下降的一个折中办法。其思想是<strong>每次迭代</strong>使用<strong>小批量</strong>的样本来对参数进行更新。小批量通常包含10～1000个随机选择的样本。小批量梯度下降可以减少SGD中的杂乱样本数量，但仍然比全批量更高效。</p>
<h1 id="第四章-多元线性回归：波士顿房价预测问题"><a href="#第四章-多元线性回归：波士顿房价预测问题" class="headerlink" title="第四章 多元线性回归：波士顿房价预测问题"></a>第四章 多元线性回归：波士顿房价预测问题</h1><h2 id="4-1-机器学习中的线性代数基础"><a href="#4-1-机器学习中的线性代数基础" class="headerlink" title="4.1 机器学习中的线性代数基础"></a>4.1 机器学习中的线性代数基础</h2><p>为了之后的学习，先在这里简单介绍一下机器学习中的线性代数。</p>
<h3 id="4-1-1-线性代数的数学对象"><a href="#4-1-1-线性代数的数学对象" class="headerlink" title="4.1.1 线性代数的数学对象"></a>4.1.1 线性代数的数学对象</h3><p>线性代数中有如下的<strong>数学对象</strong>：</p>
<ul>
<li><strong>标量</strong></li>
<li><strong>向量</strong></li>
<li><strong>矩阵</strong></li>
</ul>
<p>如图：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter4/%E5%9B%BE4-1.png"></p>
<p><strong>标量</strong>只是一个单一的数字。示例如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: scalar_value = <span class="number">18</span>	<span class="comment"># 标量</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: scalar_value</span><br><span class="line">Out[<span class="number">3</span>]: <span class="number">18</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: scalar_value.shape	<span class="comment"># 报错，因为int类型的标量没有shape属性</span></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line"></span><br><span class="line">  File <span class="string">&quot;&lt;ipython-input-4-1e2e843c11f1&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    scalar_value.shape</span><br><span class="line"></span><br><span class="line">AttributeError: <span class="string">&#x27;int&#x27;</span> <span class="built_in">object</span> has no attribute <span class="string">&#x27;shape&#x27;</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: scalar_np = np.array(scalar_value)	<span class="comment"># 将其转化为ndarray</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: scalar_np.shape	<span class="comment"># 有shape属性</span></span><br><span class="line">Out[<span class="number">6</span>]: ()</span><br></pre></td></tr></table></figure>

<p><strong>向量</strong>是一个有序的数字数组，可以在一行或一列中。<br>请看下面的代码：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">7</span>]: vector_value = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]    <span class="comment"># 列表</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: vector_np = np.array(vector_value)    <span class="comment"># 转化为ndarray</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># shape显示为一维数组，其实这既不是行向量也不是列向量</span></span><br><span class="line">In [<span class="number">9</span>]: <span class="built_in">print</span>(vector_np,vector_np.shape)</span><br><span class="line">[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>] (<span class="number">3</span>,)</span><br></pre></td></tr></table></figure>
<p>我们在上面的代码中利用列表创建了一个ndarray数组，该数组是一维的，所以并不是行向量或列向量。仔细想想，不管行向量还是列向量，都是<strong>矩阵</strong>的特殊形式，应当是<strong>二维</strong>的，比如列向量为m行1列。<br>因此，我们应当以如下的方式创建行、列向量。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">10</span>]: vector_row = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]])	<span class="comment"># 创建行向量</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: vector_row	<span class="comment"># 行向量</span></span><br><span class="line">Out[<span class="number">11</span>]: array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">12</span>]: vector_row.shape	<span class="comment"># shape为(1,3)，即1行3列，为行向量</span></span><br><span class="line">Out[<span class="number">12</span>]: (<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">13</span>]: vector_column = np.array([[<span class="number">4</span>],[<span class="number">5</span>],[<span class="number">6</span>]])	<span class="comment"># 创建列向量</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">14</span>]: vector_column	<span class="comment"># 列向量</span></span><br><span class="line">Out[<span class="number">14</span>]: </span><br><span class="line">array([[<span class="number">4</span>],</span><br><span class="line">       [<span class="number">5</span>],</span><br><span class="line">       [<span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">15</span>]: vector_column.shape	<span class="comment"># shape为(3,1)，即3行1列，为列向量</span></span><br><span class="line">Out[<span class="number">15</span>]: (<span class="number">3</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p><strong>矩阵</strong>是一个有序的二维数组,它有两个索引。第一个指向该行,第二个指向该列。向量也是一个矩阵,但只有一行或一列。<br>创建矩阵的示例代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">16</span>]: matrix_list = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]	<span class="comment"># 列表</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">17</span>]: matrix_list</span><br><span class="line">Out[<span class="number">17</span>]: [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">In [<span class="number">18</span>]: matrix_np = np.array(matrix_list)	<span class="comment"># 创建矩阵</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">19</span>]: matrix_np</span><br><span class="line">Out[<span class="number">19</span>]: </span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">20</span>]: matrix_np.shape	<span class="comment"># 2行3列的矩阵</span></span><br><span class="line">Out[<span class="number">20</span>]: (<span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<h3 id="4-1-2-线性代数基本运算规则"><a href="#4-1-2-线性代数基本运算规则" class="headerlink" title="4.1.2 线性代数基本运算规则"></a>4.1.2 线性代数基本运算规则</h3><p>矩阵标量运算：如果<strong>矩阵</strong>乘、除或者加、减一个<strong>标量</strong>，采用广播运算，即矩阵的每个元素都与该标量进行数学运算。<br>如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter4/%E5%9B%BE4-2.png"></p>
<p>矩阵之间的加法和减法：要求矩阵具有<strong>相同的尺寸</strong>，并且结果将是与原矩阵尺寸相同的矩阵。计算时只需在第一个矩阵中添加或减去第二个矩阵中对应的值。<br>如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter4/%E5%9B%BE4-3.png"></p>
<p>矩阵之间的<strong>点乘（点积）</strong>：要求矩阵具有<strong>相同的尺寸</strong>，矩阵各个<strong>对应元素相乘</strong>。<br>如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter4/%E5%9B%BE4-4.png"></p>
<p>矩阵之间<strong>相乘（叉乘）</strong>：叉乘才是我们一般说的矩阵的乘法。只有<strong>第一个矩阵的列数与第二个矩阵的行数相等</strong>时，二者才能相乘。设第一个矩阵A的尺寸为m*n，第二个矩阵B的尺寸为n*k，则A*B的尺寸为m*k。<br>如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter4/%E5%9B%BE4-5.png"><br>由于向量可以看作是矩阵的特例，所以就不详细讲向量的乘法了。</p>
<p>示例代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: matrix_a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],	<span class="comment"># 2行3列</span></span><br><span class="line">                            [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: matrix_b = np.array([[-<span class="number">1</span>,-<span class="number">2</span>,-<span class="number">3</span>],	<span class="comment"># 2行3列</span></span><br><span class="line">                            [-<span class="number">4</span>,-<span class="number">5</span>,-<span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: matrix_a * matrix_b		<span class="comment"># 矩阵点乘</span></span><br><span class="line">Out[<span class="number">4</span>]: </span><br><span class="line">array([[ -<span class="number">1</span>,  -<span class="number">4</span>,  -<span class="number">9</span>],</span><br><span class="line">       [-<span class="number">16</span>, -<span class="number">25</span>, -<span class="number">36</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: np.multiply(matrix_a,matrix_b)		<span class="comment"># 作用与*相同</span></span><br><span class="line">Out[<span class="number">5</span>]: </span><br><span class="line">array([[ -<span class="number">1</span>,  -<span class="number">4</span>,  -<span class="number">9</span>],</span><br><span class="line">       [-<span class="number">16</span>, -<span class="number">25</span>, -<span class="number">36</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: matrix_a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],	<span class="comment"># 2行3列</span></span><br><span class="line">                            [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: matrix_b = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],	<span class="comment"># 3行4列</span></span><br><span class="line">                            [<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>],</span><br><span class="line">                            [<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: np.dot(matrix_a,matrix_b)	<span class="comment"># 矩阵的乘法（叉乘），结果2行4列</span></span><br><span class="line">Out[<span class="number">8</span>]: </span><br><span class="line">array([[<span class="number">14</span>, <span class="number">16</span>, <span class="number">10</span>, <span class="number">10</span>],</span><br><span class="line">       [<span class="number">32</span>, <span class="number">37</span>, <span class="number">28</span>, <span class="number">28</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: vector_row = np.array([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]])	<span class="comment"># 行向量</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">10</span>]: vector_column = np.array([[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">1</span>]])	<span class="comment"># 列向量</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: np.dot(vector_row,vector_column)	<span class="comment"># 行向量乘列向量，为单个数</span></span><br><span class="line">Out[<span class="number">11</span>]: array([[<span class="number">3</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">12</span>]: np.dot(vector_column,vector_row)	<span class="comment"># 列向量乘行向量，为矩阵</span></span><br><span class="line">Out[<span class="number">12</span>]: </span><br><span class="line">array([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">       </span><br><span class="line">In [<span class="number">13</span>]: np.matmul(matrix_a,matrix_b)	<span class="comment"># 矩阵相乘（叉乘）用这种方式也可以</span></span><br><span class="line">Out[<span class="number">13</span>]: </span><br><span class="line">array([[<span class="number">14</span>, <span class="number">16</span>, <span class="number">10</span>, <span class="number">10</span>],</span><br><span class="line">       [<span class="number">32</span>, <span class="number">37</span>, <span class="number">28</span>, <span class="number">28</span>]])</span><br></pre></td></tr></table></figure>
<p>虽然我们说上面的是矩阵，但是这些矩阵其实是ndarray类型，本质上还是二维数组。因此在用*做乘法时表现出来的是对应元素相乘，即矩阵的点乘运算。如果想要使用矩阵乘法或矩阵向量乘法的话，请用**np.dot()<strong>方法或</strong>np.matmul()**方法。<br>那么为什么numpy库不实现一个matrix类型呢，那样不是更方便吗？<br>实际上numpy中确实有一个matrix类型，不过官方强烈不推荐使用，这个类型在将来很可能被废除。</p>
<p>关于矩阵转置的实现，可以用**np.transpose()**方法。<br>示例代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">13</span>]: matrix_a</span><br><span class="line">Out[<span class="number">13</span>]: </span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">14</span>]: np.transpose(matrix_a)</span><br><span class="line">Out[<span class="number">14</span>]: </span><br><span class="line">array([[<span class="number">1</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">5</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">15</span>]: vector_row</span><br><span class="line">Out[<span class="number">15</span>]: array([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">16</span>]: np.transpose(vector_raw)</span><br><span class="line">Out[<span class="number">16</span>]: </span><br><span class="line">array([[<span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>]])</span><br></pre></td></tr></table></figure>

<h2 id="4-2-数据与问题分析"><a href="#4-2-数据与问题分析" class="headerlink" title="4.2 数据与问题分析"></a>4.2 数据与问题分析</h2><h3 id="4-2-1-波士顿房价问题描述"><a href="#4-2-1-波士顿房价问题描述" class="headerlink" title="4.2.1 波士顿房价问题描述"></a>4.2.1 波士顿房价问题描述</h3><p>波士顿房价数据集包括<strong>506</strong>个样本，每个样本包括<strong>12个特征变量</strong>和该地区的平均房价。<br>房价显然和多个特征变量相关，不是单变量线性回归（<strong>一元线性回归</strong>）问题。需要选择多个特征变量来建立线性方程,这就是多变量线性回归（<strong>多元线性回归</strong>）问题。</p>
<p>回顾一下上一章所说的使用TensorFlow进行算法设计与训练的核心步骤：</p>
<ol>
<li>准备数据</li>
<li>构建模型</li>
<li>模型训练</li>
<li>进行预测</li>
</ol>
<p>本章我们仍是按照上述步骤进行。</p>
<h3 id="4-2-2-数据文件读取"><a href="#4-2-2-数据文件读取" class="headerlink" title="4.2.2 数据文件读取"></a>4.2.2 数据文件读取</h3><p>该数据集简单介绍如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter4/%E5%9B%BE4-6.png"></p>
<p>通过pandas读取数据文件，列出统计概述：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据文件</span></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;data/boston.csv&#x27;</span>,header=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示数据摘要描述信息</span></span><br><span class="line"><span class="built_in">print</span>(df.describe())</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">             CRIM         ZN       INDUS         CHAS         NOX          RM  \</span><br><span class="line">count  <span class="number">506.000000</span>  <span class="number">506.000000</span>  <span class="number">506.000000</span>  <span class="number">506.000000</span>  <span class="number">506.000000</span>  <span class="number">506.000000</span>   </span><br><span class="line">mean     <span class="number">3.613524</span>   <span class="number">11.363636</span>   <span class="number">11.136779</span>    <span class="number">0.069170</span>    <span class="number">0.554695</span>    <span class="number">6.284634</span>   </span><br><span class="line">std      <span class="number">8.601545</span>   <span class="number">23.322453</span>    <span class="number">6.860353</span>    <span class="number">0.253994</span>    <span class="number">0.115878</span>    <span class="number">0.702617</span>   </span><br><span class="line"><span class="built_in">min</span>      <span class="number">0.006320</span>    <span class="number">0.000000</span>    <span class="number">0.460000</span>    <span class="number">0.000000</span>    <span class="number">0.385000</span>    <span class="number">3.561000</span>   </span><br><span class="line"><span class="number">25</span>%      <span class="number">0.082045</span>    <span class="number">0.000000</span>    <span class="number">5.190000</span>    <span class="number">0.000000</span>    <span class="number">0.449000</span>    <span class="number">5.885500</span>   </span><br><span class="line"><span class="number">50</span>%      <span class="number">0.256510</span>    <span class="number">0.000000</span>    <span class="number">9.690000</span>    <span class="number">0.000000</span>    <span class="number">0.538000</span>    <span class="number">6.208500</span>   </span><br><span class="line"><span class="number">75</span>%      <span class="number">3.677082</span>   <span class="number">12.500000</span>   <span class="number">18.100000</span>    <span class="number">0.000000</span>    <span class="number">0.624000</span>    <span class="number">6.623500</span>   </span><br><span class="line"><span class="built_in">max</span>     <span class="number">88.976200</span>  <span class="number">100.000000</span>   <span class="number">27.740000</span>    <span class="number">1.000000</span>    <span class="number">0.871000</span>    <span class="number">8.780000</span>   </span><br><span class="line"></span><br><span class="line">              AGE         DIS         RAD         TAX     PTRATIO       LSTAT  \</span><br><span class="line">count  <span class="number">506.000000</span>  <span class="number">506.000000</span>  <span class="number">506.000000</span>  <span class="number">506.000000</span>  <span class="number">506.000000</span>  <span class="number">506.000000</span>   </span><br><span class="line">mean    <span class="number">68.574901</span>    <span class="number">3.795043</span>    <span class="number">9.549407</span>  <span class="number">408.237154</span>   <span class="number">18.455534</span>   <span class="number">12.653063</span>   </span><br><span class="line">std     <span class="number">28.148861</span>    <span class="number">2.105710</span>    <span class="number">8.707259</span>  <span class="number">168.537116</span>    <span class="number">2.164946</span>    <span class="number">7.141062</span>   </span><br><span class="line"><span class="built_in">min</span>      <span class="number">2.900000</span>    <span class="number">1.129600</span>    <span class="number">1.000000</span>  <span class="number">187.000000</span>   <span class="number">12.600000</span>    <span class="number">1.730000</span>   </span><br><span class="line"><span class="number">25</span>%     <span class="number">45.025000</span>    <span class="number">2.100175</span>    <span class="number">4.000000</span>  <span class="number">279.000000</span>   <span class="number">17.400000</span>    <span class="number">6.950000</span>   </span><br><span class="line"><span class="number">50</span>%     <span class="number">77.500000</span>    <span class="number">3.207450</span>    <span class="number">5.000000</span>  <span class="number">330.000000</span>   <span class="number">19.050000</span>   <span class="number">11.360000</span>   </span><br><span class="line"><span class="number">75</span>%     <span class="number">94.075000</span>    <span class="number">5.188425</span>   <span class="number">24.000000</span>  <span class="number">666.000000</span>   <span class="number">20.200000</span>   <span class="number">16.955000</span>   </span><br><span class="line"><span class="built_in">max</span>    <span class="number">100.000000</span>   <span class="number">12.126500</span>   <span class="number">24.000000</span>  <span class="number">711.000000</span>   <span class="number">22.000000</span>   <span class="number">37.970000</span>   </span><br><span class="line"></span><br><span class="line">             MEDV  </span><br><span class="line">count  <span class="number">506.000000</span>  </span><br><span class="line">mean    <span class="number">22.532806</span>  </span><br><span class="line">std      <span class="number">9.197104</span>  </span><br><span class="line"><span class="built_in">min</span>      <span class="number">5.000000</span>  </span><br><span class="line"><span class="number">25</span>%     <span class="number">17.025000</span>  </span><br><span class="line"><span class="number">50</span>%     <span class="number">21.200000</span>  </span><br><span class="line"><span class="number">75</span>%     <span class="number">25.000000</span>  </span><br><span class="line"><span class="built_in">max</span>     <span class="number">50.000000</span></span><br></pre></td></tr></table></figure>

<p>下面简单介绍一下上面代码中出现的pd.read_csv()方法。其参数如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.read_csv(filepath_or_buffer, sep=<span class="string">&#x27;,&#x27;</span>, delimiter=<span class="literal">None</span>, header=<span class="string">&#x27;infer&#x27;</span>, names=<span class="literal">None</span>, index_col=<span class="literal">None</span>, usecols=<span class="literal">None</span>, squeeze=<span class="literal">False</span>, prefix=<span class="literal">None</span>, mangle_dupe_cols=<span class="literal">True</span>, dtype=<span class="literal">None</span>, engine=<span class="literal">None</span>, converters=<span class="literal">None</span>, true_values=<span class="literal">None</span>, false_values=<span class="literal">None</span>, skipinitialspace=<span class="literal">False</span>, skiprows=<span class="literal">None</span>, nrows=<span class="literal">None</span>, na_values=<span class="literal">None</span>, keep_default_na=<span class="literal">True</span>, na_filter=<span class="literal">True</span>, verbose=<span class="literal">False</span>, skip_blank_lines=<span class="literal">True</span>, parse_dates=<span class="literal">False</span>, infer_datetime_format=<span class="literal">False</span>, keep_date_col=<span class="literal">False</span>, date_parser=<span class="literal">None</span>, dayfirst=<span class="literal">False</span>, iterator=<span class="literal">False</span>, chunksize=<span class="literal">None</span>, compression=<span class="string">&#x27;infer&#x27;</span>, thousands=<span class="literal">None</span>, decimal=<span class="string">b&#x27;.&#x27;</span>, lineterminator=<span class="literal">None</span>, quotechar=<span class="string">&#x27;&quot;&#x27;</span>, quoting=<span class="number">0</span>, escapechar=<span class="literal">None</span>, comment=<span class="literal">None</span>, encoding=<span class="literal">None</span>, dialect=<span class="literal">None</span>, tupleize_cols=<span class="literal">False</span>, error_bad_lines=<span class="literal">True</span>, warn_bad_lines=<span class="literal">True</span>, skipfooter=<span class="number">0</span>, skip_footer=<span class="number">0</span>, doublequote=<span class="literal">True</span>, delim_whitespace=<span class="literal">False</span>, as_recarray=<span class="literal">False</span>, compact_ints=<span class="literal">False</span>, use_unsigned=<span class="literal">False</span>, low_memory=<span class="literal">True</span>, buffer_lines=<span class="literal">None</span>, memory_map=<span class="literal">False</span>, float_precision=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>下面简单介绍一下比较常用的几个参数。<br><strong>filepath_or_buffer</strong>：必需，文件所在处的路径。<br><strong>seq</strong>：指定分割符，默认为逗号“,”。<br><strong>header</strong>：int, list of int, default ‘infer’。指定哪一行作为列名，默认header&#x3D;0。如果人为指定了列名，header&#x3D;None。<br><strong>names</strong>：指定列名，如果文件中不包含标题行，则显式指定header&#x3D;None。</p>
<p>想快速读取常规大小的数据文件时，通过创建读缓存区和其他的机制可能会造成额外的开销。此时建议采用Pandas库来处理。</p>
<h3 id="4-2-3-准备建模"><a href="#4-2-3-准备建模" class="headerlink" title="4.2.3 准备建模"></a>4.2.3 准备建模</h3><p>房价和多个特征变量相关，本讲尝试使用多元线性回归建模。结果可以由不同特征的输入值和对应的权重相乘求和，加上偏置项计算求解。如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter4/%E5%9B%BE4-7.png"><br>由于空间所限，图形中只画出了3个x变量，实际上本案例中是有12个x变量的。<br>其中还用到了矩阵用来简化表示形式。矩阵运算是机器学习的基本手段，必须掌握！</p>
<h2 id="4-3-第一个版本的模型构建"><a href="#4-3-第一个版本的模型构建" class="headerlink" title="4.3 第一个版本的模型构建"></a>4.3 第一个版本的模型构建</h2><h3 id="4-3-1-数据准备与模型定义"><a href="#4-3-1-数据准备与模型定义" class="headerlink" title="4.3.1 数据准备与模型定义"></a>4.3.1 数据准备与模型定义</h3><p>载入所需数据：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取df的值，ndarray类型</span></span><br><span class="line">df = df.values</span><br><span class="line"></span><br><span class="line"><span class="comment"># x_data为前12列特征数据</span></span><br><span class="line">x_data = df[:,:<span class="number">12</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># y_data为最后一列的标签数据</span></span><br><span class="line">y_data = df[:,<span class="number">12</span>]</span><br></pre></td></tr></table></figure>

<p>定义<strong>特征数据</strong>和<strong>标签数据</strong>的占位符：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义特征数据和标签数据的占位符</span></span><br><span class="line"><span class="comment"># 这里的None表示行的数量未知，只指定列的shape</span></span><br><span class="line">x = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">12</span>],name=<span class="string">&#x27;X&#x27;</span>)</span><br><span class="line">y = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">1</span>],name=<span class="string">&#x27;Y&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>shape中<strong>None</strong> 表示行的数量未知，在实际训练时决定一次代入多少行样本，从一个样本的随机SDG到批量SDG都可以。</p>
<p>定义模型结构：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义了一个命名空间</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;Model&#x27;</span>):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># w初始化值为shape=(12,1)的随机数</span></span><br><span class="line">    w = tf.Variable(tf.random_normal([<span class="number">12</span>,<span class="number">1</span>],stddev=<span class="number">0.01</span>),name=<span class="string">&#x27;W&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#  b初始化值为1.0</span></span><br><span class="line">    b = tf.Variable(<span class="number">1.0</span>,name=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># w和x是矩阵相乘，用matmul，不能用multiply或者*</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">model</span>(<span class="params">x,w,b</span>):</span><br><span class="line">        <span class="keyword">return</span> tf.matmul(w,x) + b</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 预测计算操作，前向计算节点</span></span><br><span class="line">    pred = model(x,w,b)</span><br></pre></td></tr></table></figure>
<p><strong>命名空间name_scope</strong>:TensorFlow计算图模型中常有数以千计节点，在可视化过程中很难一下子全部展示出来，因此可用**tf.name_scope()<strong>方法为变量划分范围，在可视化中，这表示在计算图中的一个层级，即将整个命名空间作为一个大的节点。<br>下面介绍一下</strong>tf.random_normal()**方法，该方法从正态分布中输出随机值，其参数如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.random_normal(shape,</span><br><span class="line">                  mean=<span class="number">0.0</span>,</span><br><span class="line">                  stddev=<span class="number">1.0</span>,</span><br><span class="line">                  dtype=dtypes.float32,</span><br><span class="line">                  seed=<span class="literal">None</span>,</span><br><span class="line">                  name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>其返回值为一个<strong>指定形状的张量</strong>，具体参数如下：</p>
<ul>
<li>shape: 输出张量的形状，必选</li>
<li>mean: 正态分布的均值，默认为0.0</li>
<li>stddev: 正态分布的标准差，默认为1.0</li>
<li>dtype: 输出的类型，默认为tf.float32</li>
<li>seed: 随机数种子</li>
<li>name: 操作的名称</li>
</ul>
<p>上面的程序中还用到了tf.matmul()方法，TensorFlow中的**matmul()<strong>方法与</strong>multiply()**方法与numpy中所实现的功能是相同的，multiply()方法实现矩阵的点乘，matmul实现矩阵相乘（叉乘）。只不过tensorflow中的方法针对的是张量（Tensor类型），而numpy中的方法针对的是数组（ndarray类型）。</p>
<h3 id="4-3-2-模型训练"><a href="#4-3-2-模型训练" class="headerlink" title="4.3.2 模型训练"></a>4.3.2 模型训练</h3><p>设置训练超参数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 迭代轮次</span></span><br><span class="line">train_epochs = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 学习率</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br></pre></td></tr></table></figure>

<p>定义均方差损失函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义损失函数，均方误差</span></span><br><span class="line">loss_function = tf.reduce_mean(tf.square(y - pred))</span><br></pre></td></tr></table></figure>

<p>选择优化器：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建梯度下降优化器</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_function)</span><br></pre></td></tr></table></figure>

<p>声明并启动会话：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义初始化变量的操作</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure>

<p>迭代训练：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">loss_sum = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(train_epochs):</span><br><span class="line">    <span class="keyword">for</span> xs,ys <span class="keyword">in</span> <span class="built_in">zip</span>(x_data,y_data):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Feed数据必须和Placeholder的shape一致</span></span><br><span class="line">        xs = xs.reshape(<span class="number">1</span>,<span class="number">12</span>)</span><br><span class="line">        ys = ys.reshape(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        _,loss = sess.run([optimizer,loss_function],feed_dict=&#123;x:xs,y:ys&#125;)</span><br><span class="line">        </span><br><span class="line">        loss_sum = loss_sum + loss</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 打乱数据顺序</span></span><br><span class="line">    x_data,y_data = shuffle(x_data,y_data)</span><br><span class="line">    </span><br><span class="line">    b0temp = b.<span class="built_in">eval</span>(session=sess)</span><br><span class="line">    w0temp = b.<span class="built_in">eval</span>(session=sess)</span><br><span class="line">    loss_average = loss_sum/<span class="built_in">len</span>(y_data)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch= %d,   loss= %f,   b= %f,   w= %f&#x27;</span> % (epoch+<span class="number">1</span>,loss_average,b0temp,w0temp))</span><br></pre></td></tr></table></figure>
<p>上面的训练过程中为什么要<strong>打乱数据顺序</strong>呢？<br>这是为了避免w与b与数据的顺序之间形成关联。比如说有个训练数据集为苹果、梨、橘子。如果一直以这个顺序进行训练，那么训练的结果有可能与数据的顺序对应起来，认为第一个输入的为苹果，第二个输入的为梨。为了避免这种情况，需要<strong>在每一次迭代完成后打乱训练数据</strong>。</p>
<p>下面介绍一下**sklearn.utils.shuffle()**方法，参数如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.utils.shuffle(*arrays, **options)</span><br></pre></td></tr></table></figure>
<p>*array为带索引的序列，可以是arrays，lists，dataframes或者scipy sparse matrices。</p>
<h2 id="4-4-后续版本的持续改进"><a href="#4-4-后续版本的持续改进" class="headerlink" title="4.4 后续版本的持续改进"></a>4.4 后续版本的持续改进</h2><h3 id="4-4-1-版本2：特征数据归一化"><a href="#4-4-1-版本2：特征数据归一化" class="headerlink" title="4.4.1 版本2：特征数据归一化"></a>4.4.1 版本2：特征数据归一化</h3><p>按照上面的操作完成后，开始进行训练，但本次的训练结果出现了一些意外，部分输出如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">epoch= <span class="number">1</span>,   loss= nan,   b= nan,   w= nan</span><br><span class="line">epoch= <span class="number">2</span>,   loss= nan,   b= nan,   w= nan</span><br><span class="line">epoch= <span class="number">3</span>,   loss= nan,   b= nan,   w= nan</span><br><span class="line">epoch= <span class="number">4</span>,   loss= nan,   b= nan,   w= nan</span><br><span class="line">epoch= <span class="number">5</span>,   loss= nan,   b= nan,   w= nan</span><br><span class="line">epoch= <span class="number">6</span>,   loss= nan,   b= nan,   w= nan</span><br><span class="line">epoch= <span class="number">7</span>,   loss= nan,   b= nan,   w= nan</span><br><span class="line">epoch= <span class="number">8</span>,   loss= nan,   b= nan,   w= nan</span><br><span class="line">epoch= <span class="number">9</span>,   loss= nan,   b= nan,   w= nan</span><br><span class="line">epoch= <span class="number">10</span>,   loss= nan,   b= nan,   w= nan</span><br></pre></td></tr></table></figure>
<p>输出都是nan，这是怎么回事呢？<br>在训练的过程中，要考虑不同特征值取值范围的影响。有的特征值可能数值很小，比如只有零点几，但是它对标签的影响却很大；而有的特征值虽然数值很大，比如好几百，但是它对标签的影响却很小。要达到这种效果，与极小特征值相关的权值必须特别大才可以。<br>而我们是采用的梯度下降优化方法进行训练的，误差梯度可在更新中累积，变成非常大的梯度，然后导致网络权重的大幅更新，并因此使网络变得不稳定。在极端情况下，权重的值变得非常大，以至于溢出，导致 NaN 值。 网络层之间的梯度（值大于 1.0）重复相乘导致的指数级增长会产生<strong>梯度爆炸</strong>。</p>
<p>至于本例中的解决办法，就是数据<strong>归一化</strong>，将特征数据<strong>映射到[0~1]区间</strong>。<br>公式如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter4/%E5%9B%BE4-8.png"><br>稍微修改一下代码：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对特征数据（0～11列）做归一化</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">12</span>):</span><br><span class="line">    df[:,i] = (df[:,i] - df[:,i].<span class="built_in">min</span>())/(df[:,i].<span class="built_in">max</span>() - df[:,i].<span class="built_in">min</span>())</span><br><span class="line"></span><br><span class="line"><span class="comment"># x_data为前12列特征数据</span></span><br><span class="line">x_data = df[:,:<span class="number">12</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># y_data为最后一列的标签数据</span></span><br><span class="line">y_data = df[:,<span class="number">12</span>]</span><br></pre></td></tr></table></figure>
<p>注意！<strong>仅对特征数据进行归一化，而不对标签数据进行归一化</strong>。<br>numpy中的max()与min()方法均有如下的两种调用方式：</p>
<ul>
<li>ndarray.min() &#x2F; np.min(ndarray)</li>
<li>ndarray.max() &#x2F; np.max(ndarray)</li>
</ul>
<p>测试模型。由于所有的数据都用来训练了，所以暂时没有新的数据，只能用训练的数据来测试了。代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 任选一条数据来测试</span></span><br><span class="line">n = np.random.randint(<span class="number">506</span>)</span><br><span class="line"><span class="built_in">print</span>(n)</span><br><span class="line">x_test = x_data[n]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 别忘了reshape</span></span><br><span class="line">x_test = x_test.reshape(<span class="number">1</span>,<span class="number">12</span>)</span><br><span class="line">predict = sess.run(pred,feed_dict=&#123;x:x_test&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;预测值：%f&#x27;</span> % predict)</span><br><span class="line"></span><br><span class="line">target = y_data[n]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;标签值：%f&#x27;</span> % target)</span><br></pre></td></tr></table></figure>
<p>其中一部分输出如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">430</span></span><br><span class="line">预测值：<span class="number">13.970398</span></span><br><span class="line">标签值：<span class="number">13.800000</span></span><br><span class="line"></span><br><span class="line"><span class="number">237</span></span><br><span class="line">预测值：<span class="number">25.605413</span></span><br><span class="line">标签值：<span class="number">24.800000</span></span><br></pre></td></tr></table></figure>
<p>也还算可以，房价的影响因素是比较复杂的，用多元线性回归这种简单的模型能做到这种程度也算可以了。</p>
<h3 id="4-4-2-版本2：可视化训练过程中的损失值"><a href="#4-4-2-版本2：可视化训练过程中的损失值" class="headerlink" title="4.4.2 版本2：可视化训练过程中的损失值"></a>4.4.2 版本2：可视化训练过程中的损失值</h3><p>修改训练过程代码，只是每轮训练后添加一个这一轮的loss平均值。<br>代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用于保存loss值的列表</span></span><br><span class="line">loss_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(train_epochs):</span><br><span class="line">    loss_sum = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> xs,ys <span class="keyword">in</span> <span class="built_in">zip</span>(x_data,y_data):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Feed数据必须和Placeholder的shape一致</span></span><br><span class="line">        xs = xs.reshape(<span class="number">1</span>,<span class="number">12</span>)</span><br><span class="line">        ys = ys.reshape(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        _,loss = sess.run([optimizer,loss_function],feed_dict=&#123;x:xs,y:ys&#125;)</span><br><span class="line">        </span><br><span class="line">        loss_sum = loss_sum + loss</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 打乱数据顺序</span></span><br><span class="line">    x_data,y_data = shuffle(x_data,y_data)</span><br><span class="line">    </span><br><span class="line">    b0temp = b.<span class="built_in">eval</span>(session=sess)</span><br><span class="line">    w0temp = w.<span class="built_in">eval</span>(session=sess)</span><br><span class="line">    loss_average = loss_sum/<span class="built_in">len</span>(y_data)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#每轮添加一次</span></span><br><span class="line">    loss_list.append(loss_average)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch=&#x27;</span>,epoch+<span class="number">1</span>,<span class="string">&#x27;loss=&#x27;</span>,loss_average,<span class="string">&#x27;b=&#x27;</span>,b0temp,<span class="string">&#x27;w=&#x27;</span>,w0temp)</span><br><span class="line">    </span><br><span class="line">plt.plot(loss_list)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>绘制图像如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter4/%E5%9B%BE4-9.png"></p>
<p>每步(单个样本)训练后添加这个loss值。<br>代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用于保存loss值的列表</span></span><br><span class="line">loss_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(train_epochs):</span><br><span class="line">    loss_sum = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> xs,ys <span class="keyword">in</span> <span class="built_in">zip</span>(x_data,y_data):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Feed数据必须和Placeholder的shape一致</span></span><br><span class="line">        xs = xs.reshape(<span class="number">1</span>,<span class="number">12</span>)</span><br><span class="line">        ys = ys.reshape(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        _,loss = sess.run([optimizer,loss_function],feed_dict=&#123;x:xs,y:ys&#125;)</span><br><span class="line">        </span><br><span class="line">        loss_sum = loss_sum + loss</span><br><span class="line">           </span><br><span class="line">        <span class="comment">#每轮添加一次</span></span><br><span class="line">        loss_list.append(loss)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 打乱数据顺序</span></span><br><span class="line">    x_data,y_data = shuffle(x_data,y_data)</span><br><span class="line">    </span><br><span class="line">    b0temp = b.<span class="built_in">eval</span>(session=sess)</span><br><span class="line">    w0temp = w.<span class="built_in">eval</span>(session=sess)</span><br><span class="line">    loss_average = loss_sum/<span class="built_in">len</span>(y_data)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch=&#x27;</span>,epoch+<span class="number">1</span>,<span class="string">&#x27;loss=&#x27;</span>,loss_average,<span class="string">&#x27;b=&#x27;</span>,b0temp,<span class="string">&#x27;w=&#x27;</span>,w0temp)</span><br><span class="line">    </span><br><span class="line">plt.plot(loss_list)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>绘制图像如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter4/%E5%9B%BE4-10.png"></p>
<p>这么一对比的话，当然是前一种方法好啦。</p>
<h3 id="4-4-3-版本4：加上TensorBoard可视化代码"><a href="#4-4-3-版本4：加上TensorBoard可视化代码" class="headerlink" title="4.4.3 版本4：加上TensorBoard可视化代码"></a>4.4.3 版本4：加上TensorBoard可视化代码</h3><p>修改代码：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义初始化变量的操作</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 清除default graph和不断增加的节点</span></span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置日志存储目录</span></span><br><span class="line">logdir = <span class="string">&#x27;log/&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置一个操作，用于记录损失值loss，后面在TensorBoard中SCALARS栏可见</span></span><br><span class="line">sum_loss_op = tf.summary.scalar(<span class="string">&#x27;loss&#x27;</span>,loss_function)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 合并所有需要记录的摘要日志文件，方便一次性写入</span></span><br><span class="line">merged = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建摘要writer，将计算图写入摘要文件，后面在TensorBoard中GRAPHS栏可见</span></span><br><span class="line">writer = tf.summary.FileWriter(logdir,sess.graph)</span><br></pre></td></tr></table></figure>

<p>对训练过程中的代码做如下修改：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">_,summary_str,loss = sess.run([optimizer,sum_loss_op,loss_function],feed_dict=&#123;x:xs,y:ys&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将摘要协议缓冲区添加到事件文件中</span></span><br><span class="line">writer.add_summary(summary_str,epoch)</span><br></pre></td></tr></table></figure>
<p>关于上面所用的一些TensorBoard方法以及TensorBoard的一些详细信息，会在后面的章节中进行介绍。<br>loss的图形如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter4/%E5%9B%BE4-11.png"><br>计算图如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter4/%E5%9B%BE4-12.png"></p>
<h1 id="第五章-MNIST手写数字识别：分类应用入门"><a href="#第五章-MNIST手写数字识别：分类应用入门" class="headerlink" title="第五章 MNIST手写数字识别：分类应用入门"></a>第五章 MNIST手写数字识别：分类应用入门</h1><h2 id="5-1-MNIST手写数字识别数据解读"><a href="#5-1-MNIST手写数字识别数据解读" class="headerlink" title="5.1 MNIST手写数字识别数据解读"></a>5.1 MNIST手写数字识别数据解读</h2><h3 id="5-1-1-数据集简介"><a href="#5-1-1-数据集简介" class="headerlink" title="5.1.1 数据集简介"></a>5.1.1 数据集简介</h3><p><strong>MNIST数据集</strong>来自美国国家标准与技术研究所，National Institute of Standards and Technology (NIST)。数据集由来自 250 个不同人手写的数字构成,，其中 50% 是高中学生,，50% 来自人口普查局 (the Census Bureau) 的工作人员。<br>其中，<strong>训练集</strong>中有55000组数据，<strong>验证集</strong>中有5000组数据，<strong>测试集</strong>中有10000组数据。</p>
<p>MNIST数据集可在<a target="_blank" rel="noopener" href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a> 获取。<br>TensorFlow提供了如下的数据集读取方法：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在MNIST_data目录下读取MNIST数据集，采用独热编码</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">&#x27;MNIST_data/&#x27;</span>,one_hot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>MNIST数据集文件在读取时如果指定目录下不存在，则会自动去下载，需等待一定时间。如果已经存在了，则直接读取。<br>我将MNIST数据集放在了MINIS_data目录下，该数据集其实就是如下的四个压缩包：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-1.png"><br>读取MNIST数据集时TensorFlow会自动解压，并将解压后的数据返回。</p>
<p>下面具体了解一下MNIST数据集：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练集train数量：&#x27;</span>,mnist.train.num_examples,</span><br><span class="line">     <span class="string">&#x27;，验证集validation数量：&#x27;</span>,mnist.validation.num_examples,</span><br><span class="line">     <span class="string">&#x27;，测试集test数量：&#x27;</span>,mnist.test.num_examples)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train images shape:&#x27;</span>,mnist.train.images.shape,  <span class="comment"># 训练集图片（特征值）的shape</span></span><br><span class="line">     <span class="string">&#x27;labels shape:&#x27;</span>,mnist.train.labels.shape)  <span class="comment"># 训练集标签的shape</span></span><br></pre></td></tr></table></figure>
<p>输出为：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">训练集train数量： <span class="number">55000</span> ，验证集validation数量： <span class="number">5000</span> ，测试集test数量： <span class="number">10000</span></span><br><span class="line">train images shape: (<span class="number">55000</span>, <span class="number">784</span>) labels shape: (<span class="number">55000</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>可以看到该数据集的数据组数与之前所说的一样。<br>不过下面的输出中为什么训练所用的图片的shape为(55000,784)呢？标签的shape又为什么为(55000,10)呢？<br>其实55000很好理解，就是训练数据集中数据的组数。<br>训练所用的图片是28*28的，因此该图片共有28*28&#x3D;784个像素点。而该图片是由灰度值所表示的，共784个灰度值，因此其shape为(55000,784)。<br>而训练数据集中的标签是采用的<strong>10分类One Hot编码</strong>表示的，因此有10位，所以其shape为(55000,10)。</p>
<p>下面咱们具体看一幅image的数据：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">5</span>]: <span class="built_in">len</span>(mnist.train.images[<span class="number">0</span>])</span><br><span class="line">Out[<span class="number">5</span>]: <span class="number">784</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: mnist.train.images[<span class="number">0</span>].shape</span><br><span class="line">Out[<span class="number">6</span>]: (<span class="number">784</span>,)</span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: mnist.train.images[<span class="number">0</span>]</span><br><span class="line">Out[<span class="number">7</span>]: array([<span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.3803922</span> , <span class="number">0.37647063</span>, <span class="number">0.3019608</span> ,</span><br><span class="line">       <span class="number">0.46274513</span>, <span class="number">0.2392157</span> , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.3529412</span> , <span class="number">0.5411765</span> , <span class="number">0.9215687</span> ,</span><br><span class="line">       <span class="number">0.9215687</span> , <span class="number">0.9215687</span> , <span class="number">0.9215687</span> , <span class="number">0.9215687</span> , <span class="number">0.9215687</span> ,</span><br><span class="line">       <span class="number">0.9843138</span> , <span class="number">0.9843138</span> , <span class="number">0.9725491</span> , <span class="number">0.9960785</span> , <span class="number">0.9607844</span> ,</span><br><span class="line">       <span class="number">0.9215687</span> , <span class="number">0.74509805</span>, <span class="number">0.08235294</span>, <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.54901963</span>,</span><br><span class="line">       <span class="number">0.9843138</span> , <span class="number">0.9960785</span> , <span class="number">0.9960785</span> , <span class="number">0.9960785</span> , <span class="number">0.9960785</span> ,</span><br><span class="line">       <span class="number">0.9960785</span> , <span class="number">0.9960785</span> , <span class="number">0.9960785</span> , <span class="number">0.9960785</span> , <span class="number">0.9960785</span> ,</span><br><span class="line">       <span class="number">0.9960785</span> , <span class="number">0.9960785</span> , <span class="number">0.9960785</span> , <span class="number">0.9960785</span> , <span class="number">0.9960785</span> ,</span><br><span class="line">       <span class="number">0.7411765</span> , <span class="number">0.09019608</span>, <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.8862746</span> , <span class="number">0.9960785</span> , <span class="number">0.81568635</span>,</span><br><span class="line">       <span class="number">0.7803922</span> , <span class="number">0.7803922</span> , <span class="number">0.7803922</span> , <span class="number">0.7803922</span> , <span class="number">0.54509807</span>,</span><br><span class="line">       <span class="number">0.2392157</span> , <span class="number">0.2392157</span> , <span class="number">0.2392157</span> , <span class="number">0.2392157</span> , <span class="number">0.2392157</span> ,</span><br><span class="line">       <span class="number">0.5019608</span> , <span class="number">0.8705883</span> , <span class="number">0.9960785</span> , <span class="number">0.9960785</span> , <span class="number">0.7411765</span> ,</span><br><span class="line">       <span class="number">0.08235294</span>, <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.14901961</span>, <span class="number">0.32156864</span>, <span class="number">0.0509804</span> , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.13333334</span>,</span><br><span class="line">       <span class="number">0.8352942</span> , <span class="number">0.9960785</span> , <span class="number">0.9960785</span> , <span class="number">0.45098042</span>, <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.32941177</span>, <span class="number">0.9960785</span> ,</span><br><span class="line">       <span class="number">0.9960785</span> , <span class="number">0.9176471</span> , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.32941177</span>, <span class="number">0.9960785</span> , <span class="number">0.9960785</span> , <span class="number">0.9176471</span> ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.4156863</span> , <span class="number">0.6156863</span> ,</span><br><span class="line">       <span class="number">0.9960785</span> , <span class="number">0.9960785</span> , <span class="number">0.95294124</span>, <span class="number">0.20000002</span>, <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.09803922</span>, <span class="number">0.45882356</span>, <span class="number">0.8941177</span> , <span class="number">0.8941177</span> ,</span><br><span class="line">       <span class="number">0.8941177</span> , <span class="number">0.9921569</span> , <span class="number">0.9960785</span> , <span class="number">0.9960785</span> , <span class="number">0.9960785</span> ,</span><br><span class="line">       <span class="number">0.9960785</span> , <span class="number">0.94117653</span>, <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.26666668</span>, <span class="number">0.4666667</span> , <span class="number">0.86274517</span>,</span><br><span class="line">       <span class="number">0.9960785</span> , <span class="number">0.9960785</span> , <span class="number">0.9960785</span> , <span class="number">0.9960785</span> , <span class="number">0.9960785</span> ,</span><br><span class="line">       <span class="number">0.9960785</span> , <span class="number">0.9960785</span> , <span class="number">0.9960785</span> , <span class="number">0.9960785</span> , <span class="number">0.5568628</span> ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.14509805</span>, <span class="number">0.73333335</span>,</span><br><span class="line">       <span class="number">0.9921569</span> , <span class="number">0.9960785</span> , <span class="number">0.9960785</span> , <span class="number">0.9960785</span> , <span class="number">0.8745099</span> ,</span><br><span class="line">       <span class="number">0.8078432</span> , <span class="number">0.8078432</span> , <span class="number">0.29411766</span>, <span class="number">0.26666668</span>, <span class="number">0.8431373</span> ,</span><br><span class="line">       <span class="number">0.9960785</span> , <span class="number">0.9960785</span> , <span class="number">0.45882356</span>, <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.4431373</span> , <span class="number">0.8588236</span> , <span class="number">0.9960785</span> , <span class="number">0.9490197</span> , <span class="number">0.89019614</span>,</span><br><span class="line">       <span class="number">0.45098042</span>, <span class="number">0.34901962</span>, <span class="number">0.12156864</span>, <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.7843138</span> , <span class="number">0.9960785</span> , <span class="number">0.9450981</span> ,</span><br><span class="line">       <span class="number">0.16078432</span>, <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.6627451</span> , <span class="number">0.9960785</span> ,</span><br><span class="line">       <span class="number">0.6901961</span> , <span class="number">0.24313727</span>, <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.18823531</span>,</span><br><span class="line">       <span class="number">0.9058824</span> , <span class="number">0.9960785</span> , <span class="number">0.9176471</span> , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.07058824</span>, <span class="number">0.48627454</span>, <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.32941177</span>, <span class="number">0.9960785</span> , <span class="number">0.9960785</span> ,</span><br><span class="line">       <span class="number">0.6509804</span> , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.54509807</span>, <span class="number">0.9960785</span> , <span class="number">0.9333334</span> , <span class="number">0.22352943</span>, <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.8235295</span> , <span class="number">0.9803922</span> , <span class="number">0.9960785</span> ,</span><br><span class="line">       <span class="number">0.65882355</span>, <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.9490197</span> , <span class="number">0.9960785</span> , <span class="number">0.93725497</span>, <span class="number">0.22352943</span>, <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.34901962</span>, <span class="number">0.9843138</span> , <span class="number">0.9450981</span> ,</span><br><span class="line">       <span class="number">0.3372549</span> , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.01960784</span>,</span><br><span class="line">       <span class="number">0.8078432</span> , <span class="number">0.96470594</span>, <span class="number">0.6156863</span> , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.01568628</span>, <span class="number">0.45882356</span>, <span class="number">0.27058825</span>,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ,</span><br><span class="line">       <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        , <span class="number">0.</span>        ], dtype=float32)</span><br></pre></td></tr></table></figure>
<p>由上面的代码可以看出，训练集中一个图像的形状为(784,)，而要想显示图像的话，要用reshape()方法将其形状转化为(28,28)，如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mnist.train.images[<span class="number">0</span>].reshape(<span class="number">28</span>,<span class="number">28</span>)</span><br></pre></td></tr></table></figure>

<p>可视化image：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_image</span>(<span class="params">image</span>):</span><br><span class="line">    plt.imshow(image,cmap=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>上面的代码中调用了**plt.imshow()**函数，下面对这个函数进行简单的介绍。<br>其参数很多，下面对两个比较重要的进行介绍。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(X, cmap=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>参数介绍：<br>X：X变量存储图像，可以是浮点型数组、unit8数组以及PIL图像，如果其为数组，则需满足以下形状：</p>
<ul>
<li>M*N      此时数组必须为浮点型，其中值为该坐标的灰度</li>
<li>M*N*3  RGB（浮点型或者unit8类型）</li>
<li>M*N*4  RGBA（浮点型或者unit8类型）</li>
</ul>
<p>cmap：cmap即colormap（图谱），当cmap&#x3D;’binary’对应的图谱如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-2.png"></p>
<p>调用可视化image的函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_image(mnist.train.images[<span class="number">1</span>].reshape(<span class="number">28</span>,<span class="number">28</span>))</span><br></pre></td></tr></table></figure>
<p>绘制图形如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-3.png"></p>
<h3 id="5-1-2-数据标签与独热编码"><a href="#5-1-2-数据标签与独热编码" class="headerlink" title="5.1.2 数据标签与独热编码"></a>5.1.2 数据标签与独热编码</h3><p>在介绍独热编码之前，我们先来看一下MNIST数据集中的标签。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">8</span>]: mnist.train.labels[<span class="number">1</span>]</span><br><span class="line">Out[<span class="number">8</span>]: array([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>])</span><br></pre></td></tr></table></figure>
<p>可以看到，输出结果并不是一个数，而是有10个元素的数组。这其实就是10分类的独热编码，该例中的独热编码表示3。</p>
<p><strong>独热编码（one hot encoding）</strong>：一种<strong>稀疏向量</strong>，其中<strong>一个元素设为1，所有其他元素均设为0</strong>。<br>独热编码常用于表示拥有<strong>有限个可能值</strong>的字符串或标识符.<br>例如：假设某个植物学数据集记录了15000 个不同的物种，其中每个物种都用独一无二的字符串标识符来表示。在特征工程过程中，可能需要将这些字符串标识符编码为独热向量，<strong>向量的大小为 15000</strong>。</p>
<p>为什么要用独热编码呢？主要有以下原因：<br>利用独热编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。机器学习算法中，特征之间距离的计算或相似度的常用计算方法都是基于欧式空间的，将离散型特征使用独热编码，会让特征之间的距离计算更加合理。</p>
<p>那么该如何从独热编码中取得我们所需要的值呢？<br>可以采用<strong>argmax()函数</strong>，其返回最大值的索引。该函数在Numpy库和TensorFlow中都有，且用法大体相同。不过要注意的一点是Numpy中该函数的返回值为一个数，而TensorFlow中的返回值为一个张量。<br>下面是示例代码：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.argmax(mnist.train.labels[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>输出值为3。</p>
<p>此外，<strong>argmax()函数</strong>还有一个axis参数，这个参数对TensorFlow与Numpy的作用都一样，下面通过代码详细的介绍一下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">2</span>]: <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: a = np.arange(<span class="number">0</span>,<span class="number">10</span>).reshape(<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: a</span><br><span class="line">Out[<span class="number">4</span>]: </span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: np.argmax(a)	<span class="comment"># axis默认为None，输出结果相当于把多维数组平铺后的下标</span></span><br><span class="line">Out[<span class="number">5</span>]: <span class="number">9</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: np.argmax(a,axis=<span class="number">0</span>)	<span class="comment"># 指定axis参数为0，按第一维（行）的元素取值，即同列的每一行</span></span><br><span class="line">Out[<span class="number">6</span>]: array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: np.argmax(a,axis=<span class="number">1</span>)	<span class="comment"># 指定axis为1，按第二维（列）的元素取值，即同行的每一列</span></span><br><span class="line">Out[<span class="number">7</span>]: array([<span class="number">4</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: np.argmax(a,axis=-<span class="number">1</span>)	<span class="comment"># 指定axis为-1，则按第最后维的元素取值</span></span><br><span class="line">Out[<span class="number">8</span>]: array([<span class="number">4</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<p>需要注意的是，TensorFlow中的argmax()函数返回的是张量，如果要获得其值，需要在会话中运行。</p>
<p>值得一提的时，如果使用input_data.read_data_sets()函数读取数据时将one_hot参数设为False，即不启用独热码，那么标签值就以单个数据的形式表示。<br>示例代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在MNIST_data目录下读取MNIST数据集，采用独热编码</span></span><br><span class="line">mnist_no_one_hot = input_data.read_data_sets(<span class="string">&#x27;MNIST_data/&#x27;</span>,one_hot=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mnist_no_one_hot.train.labels[<span class="number">0</span>:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p>输出结果如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">7</span> <span class="number">3</span> <span class="number">4</span> <span class="number">6</span> <span class="number">1</span> <span class="number">8</span> <span class="number">1</span> <span class="number">0</span> <span class="number">9</span> <span class="number">8</span>]</span><br></pre></td></tr></table></figure>

<h3 id="5-1-3-数据集的划分"><a href="#5-1-3-数据集的划分" class="headerlink" title="5.1.3 数据集的划分"></a>5.1.3 数据集的划分</h3><p>构建和训练机器学习模型是希望<strong>对新的数据做出良好预测</strong>。如何去保证训练的实效，可以应对以前未见过的数据呢？<br>一种方法是将数据集分成两个子集：</p>
<ul>
<li><strong>训练集</strong>：用于训练模型的子集</li>
<li><strong>测试集</strong>：用于测试模型的子集</li>
</ul>
<p>如下图所示：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-4.png"><br><strong>测试集</strong>应满足以下两个条件：</p>
<ul>
<li><strong>规模足够大</strong>，可产生具有统计意义的结果</li>
<li><strong>能代表整个数据集</strong>，测试集的特征应该与训练集的特征相同</li>
</ul>
<p>工作流程如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-5.png"><br>使用测试集和训练集来推动模型开发迭代的流程。<br>在每次迭代时，都会对训练数据进行训练并评估测试数据，并以基于测试数据的评估结果为指导来选择和更改各种模型超参数，例如学习速率和特征。这种方法是否存在问题?<br><strong>多次重复执行该流程可能导致模型不知不觉地拟合了特定测试集的特性</strong>。</p>
<p>基于以上的考虑，我们采用了一种新的数据划分方法的。通过将数据集划分为三个子集,可以大幅降低<strong>过拟合</strong>的发生几率：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-6.png"><br>使用<strong>验证集</strong>评估训练集的结果。在模型“通过”验证集之后，使用测试集再次检查评估结果。</p>
<p>新的工作流程如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-7.png"></p>
<p>下面我们看一下MNIST数据集中验证数据与测试数据的情况：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看验证数据集情况</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;validation images:&#x27;</span>,mnist.validation.images.shape,</span><br><span class="line">     <span class="string">&#x27;labels:&#x27;</span>,mnist.validation.labels.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看测试数据集情况</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test images:&#x27;</span>,mnist.test.images.shape,</span><br><span class="line">     <span class="string">&#x27;labels:&#x27;</span>,mnist.test.labels.shape)</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">validation images: (<span class="number">5000</span>, <span class="number">784</span>) labels: (<span class="number">5000</span>, <span class="number">10</span>)</span><br><span class="line">test images: (<span class="number">10000</span>, <span class="number">784</span>) labels: (<span class="number">10000</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<h3 id="5-1-4-数据的批量读取"><a href="#5-1-4-数据的批量读取" class="headerlink" title="5.1.4 数据的批量读取"></a>5.1.4 数据的批量读取</h3><p>实现数据的批量读取有下面的两种方法。<br>第一种是Python的切片，示例代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(mnist.train.labels[<span class="number">0</span>:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]]</span><br></pre></td></tr></table></figure>

<p>第二种方法是使用mnist包提供的<strong>next_batch()方法</strong>，更推荐使用这种方式。<br>示例代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 批量读取数据</span></span><br><span class="line"><span class="comment"># 也可以以mnist.train.next_batch(10)的方式调用，而不用指明batch_size参数</span></span><br><span class="line">batch_image_xs,batch_labels_ys = mnist.train.next_batch(batch_size=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(batch_labels_ys)</span><br></pre></td></tr></table></figure>
<p>上面的<strong>batch_size</strong>参数的含义是批次读取数据的个数。并且<strong>返回值包含两部分</strong>，一部分是图像的数据，一部分是标签的数据，需要分别进行接受。<br>另外，当第二次调用next_batch()时，其会<strong>在上一次读的基础上接着往后读</strong>，直到所有的样本都被取完。然后再取之前会<strong>对数据集先做一次shuffle</strong>，然后再重新读取。<br>输出结果与上一个相同：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]]</span><br></pre></td></tr></table></figure>

<h2 id="5-2-分类模型构建与训练"><a href="#5-2-分类模型构建与训练" class="headerlink" title="5.2 分类模型构建与训练"></a>5.2 分类模型构建与训练</h2><h3 id="5-2-1-模型构建"><a href="#5-2-1-模型构建" class="headerlink" title="5.2.1 模型构建"></a>5.2.1 模型构建</h3><p>定义待输入数据的占位符：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mnist中每张图片中共有28*28=784个像素点</span></span><br><span class="line">x = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">784</span>],name=<span class="string">&#x27;X&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0～9一共10个数字=&gt;10个类别</span></span><br><span class="line">y = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">10</span>],name=<span class="string">&#x27;Y&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>定义模型变量。在本案例中，以正态分布的随机数初始化权重w，以常数0初始化偏置b。<br>代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义变量</span></span><br><span class="line">w = tf.Variable(tf.random_normal([<span class="number">784</span>,<span class="number">10</span>]),name=<span class="string">&#x27;W&#x27;</span>)</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]),name=<span class="string">&#x27;b&#x27;</span>)	<span class="comment"># b的shape为(10,)</span></span><br></pre></td></tr></table></figure>

<p>定义前向计算：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf.matmul的shape为(None,10)，None取决于用于训练的批量的大小</span></span><br><span class="line"><span class="comment"># b的shape为(10,)</span></span><br><span class="line">forward = tf.matmul(x,w) + b</span><br></pre></td></tr></table></figure>
<p>这里的<strong>b会与w的每一行相加</strong>。<br>具体原理类似于下面这张图：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-8.png"></p>
<p>到上一步并不算结束，因为我们这个是分类问题，所以这一步要对计算出的y值进行结果的分类。<br>代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pred = tf.nn.softmax(forward)    <span class="comment"># Softmax分类</span></span><br></pre></td></tr></table></figure>
<p><strong>softmax()函数</strong>会将y值转化为目标属于该10分类中哪一类的<strong>概率值</strong>。</p>
<p>与上一章相比，我们在这一章是从<strong>预测问题</strong>到<strong>分类问题</strong>，从<strong>线性回归</strong>到<strong>逻辑回归</strong>。</p>
<h3 id="5-2-2-逻辑回归"><a href="#5-2-2-逻辑回归" class="headerlink" title="5.2.2 逻辑回归"></a>5.2.2 逻辑回归</h3><p>许多问题的预测结果是一个在连续空间的数值，比如房价预测问题，可以用线性模型来描述：</p>
<blockquote>
<p>Y &#x3D; x<sub>1</sub>*w<sub>1</sub> + x<sub>2</sub>*w<sub>2</sub>  + … + x<sub>n</sub>*w<sub>n</sub>  + b</p>
</blockquote>
<p>但也有很多场景需要输出的是<strong>概率估算值</strong>，例如：</p>
<ul>
<li>根据邮件内容判断是垃圾邮件的可能性</li>
<li>根据医学影像判断肿瘤是恶性的可能性</li>
<li>手写数字分别是 0、1、2、3、4、5、6、7、8、9的可能性（概率）</li>
</ul>
<p>这时需要将预测输出值控制在**[0,1]<strong>区间内。<br><strong>二元分类问题</strong>的目标是正确预测两个可能的标签中的一个，</strong>逻辑回归**(Logistic Regression)可以用于处理这类问题。</p>
<p>那么逻辑回归模型如何确保输出值始终落在 0 和 1 之间呢？<br>Sigmod函数（S型函数）生成的输出值正好具有这些特性，其定义如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-9.png"><br>图像如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-10.png"><br>该函数具有如下特点：</p>
<ul>
<li>定义域为全体实数，值域在[0,1]之间</li>
<li>Z值在0点对应的结果为0.5</li>
<li>sigmoid函数连续可微分</li>
</ul>
<p>可用其对线性模型的结果做进一步处理，从而输出介于0到1之间的概率值。<br>如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-11.png"></p>
<p>逻辑回归中的损失函数该如何定义呢？<br>前面线性回归的损失函数是<strong>平方损失</strong>，如果逻辑回归的损失函数也定义为<strong>平方损失</strong>，那么：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-12.png"><br>将Sigmoid函数带入上述函数可得下面的图像：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-13.png"><br>可以看到，该函数为非凸函数，有多个极小值。如果采用梯度下降法，会容易导致陷入局部最优解中。</p>
<p>二元逻辑回归的损失函数一般采用<strong>对数损失函数</strong>，定义如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-14.png"><br>可以看到，这时候这个函数就为凸函数了，明显要比平方损失函数要好。</p>
<h3 id="5-2-3-多元分类和Softmax"><a href="#5-2-3-多元分类和Softmax" class="headerlink" title="5.2.3 多元分类和Softmax"></a>5.2.3 多元分类和Softmax</h3><p><strong>逻辑回归</strong>可生成介于0到1.0之间的小数。<br><strong>Softmax</strong>将这一想法延伸到<strong>多类别</strong>领域。<br>在多类别问题中,Softmax 会<strong>为每个类别分配一个用小数表示的概率</strong>。这些用小数表示的<strong>概率相加之和必须是 1.0</strong>。<br>示例如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-15.png"></p>
<p>本章就是用到了Softmax来实现手写数字识别的问题。下面看一下神经网络中的Softmax层：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-16.png"><br>Softmax的方程如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-17.png"><br>分母是对所有的y计算e的y次方，然后再求和。这样就能保证概率相加之和为1了。<br>此公式本质上是将逻辑回归公式延伸到了多类别。</p>
<p>下面介绍一下交叉熵损失函数。<br><strong>交叉熵</strong>是一个信息论中的概念，它原来是用来估算平均编码长度的。给定两个概率分布p和q，通过q来表示p的交叉熵为：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-18.png"><br>这里的log实际上就是数学上的lg，计算机里面经常这样表示……<br>交叉熵刻画的是<strong>两个概率分布之间的距离</strong>，p代表正确答案，q代表的是预测值，交叉熵越小，两个概率的分布越接近。<br>交叉熵计算示例：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E8%A1%A5%E5%85%85.png"><br>可以看到，交叉熵运算时是对应元素先进行运算，然后再进行求和。</p>
<p>在了解了交叉熵的概念后，可以开始定义交叉熵损失函数了。交叉熵损失函数定义如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-19.png"><br>代码实现如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 交叉熵，指定轴axis为1，计算张量沿着轴1的和</span></span><br><span class="line">loss_function = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred),axis=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>需要注意的一点是，<strong>tf.reduce_mean()函数</strong>和<strong>tf.reduce_sum()函数</strong>默认情况下输出结果是<strong>降维</strong>的。<br>这二个函数的参数是一样的，下面介绍一下<strong>tf.reduce_sum()函数</strong>的参数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.reduce_sum(</span><br><span class="line">    input_tensor, </span><br><span class="line">    axis=<span class="literal">None</span>, </span><br><span class="line">    keepdims=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>input_tensor：待计算的向量。<br>axis：指定的轴，如果不指定，<strong>默认对所有元素进行计算</strong>。<br>keepdims：是否保持原有张量的维度。<strong>默认为False，即对计算结果进行降维</strong>。<br>name：操作的名称。</p>
<p>下面是示例代码：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],</span><br><span class="line">                [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],</span><br><span class="line">                [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]],dtype=tf.int32,name=<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">sum_a = tf.reduce_sum(a)    <span class="comment"># 未指定轴，计算所有元素的和，默认降维</span></span><br><span class="line">sum_axis1_a = tf.reduce_sum(a,axis=<span class="number">1</span>)   <span class="comment"># 指定计算轴1的和，默认降维</span></span><br><span class="line">mean = tf.reduce_mean(sum_axis1_a)    <span class="comment"># 未指定轴，计算所有元素的均值，默认降维</span></span><br><span class="line">mean_a = tf.reduce_mean(a)    <span class="comment"># 未指定轴，计算所有元素的均值，默认降维</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="built_in">print</span>(sess.run(sum_a),sum_a.shape)</span><br><span class="line">    <span class="built_in">print</span>(sess.run(sum_axis1_a),sum_axis1_a.shape)</span><br><span class="line">    <span class="built_in">print</span>(sess.run(mean),mean.shape)</span><br><span class="line">    <span class="built_in">print</span>(sess.run(mean_a),mean_a.shape)</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">45</span> ()	<span class="comment"># a中所有元素的和</span></span><br><span class="line">[<span class="number">15</span> <span class="number">15</span> <span class="number">15</span>] (<span class="number">3</span>,)	<span class="comment"># 按a轴1计算的和，即每一行的和</span></span><br><span class="line"><span class="number">15</span> ()	<span class="comment"># 对a每一行的和计算均值</span></span><br><span class="line"><span class="number">3</span> ()		<span class="comment"># a中所有元素的均值</span></span><br></pre></td></tr></table></figure>

<h3 id="5-2-4-分类模型构建与训练实践"><a href="#5-2-4-分类模型构建与训练实践" class="headerlink" title="5.2.4 分类模型构建与训练实践"></a>5.2.4 分类模型构建与训练实践</h3><p>设置训练参数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置训练参数</span></span><br><span class="line">train_epochs = <span class="number">50</span>    <span class="comment"># 训练轮数</span></span><br><span class="line">batch_size = <span class="number">100</span>    <span class="comment"># 单次训练样本数（批次大小）</span></span><br><span class="line">total_batch = <span class="built_in">int</span>(mnist.train.num_examples / batch_size)    <span class="comment"># 一轮训练有多少批次</span></span><br><span class="line">display_step = <span class="number">1</span>    <span class="comment"># 显示粒度</span></span><br><span class="line">learning_rate = <span class="number">0.01</span>    <span class="comment"># 学习率</span></span><br></pre></td></tr></table></figure>

<p>选择优化器：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#选择优化器</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_function)</span><br></pre></td></tr></table></figure>

<p>定义准确率：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查预测类别tf.argmax(pred,axis=1)与实际类别tf.argmax(y,axis=1)的匹配情况</span></span><br><span class="line"><span class="comment"># 相等返回True，否则返回False</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(pred,axis=<span class="number">1</span>),tf.argmax(y,axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准确率，将布尔值转化为浮点数，并计算平均值</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br></pre></td></tr></table></figure>
<p>上面代码中<strong>tf.cast()函数</strong>的作用是实现数据类型转换。参数如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># x为待转换的数据（张量）</span></span><br><span class="line"><span class="comment"># dtype为目标数据类型</span></span><br><span class="line">tf.cast(x, dtype, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>True转化为浮点数结果为1.0，False转化为浮点数结果为0.0。</p>
<p>声明会话，初始化变量：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()    <span class="comment"># 声明会话</span></span><br><span class="line">init = tf.global_variables_initializer()    <span class="comment"># 变量初始化</span></span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure>

<p>训练模型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(train_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> <span class="built_in">range</span>(total_batch):</span><br><span class="line">        xs,ys = mnist.train.next_batch(batch_size)   <span class="comment"># 读取批次数据</span></span><br><span class="line">        sess.run(optimizer,feed_dict=&#123;x:xs,y:ys&#125;)   <span class="comment"># 执行批次训练</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># toatl_batch个批次训练完成后，使用验证数据计算误差与准确率，验证集没有分批</span></span><br><span class="line">    loss,acc = sess.run([loss_function,accuracy],</span><br><span class="line">                        feed_dict=&#123;x:mnist.validation.images,y:mnist.validation.labels&#125;)</span><br><span class="line">    <span class="comment"># 打印训练过程中的详细信息</span></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Train Epoch: %02d,  Loss= %.9f,  Accurary= %.4f&#x27;</span></span><br><span class="line">             % (epoch + <span class="number">1</span>,loss,acc))</span><br><span class="line">        </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Train Finished!&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">Train Epoch: 01,  Loss= <span class="number">4.879797935</span>,  Accurary= <span class="number">0.3038</span></span><br><span class="line">Train Epoch: 02,  Loss= <span class="number">3.045671225</span>,  Accurary= <span class="number">0.4814</span></span><br><span class="line">Train Epoch: 03,  Loss= <span class="number">2.285549641</span>,  Accurary= <span class="number">0.5728</span></span><br><span class="line">Train Epoch: 04,  Loss= <span class="number">1.876547456</span>,  Accurary= <span class="number">0.6324</span></span><br><span class="line">Train Epoch: 05,  Loss= <span class="number">1.623455882</span>,  Accurary= <span class="number">0.6726</span></span><br><span class="line">Train Epoch: 06,  Loss= <span class="number">1.451602697</span>,  Accurary= <span class="number">0.7018</span></span><br><span class="line">Train Epoch: 07,  Loss= <span class="number">1.329810500</span>,  Accurary= <span class="number">0.7232</span></span><br><span class="line">Train Epoch: 08,  Loss= <span class="number">1.233531237</span>,  Accurary= <span class="number">0.7424</span></span><br><span class="line">Train Epoch: 09,  Loss= <span class="number">1.159393549</span>,  Accurary= <span class="number">0.7574</span></span><br><span class="line">Train Epoch: <span class="number">10</span>,  Loss= <span class="number">1.099836111</span>,  Accurary= <span class="number">0.7688</span></span><br><span class="line">Train Epoch: <span class="number">11</span>,  Loss= <span class="number">1.049868107</span>,  Accurary= <span class="number">0.7794</span></span><br><span class="line">Train Epoch: <span class="number">12</span>,  Loss= <span class="number">1.007820368</span>,  Accurary= <span class="number">0.7840</span></span><br><span class="line">Train Epoch: <span class="number">13</span>,  Loss= <span class="number">0.971541822</span>,  Accurary= <span class="number">0.7926</span></span><br><span class="line">Train Epoch: <span class="number">14</span>,  Loss= <span class="number">0.940058231</span>,  Accurary= <span class="number">0.7996</span></span><br><span class="line">Train Epoch: <span class="number">15</span>,  Loss= <span class="number">0.911968648</span>,  Accurary= <span class="number">0.8044</span></span><br><span class="line">Train Epoch: <span class="number">16</span>,  Loss= <span class="number">0.887769699</span>,  Accurary= <span class="number">0.8090</span></span><br><span class="line">Train Epoch: <span class="number">17</span>,  Loss= <span class="number">0.865332007</span>,  Accurary= <span class="number">0.8130</span></span><br><span class="line">Train Epoch: <span class="number">18</span>,  Loss= <span class="number">0.845306814</span>,  Accurary= <span class="number">0.8164</span></span><br><span class="line">Train Epoch: <span class="number">19</span>,  Loss= <span class="number">0.827485740</span>,  Accurary= <span class="number">0.8202</span></span><br><span class="line">Train Epoch: <span class="number">20</span>,  Loss= <span class="number">0.810428739</span>,  Accurary= <span class="number">0.8242</span></span><br><span class="line">Train Epoch: <span class="number">21</span>,  Loss= <span class="number">0.795368671</span>,  Accurary= <span class="number">0.8256</span></span><br><span class="line">Train Epoch: <span class="number">22</span>,  Loss= <span class="number">0.781131923</span>,  Accurary= <span class="number">0.8306</span></span><br><span class="line">Train Epoch: <span class="number">23</span>,  Loss= <span class="number">0.768008232</span>,  Accurary= <span class="number">0.8332</span></span><br><span class="line">Train Epoch: <span class="number">24</span>,  Loss= <span class="number">0.755907059</span>,  Accurary= <span class="number">0.8346</span></span><br><span class="line">Train Epoch: <span class="number">25</span>,  Loss= <span class="number">0.745053411</span>,  Accurary= <span class="number">0.8362</span></span><br><span class="line">Train Epoch: <span class="number">26</span>,  Loss= <span class="number">0.733891785</span>,  Accurary= <span class="number">0.8374</span></span><br><span class="line">Train Epoch: <span class="number">27</span>,  Loss= <span class="number">0.723994493</span>,  Accurary= <span class="number">0.8398</span></span><br><span class="line">Train Epoch: <span class="number">28</span>,  Loss= <span class="number">0.714434564</span>,  Accurary= <span class="number">0.8412</span></span><br><span class="line">Train Epoch: <span class="number">29</span>,  Loss= <span class="number">0.705953479</span>,  Accurary= <span class="number">0.8442</span></span><br><span class="line">Train Epoch: <span class="number">30</span>,  Loss= <span class="number">0.697211921</span>,  Accurary= <span class="number">0.8474</span></span><br><span class="line">Train Epoch: <span class="number">31</span>,  Loss= <span class="number">0.689035177</span>,  Accurary= <span class="number">0.8490</span></span><br><span class="line">Train Epoch: <span class="number">32</span>,  Loss= <span class="number">0.681854963</span>,  Accurary= <span class="number">0.8484</span></span><br><span class="line">Train Epoch: <span class="number">33</span>,  Loss= <span class="number">0.674482226</span>,  Accurary= <span class="number">0.8496</span></span><br><span class="line">Train Epoch: <span class="number">34</span>,  Loss= <span class="number">0.667658150</span>,  Accurary= <span class="number">0.8512</span></span><br><span class="line">Train Epoch: <span class="number">35</span>,  Loss= <span class="number">0.661404192</span>,  Accurary= <span class="number">0.8514</span></span><br><span class="line">Train Epoch: <span class="number">36</span>,  Loss= <span class="number">0.654845059</span>,  Accurary= <span class="number">0.8534</span></span><br><span class="line">Train Epoch: <span class="number">37</span>,  Loss= <span class="number">0.648665011</span>,  Accurary= <span class="number">0.8546</span></span><br><span class="line">Train Epoch: <span class="number">38</span>,  Loss= <span class="number">0.642798781</span>,  Accurary= <span class="number">0.8550</span></span><br><span class="line">Train Epoch: <span class="number">39</span>,  Loss= <span class="number">0.637200952</span>,  Accurary= <span class="number">0.8558</span></span><br><span class="line">Train Epoch: <span class="number">40</span>,  Loss= <span class="number">0.631890357</span>,  Accurary= <span class="number">0.8572</span></span><br><span class="line">Train Epoch: <span class="number">41</span>,  Loss= <span class="number">0.626989901</span>,  Accurary= <span class="number">0.8580</span></span><br><span class="line">Train Epoch: <span class="number">42</span>,  Loss= <span class="number">0.622284353</span>,  Accurary= <span class="number">0.8588</span></span><br><span class="line">Train Epoch: <span class="number">43</span>,  Loss= <span class="number">0.616930246</span>,  Accurary= <span class="number">0.8598</span></span><br><span class="line">Train Epoch: <span class="number">44</span>,  Loss= <span class="number">0.612941027</span>,  Accurary= <span class="number">0.8614</span></span><br><span class="line">Train Epoch: <span class="number">45</span>,  Loss= <span class="number">0.608228505</span>,  Accurary= <span class="number">0.8620</span></span><br><span class="line">Train Epoch: <span class="number">46</span>,  Loss= <span class="number">0.603960276</span>,  Accurary= <span class="number">0.8638</span></span><br><span class="line">Train Epoch: <span class="number">47</span>,  Loss= <span class="number">0.599614084</span>,  Accurary= <span class="number">0.8646</span></span><br><span class="line">Train Epoch: <span class="number">48</span>,  Loss= <span class="number">0.595651090</span>,  Accurary= <span class="number">0.8652</span></span><br><span class="line">Train Epoch: <span class="number">49</span>,  Loss= <span class="number">0.591695189</span>,  Accurary= <span class="number">0.8662</span></span><br><span class="line">Train Epoch: <span class="number">50</span>,  Loss= <span class="number">0.587703526</span>,  Accurary= <span class="number">0.8664</span></span><br><span class="line">Train Finished!</span><br></pre></td></tr></table></figure>
<p>可以看出，损失值Loss是越来越小的，同时，准确率Accurary越来越高。</p>
<p>评估模型。完成训练后，在<strong>测试集</strong>上评估模型的准确率：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">accu_test = sess.run(accuracy,</span><br><span class="line">                     feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Test Accuracy&#x27;</span>,accu_test)</span><br></pre></td></tr></table></figure>
<p>输出如下</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Test Accuracy <span class="number">0.8683</span></span><br></pre></td></tr></table></figure>

<h3 id="5-2-5-预测结果的可视化"><a href="#5-2-5-预测结果的可视化" class="headerlink" title="5.2.5 预测结果的可视化"></a>5.2.5 预测结果的可视化</h3><p>在建立模型并进行训练后，若认为准确率可以接受，则可以使用此模型进行预测。<br>代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 由于pred预测结果是one hot编码格式，所以需要转换为0～9的数字</span></span><br><span class="line">prediction_result = sess.run(tf.argmax(pred,axis=<span class="number">1</span>),</span><br><span class="line">                             feed_dict=&#123;x:mnist.test.images&#125;)</span><br><span class="line"><span class="comment"># 查看预测结果中的前10项</span></span><br><span class="line">prediction_result[<span class="number">0</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<p>输出结果如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([<span class="number">7</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">6</span>, <span class="number">9</span>])</span><br></pre></td></tr></table></figure>
<p>这种做法非常的不直观，而且也不知道预测出来的结果是对是错。能不能有一种更直观的表示方法呢？</p>
<p>下面我们对预测结果做可视化处理。<br>定义函数如下</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_images_labels_prediction</span>(<span class="params">images,    <span class="comment"># 图像列表</span></span></span><br><span class="line"><span class="params">                                 labels,     <span class="comment"># 标签列表</span></span></span><br><span class="line"><span class="params">                                 prediction, <span class="comment"># 预测值列表   </span></span></span><br><span class="line"><span class="params">                                 index,      <span class="comment"># 从第index个开始显示</span></span></span><br><span class="line"><span class="params">                                 num=<span class="number">10</span></span>):    <span class="comment"># 一次显示num幅图片</span></span><br><span class="line">    fig = plt.gcf()    <span class="comment"># 获取当前图表，Get Current Figure</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 设置图表大小，两个参数分别为宽和高，单位为英寸，1英寸等于2.54cm</span></span><br><span class="line">    fig.set_size_inches(<span class="number">10</span>,<span class="number">12</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> num &gt; <span class="number">25</span>:</span><br><span class="line">        num = <span class="number">25</span>    <span class="comment"># 最多显示25个子图</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,num):</span><br><span class="line">        <span class="comment"># 将当前的图表分为5行5列，共计25个子图，并定位到其中的第i+1个子图</span></span><br><span class="line">        ax = plt.subplot(<span class="number">5</span>,<span class="number">5</span>,i+<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 显示第index个图像</span></span><br><span class="line">        ax.imshow(images[index].reshape(<span class="number">28</span>,<span class="number">28</span>),cmap=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 构建该图上要显示的title信息，这里的labels[index]为一维数组，所以不需要指定axis</span></span><br><span class="line">        title = <span class="string">&#x27;label=&#x27;</span> + <span class="built_in">str</span>(np.argmax(labels[index]))</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(prediction) &gt; <span class="number">0</span>:    <span class="comment"># 如果预测值不为空</span></span><br><span class="line">            title += <span class="string">&#x27;, predict=&#x27;</span> + <span class="built_in">str</span>(prediction[index])    <span class="comment"># 加入预测值信息</span></span><br><span class="line">        </span><br><span class="line">        ax.set_title(title,fontsize=<span class="number">10</span>)    <span class="comment"># 显示图上的title信息</span></span><br><span class="line">        ax.set_xticks([])    <span class="comment"># 不显示坐标轴</span></span><br><span class="line">        ax.set_yticks([])</span><br><span class="line">        index += <span class="number">1</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>调用该函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plot_images_labels_prediction(mnist.test.images,</span><br><span class="line">                             mnist.test.labels,</span><br><span class="line">                             prediction_result,<span class="number">0</span>,<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>输出如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter5/%E5%9B%BE5-20.png"></p>
<h1 id="第六章-MNIST手写数字识别进阶：多层神经网络与应用"><a href="#第六章-MNIST手写数字识别进阶：多层神经网络与应用" class="headerlink" title="第六章 MNIST手写数字识别进阶：多层神经网络与应用"></a>第六章 MNIST手写数字识别进阶：多层神经网络与应用</h1><h2 id="6-1-单隐藏层神经网络构建与应用"><a href="#6-1-单隐藏层神经网络构建与应用" class="headerlink" title="6.1 单隐藏层神经网络构建与应用"></a>6.1 单隐藏层神经网络构建与应用</h2><h3 id="6-1-1-从单神经元到全连接神经网络"><a href="#6-1-1-从单神经元到全连接神经网络" class="headerlink" title="6.1.1 从单神经元到全连接神经网络"></a>6.1.1 从单神经元到全连接神经网络</h3><p>在上一章中，我们介绍了如何用一个神经元来处理分类问题，所采用的模型如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter6/%E5%9B%BE6-1.png"><br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter6/%E5%9B%BE6-2.png"><br>这种采用单个神经元来识别手写数字的方式，最终的准确率能够达到90%左右。那么我们能不能进一步的提高准确率呢？<br>下面我们尝试构建如下的单隐藏层全连接神经网络：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter6/%E5%9B%BE6-3.png"></p>
<p>载入数据：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取MNIST数据</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">&#x27;MNIST_data/&#x27;</span>,one_hot = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>构建输入层：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义训练数据占位符</span></span><br><span class="line">x = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">784</span>],name=<span class="string">&#x27;X&#x27;</span>)</span><br><span class="line"><span class="comment"># 定义标签数据占位符</span></span><br><span class="line">y = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">10</span>],name=<span class="string">&#x27;Y&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>构建隐藏层：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 隐藏层神经元数量</span></span><br><span class="line">H1_NN = <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># w1的前一层为784个神经元，后一层为H1_NN个神经元，所以w1的shape为[784,H1_NN]</span></span><br><span class="line">w1 = tf.Variable(tf.truncated_normal([<span class="number">784</span>,H1_NN],stddev=<span class="number">0.1</span>))</span><br><span class="line"><span class="comment"># b1的后一层为H1_NN个神经元，所以b1的shape为(H1_NN,)</span></span><br><span class="line">b1 = tf.Variable(tf.zeros([H1_NN]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 采用relu激活函数</span></span><br><span class="line">y1 = tf.nn.relu(tf.matmul(x,w1) + b1)</span><br></pre></td></tr></table></figure>
<p>上面的代码中用到了一个<strong>tf.truncated_normal()函数</strong>，该函数参数与<strong>tf..random_normal()函数</strong>参数相同。有一点不同的是该函数产生<strong>截断正态分布随机数</strong>，即随机数与均值的差值若大于两倍的标准差，则重新生成。<br>采用这种截断式的随机数做初始化要比完全随机要好。</p>
<p>构建输出层：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># w2的前一层有H1_NN个神经元，后一层有10个神经元，所以w2的shape为(H1_NN,10)</span></span><br><span class="line">w2 = tf.Variable(tf.truncated_normal([H1_NN,<span class="number">10</span>]))</span><br><span class="line"><span class="comment"># b2的后一层有10个神经元，所以b2的shape为(10,)</span></span><br><span class="line">b2 = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果层的前向计算值</span></span><br><span class="line">forward = tf.matmul(y1,w2) + b2</span><br><span class="line"><span class="comment"># 由于是10分类问题，所以最后的激活函数应当用softmax函数</span></span><br><span class="line">pred = tf.nn.softmax(forward)</span><br></pre></td></tr></table></figure>

<p>定义损失函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 交叉熵</span></span><br><span class="line">loss_function = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred),axis=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<p>设置训练参数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置训练参数</span></span><br><span class="line">train_epochs = <span class="number">40</span>    <span class="comment"># 训练轮数</span></span><br><span class="line">batch_size = <span class="number">50</span>    <span class="comment"># 单次训练样本数（批次大小）</span></span><br><span class="line">total_batch = <span class="built_in">int</span>(mnist.train.num_examples / batch_size)    <span class="comment"># 一轮训练有多少批次</span></span><br><span class="line">display_step = <span class="number">1</span>    <span class="comment"># 显示粒度</span></span><br><span class="line">learning_rate = <span class="number">0.01</span>    <span class="comment"># 学习率</span></span><br></pre></td></tr></table></figure>

<p>选择优化器：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 采用Adam优化器</span></span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss_function)</span><br></pre></td></tr></table></figure>
<p>关于TensorFlow中优化器介绍，可以看这篇博客：<a target="_blank" rel="noopener" href="https://blog.csdn.net/junchengberry/article/details/81102058">https://blog.csdn.net/junchengberry/article/details/81102058</a> 。<br>一般还是Adam优化器用的比较多。</p>
<p>定义准确率：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查预测类别tf.argmax(pred,axis=1)与实际类别tf.argmax(y,axis=1)的匹配情况</span></span><br><span class="line"><span class="comment"># 相等返回True，否则返回False</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(pred,axis=<span class="number">1</span>),tf.argmax(y,axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准确率，将布尔值转化为浮点数，并计算平均值</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br></pre></td></tr></table></figure>

<p>训练模型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 记录训练开始时间</span></span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line">start_time = time()</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(train_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> <span class="built_in">range</span>(total_batch):</span><br><span class="line">        xs,ys = mnist.train.next_batch(batch_size)</span><br><span class="line">        sess.run(optimizer,feed_dict=&#123;x:xs,y:ys&#125;)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># total_batch个批次训练完成后，使用验证数据计算误差与准确率</span></span><br><span class="line">    loss,acc = sess.run([loss_function,accuracy],</span><br><span class="line">                        feed_dict=&#123;x:mnist.validation.images,y:mnist.validation.labels&#125;)</span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Train Epoch: %02d,  Loss= %.9f,  Accurary= %.4f&#x27;</span></span><br><span class="line">             % (epoch + <span class="number">1</span>,loss,acc))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示运行总时间</span></span><br><span class="line">duration = time() - strat_time</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Train Finished takes: %.2f&#x27;</span> % (duration))</span><br></pre></td></tr></table></figure>
<p>部分训练结果如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Train Epoch: 01,  Loss= nan,  Accurary= <span class="number">0.0958</span></span><br><span class="line">Train Epoch: 02,  Loss= nan,  Accurary= <span class="number">0.0958</span></span><br><span class="line">Train Epoch: 03,  Loss= nan,  Accurary= <span class="number">0.0958</span></span><br><span class="line">Train Epoch: 04,  Loss= nan,  Accurary= <span class="number">0.0958</span></span><br><span class="line">Train Epoch: 05,  Loss= nan,  Accurary= <span class="number">0.0958</span></span><br></pre></td></tr></table></figure>
<p>很显然这不是我们想要的结果，肯定有哪个地方出现了问题。那么是哪出现了问题呢？<br>原因出自损失函数，我们的损失函数是如下定义的：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 交叉熵</span></span><br><span class="line">loss_function = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred),axis=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>注意里面有个log(pred)，在计算时有可能出现<strong>log(0)<strong>，因此出现问题的主要原因就是</strong>log(0)引起的数据不稳定</strong>。</p>
<p>TensorFlow中提供了结合softmax的交叉熵函数定义方法，使用此方法，可以避免log(0)的影响。<br>代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TensorFlow提供了softmax_cross_entropy_with_logits函数</span></span><br><span class="line"><span class="comment"># 用于避免因为log(0)值为nan造成的数据不稳定</span></span><br><span class="line">loss_function = tf.reduce_mean(</span><br><span class="line">    tf.nn.softmax_cross_entropy_with_logits(logits=forward,labels=y))</span><br></pre></td></tr></table></figure>
<p><strong>tf.nn.softmax_cross_entropy_with_logits()函数</strong>有如下几个参数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>logits：神经网络最后一层的输出，注意是<strong>未经softmax激活函数处理的输出</strong>，也就是上面代码中的forward，而不是pred。</li>
<li>labels：实际的标签，应与logits的shape相同</li>
<li>name：操作名称</li>
</ul>
<p>我们更换损失函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TensorFlow提供了softmax_cross_entropy_with_logits函数</span></span><br><span class="line"><span class="comment"># 用于避免因为log(0)值为nan造成的数据不稳定</span></span><br><span class="line">loss_function = tf.reduce_mean(</span><br><span class="line">    tf.nn.softmax_cross_entropy_with_logits(logits=forward,labels=y))</span><br></pre></td></tr></table></figure>
<p>需要注意的是，tf.nn.softmax_cross_entropy_with_logits()函数的返回值是一个<strong>张量</strong>，<strong>要求损失函数的话，还要再对其求一步均值</strong>。<br>再运行一次看看，部分结果如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Train Epoch: 01,  Loss= <span class="number">0.187707111</span>,  Accurary= <span class="number">0.9472</span></span><br><span class="line">Train Epoch: 02,  Loss= <span class="number">0.171345010</span>,  Accurary= <span class="number">0.9570</span></span><br><span class="line">Train Epoch: 03,  Loss= <span class="number">0.147639647</span>,  Accurary= <span class="number">0.9612</span></span><br><span class="line">Train Epoch: 04,  Loss= <span class="number">0.153220892</span>,  Accurary= <span class="number">0.9660</span></span><br><span class="line">Train Epoch: 05,  Loss= <span class="number">0.148393914</span>,  Accurary= <span class="number">0.9648</span></span><br></pre></td></tr></table></figure>
<p>可以看到，结果正常了，并且准确率也比单个神经元提高了不少。<br>采用测试集评估模型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">accu_test = sess.run(accuracy,</span><br><span class="line">                     feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Test Accuracy:&#x27;</span>,accu_test)</span><br></pre></td></tr></table></figure>
<p>输出为：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Test Accuracy: <span class="number">0.9688</span></span><br></pre></td></tr></table></figure>
<p>单层全连接神经网络最后的准确率能达到97%左右。</p>
<h3 id="6-1-2-模型应用"><a href="#6-1-2-模型应用" class="headerlink" title="6.1.2 模型应用"></a>6.1.2 模型应用</h3><p>进行预测：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 由于是预测，所以只需要用到x</span></span><br><span class="line">prediction_result = sess.run(tf.argmax(pred,axis=<span class="number">1</span>),</span><br><span class="line">                            feed_dict=&#123;x:mnist.test.images&#125;)</span><br><span class="line"><span class="comment"># 查看预测结果的前10项</span></span><br><span class="line">prediction_result[<span class="number">0</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([<span class="number">7</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">6</span>, <span class="number">9</span>])</span><br></pre></td></tr></table></figure>

<p>找出预测错误：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测值与标签值的比较结果列表</span></span><br><span class="line"><span class="comment"># 这里的操作对象为数组，因此要用numpy库中的函数，而不是TensorFlow库中的函数</span></span><br><span class="line">compare_lists = np.equal(prediction_result,</span><br><span class="line">                         np.argmax(mnist.test.labels,axis=<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(compare_lists,<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># err_lists列表中存放着预测错误的下标</span></span><br><span class="line">err_lists = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(compare_lists)) <span class="keyword">if</span> compare_lists[i] == <span class="literal">False</span>]</span><br><span class="line"><span class="built_in">print</span>(err_lists,<span class="built_in">len</span>(err_lists))</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[ <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span> ...  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span>] </span><br><span class="line"></span><br><span class="line">[<span class="number">8</span>, <span class="number">18</span>, <span class="number">33</span>, <span class="number">61</span>, <span class="number">65</span>, <span class="number">149</span>, <span class="number">151</span>, <span class="number">193</span>, <span class="number">245</span>, <span class="number">247</span>, <span class="number">259</span>, <span class="number">320</span>, <span class="number">321</span>, <span class="number">352</span>, <span class="number">381</span>, <span class="number">403</span>, <span class="number">445</span>, <span class="number">447</span>, <span class="number">448</span>, <span class="number">495</span>, <span class="number">543</span>, <span class="number">552</span>, <span class="number">582</span>, <span class="number">610</span>, <span class="number">619</span>, <span class="number">646</span>, <span class="number">674</span>, <span class="number">685</span>, <span class="number">691</span>, <span class="number">707</span>, <span class="number">720</span>, <span class="number">726</span>, <span class="number">760</span>, <span class="number">844</span>, <span class="number">846</span>, <span class="number">866</span>, <span class="number">883</span>, <span class="number">900</span>, <span class="number">938</span>, <span class="number">944</span>, <span class="number">951</span>, <span class="number">956</span>, <span class="number">965</span>, <span class="number">1014</span>, <span class="number">1032</span>, <span class="number">1039</span>, <span class="number">1082</span>, <span class="number">1107</span>, <span class="number">1112</span>, <span class="number">1178</span>, <span class="number">1182</span>, <span class="number">1202</span>, <span class="number">1226</span>, <span class="number">1232</span>, <span class="number">1234</span>, <span class="number">1242</span>, <span class="number">1247</span>, <span class="number">1272</span>, <span class="number">1319</span>, <span class="number">1326</span>, <span class="number">1328</span>, <span class="number">1364</span>, <span class="number">1393</span>, <span class="number">1414</span>, <span class="number">1415</span>, <span class="number">1425</span>, <span class="number">1444</span>, <span class="number">1464</span>, <span class="number">1490</span>, <span class="number">1500</span>, <span class="number">1527</span>, <span class="number">1530</span>, <span class="number">1549</span>, <span class="number">1553</span>, <span class="number">1554</span>, <span class="number">1562</span>, <span class="number">1571</span>, <span class="number">1609</span>, <span class="number">1621</span>, <span class="number">1640</span>, <span class="number">1641</span>, <span class="number">1654</span>, <span class="number">1670</span>, <span class="number">1681</span>, <span class="number">1709</span>, <span class="number">1800</span>, <span class="number">1878</span>, <span class="number">1901</span>, <span class="number">1903</span>, <span class="number">1941</span>, <span class="number">1952</span>, <span class="number">1955</span>, <span class="number">1981</span>, <span class="number">1982</span>, <span class="number">2004</span>, <span class="number">2016</span>, <span class="number">2053</span>, <span class="number">2063</span>, <span class="number">2093</span>, <span class="number">2098</span>, <span class="number">2105</span>, <span class="number">2109</span>, <span class="number">2125</span>, <span class="number">2129</span>, <span class="number">2130</span>, <span class="number">2135</span>, <span class="number">2153</span>, <span class="number">2168</span>, <span class="number">2182</span>, <span class="number">2189</span>, <span class="number">2224</span>, <span class="number">2225</span>, <span class="number">2237</span>, <span class="number">2272</span>, <span class="number">2292</span>, <span class="number">2293</span>, <span class="number">2369</span>, <span class="number">2387</span>, <span class="number">2406</span>, <span class="number">2422</span>, <span class="number">2425</span>, <span class="number">2462</span>, <span class="number">2488</span>, <span class="number">2573</span>, <span class="number">2597</span>, <span class="number">2598</span>, <span class="number">2607</span>, <span class="number">2654</span>, <span class="number">2720</span>, <span class="number">2721</span>, <span class="number">2770</span>, <span class="number">2810</span>, <span class="number">2877</span>, <span class="number">2896</span>, <span class="number">2915</span>, <span class="number">2921</span>, <span class="number">2927</span>, <span class="number">2939</span>, <span class="number">2970</span>, <span class="number">2975</span>, <span class="number">2979</span>, <span class="number">3023</span>, <span class="number">3056</span>, <span class="number">3060</span>, <span class="number">3073</span>, <span class="number">3106</span>, <span class="number">3117</span>, <span class="number">3206</span>, <span class="number">3250</span>, <span class="number">3284</span>, <span class="number">3289</span>, <span class="number">3336</span>, <span class="number">3365</span>, <span class="number">3369</span>, <span class="number">3377</span>, <span class="number">3388</span>, <span class="number">3405</span>, <span class="number">3422</span>, <span class="number">3451</span>, <span class="number">3490</span>, <span class="number">3503</span>, <span class="number">3520</span>, <span class="number">3533</span>, <span class="number">3549</span>, <span class="number">3558</span>, <span class="number">3559</span>, <span class="number">3574</span>, <span class="number">3597</span>, <span class="number">3601</span>, <span class="number">3604</span>, <span class="number">3626</span>, <span class="number">3767</span>, <span class="number">3776</span>, <span class="number">3808</span>, <span class="number">3811</span>, <span class="number">3838</span>, <span class="number">3902</span>, <span class="number">3906</span>, <span class="number">3926</span>, <span class="number">3941</span>, <span class="number">3943</span>, <span class="number">3946</span>, <span class="number">3976</span>, <span class="number">4007</span>, <span class="number">4018</span>, <span class="number">4075</span>, <span class="number">4078</span>, <span class="number">4145</span>, <span class="number">4149</span>, <span class="number">4156</span>, <span class="number">4205</span>, <span class="number">4212</span>, <span class="number">4224</span>, <span class="number">4238</span>, <span class="number">4248</span>, <span class="number">4289</span>, <span class="number">4294</span>, <span class="number">4355</span>, <span class="number">4360</span>, <span class="number">4403</span>, <span class="number">4433</span>, <span class="number">4497</span>, <span class="number">4500</span>, <span class="number">4504</span>, <span class="number">4534</span>, <span class="number">4571</span>, <span class="number">4575</span>, <span class="number">4601</span>, <span class="number">4639</span>, <span class="number">4690</span>, <span class="number">4699</span>, <span class="number">4731</span>, <span class="number">4740</span>, <span class="number">4751</span>, <span class="number">4761</span>, <span class="number">4814</span>, <span class="number">4820</span>, <span class="number">4823</span>, <span class="number">4879</span>, <span class="number">4880</span>, <span class="number">4966</span>, <span class="number">5078</span>, <span class="number">5495</span>, <span class="number">5586</span>, <span class="number">5642</span>, <span class="number">5676</span>, <span class="number">5734</span>, <span class="number">5749</span>, <span class="number">5887</span>, <span class="number">5888</span>, <span class="number">5922</span>, <span class="number">5936</span>, <span class="number">5955</span>, <span class="number">5973</span>, <span class="number">6011</span>, <span class="number">6024</span>, <span class="number">6059</span>, <span class="number">6347</span>, <span class="number">6555</span>, <span class="number">6560</span>, <span class="number">6571</span>, <span class="number">6572</span>, <span class="number">6574</span>, <span class="number">6576</span>, <span class="number">6577</span>, <span class="number">6597</span>, <span class="number">6598</span>, <span class="number">6608</span>, <span class="number">6625</span>, <span class="number">6632</span>, <span class="number">6651</span>, <span class="number">6755</span>, <span class="number">6769</span>, <span class="number">6783</span>, <span class="number">6806</span>, <span class="number">7154</span>, <span class="number">7216</span>, <span class="number">7233</span>, <span class="number">7434</span>, <span class="number">7459</span>, <span class="number">7473</span>, <span class="number">7565</span>, <span class="number">7574</span>, <span class="number">7691</span>, <span class="number">7797</span>, <span class="number">7823</span>, <span class="number">7842</span>, <span class="number">7849</span>, <span class="number">7858</span>, <span class="number">7921</span>, <span class="number">8094</span>, <span class="number">8171</span>, <span class="number">8198</span>, <span class="number">8339</span>, <span class="number">8416</span>, <span class="number">8472</span>, <span class="number">8493</span>, <span class="number">8509</span>, <span class="number">8519</span>, <span class="number">8523</span>, <span class="number">8527</span>, <span class="number">9009</span>, <span class="number">9015</span>, <span class="number">9024</span>, <span class="number">9031</span>, <span class="number">9036</span>, <span class="number">9071</span>, <span class="number">9316</span>, <span class="number">9382</span>, <span class="number">9450</span>, <span class="number">9456</span>, <span class="number">9538</span>, <span class="number">9587</span>, <span class="number">9634</span>, <span class="number">9664</span>, <span class="number">9679</span>, <span class="number">9701</span>, <span class="number">9716</span>, <span class="number">9729</span>, <span class="number">9733</span>, <span class="number">9740</span>, <span class="number">9741</span>, <span class="number">9745</span>, <span class="number">9770</span>, <span class="number">9792</span>, <span class="number">9839</span>, <span class="number">9858</span>, <span class="number">9892</span>, <span class="number">9904</span>, <span class="number">9922</span>, <span class="number">9941</span>, <span class="number">9970</span>] <span class="number">312</span></span><br></pre></td></tr></table></figure>

<p>下面对其进行可视化操作。其实只是对上一章的可视化函数上做了一点小改动。<br>代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_err_prediction</span>(<span class="params">images,    <span class="comment"># 图像列表</span></span></span><br><span class="line"><span class="params">                       labels,     <span class="comment"># 标签列表</span></span></span><br><span class="line"><span class="params">                       prediction, <span class="comment"># 预测值列表</span></span></span><br><span class="line"><span class="params">                       err_lists,  <span class="comment"># 错误预测的下标列表</span></span></span><br><span class="line"><span class="params">                       index,      <span class="comment"># 从第index个开始显示</span></span></span><br><span class="line"><span class="params">                       num=<span class="number">10</span></span>):    <span class="comment"># 默认一次显示10幅</span></span><br><span class="line">        fig = plt.gcf()</span><br><span class="line">        fig.set_size_inches(<span class="number">10</span>,<span class="number">12</span>)</span><br><span class="line">        <span class="keyword">if</span> num &gt; <span class="number">25</span>:</span><br><span class="line">            num = <span class="number">25</span>    <span class="comment"># 一次最多显示25幅</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num):</span><br><span class="line">            <span class="comment"># 显示预测错误</span></span><br><span class="line">            err_index = err_lists[index]</span><br><span class="line">            ax = plt.subplot(<span class="number">5</span>,<span class="number">5</span>,i+<span class="number">1</span>)</span><br><span class="line">            ax.imshow(images[err_index].reshape(<span class="number">28</span>,<span class="number">28</span>),cmap=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 设置标题及坐标轴</span></span><br><span class="line">            title = <span class="string">&#x27;labels=&#x27;</span> + <span class="built_in">str</span>(np.argmax(labels[err_index])) + \</span><br><span class="line">            <span class="string">&#x27;, predict=&#x27;</span> + <span class="built_in">str</span>(prediction[err_index])</span><br><span class="line">            ax.set_title(title,fontsize=<span class="number">10</span>)</span><br><span class="line">            ax.set_xticks([])</span><br><span class="line">            ax.set_yticks([])</span><br><span class="line">            index += <span class="number">1</span></span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure>
<p>调用该函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_err_prediction(mnist.test.images,mnist.test.labels,</span><br><span class="line">                    prediction_result,err_lists,<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>输出如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter6/%E5%9B%BE6-4.png"></p>
<h2 id="6-2-多层神经网络建模与模型的保存还原"><a href="#6-2-多层神经网络建模与模型的保存还原" class="headerlink" title="6.2 多层神经网络建模与模型的保存还原"></a>6.2 多层神经网络建模与模型的保存还原</h2><h3 id="6-2-1-2层神经网络的构建"><a href="#6-2-1-2层神经网络的构建" class="headerlink" title="6.2.1 2层神经网络的构建"></a>6.2.1 2层神经网络的构建</h3><p>构建模型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 隐藏层神经元数量</span></span><br><span class="line">H1_NN = <span class="number">256</span>    <span class="comment"># 第1隐藏层神经元为256个</span></span><br><span class="line">H2_NN = <span class="number">64</span>    <span class="comment"># 第2隐藏层神经元为64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入层-第1隐藏层参数和偏置项</span></span><br><span class="line">w1 = tf.Variable(tf.truncated_normal([<span class="number">784</span>,H1_NN],stddev=<span class="number">0.1</span>))</span><br><span class="line">b1 = tf.Variable(tf.zeros([H1_NN]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第1隐藏层-第2隐藏层参数和偏置项</span></span><br><span class="line">w2 = tf.Variable(tf.truncated_normal([H1_NN,H2_NN],stddev=<span class="number">0.1</span>))</span><br><span class="line">b2 = tf.Variable(tf.zeros([H2_NN]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第2隐藏层-输出层参数和偏置项</span></span><br><span class="line">w3 = tf.Variable(tf.truncated_normal([H2_NN,<span class="number">10</span>],stddev=<span class="number">0.1</span>))</span><br><span class="line">b3 = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算第1隐藏层结果</span></span><br><span class="line">y1 = tf.nn.relu(tf.matmul(x,w1) + b1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算第2隐藏层结果</span></span><br><span class="line">y2 = tf.nn.relu(tf.matmul(y1,w2) + b2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算输出结果</span></span><br><span class="line">forward = tf.matmul(y2,w3) + b3</span><br><span class="line">pred = tf.nn.softmax(forward)</span><br></pre></td></tr></table></figure>

<p>训练完成后评估模型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">accu_test = sess.run(accuracy,</span><br><span class="line">                     feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Test Accuracy:&#x27;</span>,accu_test)</span><br></pre></td></tr></table></figure>
<p>输出为：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Test Accuracy: <span class="number">0.9699</span></span><br></pre></td></tr></table></figure>

<h3 id="6-2-2-3层神经网络的构建"><a href="#6-2-2-3层神经网络的构建" class="headerlink" title="6.2.2 3层神经网络的构建"></a>6.2.2 3层神经网络的构建</h3><p>构建模型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 隐藏层神经元数量</span></span><br><span class="line">H1_NN = <span class="number">256</span>    <span class="comment"># 第1隐藏层神经元为256个</span></span><br><span class="line">H2_NN = <span class="number">64</span>    <span class="comment"># 第2隐藏层神经元为64</span></span><br><span class="line">H3_NN = <span class="number">32</span>    <span class="comment"># 第3隐藏层神经元为32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入层-第1隐藏层参数和偏置项</span></span><br><span class="line">w1 = tf.Variable(tf.truncated_normal([<span class="number">784</span>,H1_NN],stddev=<span class="number">0.1</span>))</span><br><span class="line">b1 = tf.Variable(tf.zeros([H1_NN]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第1隐藏层-第2隐藏层参数和偏置项</span></span><br><span class="line">w2 = tf.Variable(tf.truncated_normal([H1_NN,H2_NN],stddev=<span class="number">0.1</span>))</span><br><span class="line">b2 = tf.Variable(tf.zeros([H2_NN]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第2隐藏层-第3隐藏层参数和偏置项</span></span><br><span class="line">w3 = tf.Variable(tf.truncated_normal([H2_NN,H3_NN],stddev=<span class="number">0.1</span>))</span><br><span class="line">b3 = tf.Variable(tf.zeros([H3_NN]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第3隐藏层-输出层参数和偏置项</span></span><br><span class="line">w4 = tf.Variable(tf.truncated_normal([H3_NN,<span class="number">10</span>],stddev=<span class="number">0.1</span>))</span><br><span class="line">b4 = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算第1隐藏层结果</span></span><br><span class="line">y1 = tf.nn.relu(tf.matmul(x,w1) + b1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算第2隐藏层结果</span></span><br><span class="line">y2 = tf.nn.relu(tf.matmul(y1,w2) + b2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算第3隐藏层结果</span></span><br><span class="line">y3 = tf.nn.relu(tf.matmul(y2,w3) + b3)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算输出结果</span></span><br><span class="line">forward = tf.matmul(y3,w4) + b4</span><br><span class="line">pred = tf.nn.softmax(forward)</span><br></pre></td></tr></table></figure>

<p>训练完成后评估模型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">accu_test = sess.run(accuracy,</span><br><span class="line">                     feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Test Accuracy:&#x27;</span>,accu_test)</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Test Accuracy: <span class="number">0.9762</span></span><br></pre></td></tr></table></figure>
<p>需要注意的一点是，<strong>神经网络的层数并不是越多越好，需要具体问题具体分析</strong>。</p>
<h3 id="6-2-3-模型重构"><a href="#6-2-3-模型重构" class="headerlink" title="6.2.3 模型重构"></a>6.2.3 模型重构</h3><p>上面两节构建多层神经网络的代码太罗嗦了，能不能优化一下呢？<br>我们可以定义一个全连接层函数，来实现神经网络各层的定义。<br>代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义全连接层函数 fully connected</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fcn_layer</span>(<span class="params">inputs,          <span class="comment"># 输入数据</span></span></span><br><span class="line"><span class="params">             input_dim,        <span class="comment"># 输入神经元数量</span></span></span><br><span class="line"><span class="params">             output_dim,       <span class="comment"># 输出神经元数量</span></span></span><br><span class="line"><span class="params">             activation=<span class="literal">None</span></span>): <span class="comment"># 激活函数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 以截断正态分布的随机数初始化w</span></span><br><span class="line">    w = tf.Variable(tf.truncated_normal([input_dim,output_dim],stddev=<span class="number">0.1</span>))</span><br><span class="line">    <span class="comment"># 以0初始化b</span></span><br><span class="line">    b = tf.Variable(tf.zeros([output_dim]))</span><br><span class="line">    </span><br><span class="line">    xwb = tf.matmul(inputs,w) + b    <span class="comment">#建立表达式：inputs * w + b</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> activation <span class="keyword">is</span> <span class="literal">None</span>:    <span class="comment"># 默认不使用激活函数</span></span><br><span class="line">        outputs = xwb</span><br><span class="line">    <span class="keyword">else</span>:                    <span class="comment"># 若传入激活函数，则用其对输出结果进行变换</span></span><br><span class="line">        outputs = activation(xwb)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>

<p>在Python中，函数名称可以可以赋值给其他变量，被赋值的变量称为函数的引用，当该变量被使用时，就会执行变量所引用的函数。<br>示例代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义两数和函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">a,b</span>):</span><br><span class="line">    <span class="keyword">return</span> a + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># add函数名称赋值给sum_ab变量</span></span><br><span class="line">sum_ab = add</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用sum_ab变量</span></span><br><span class="line"><span class="built_in">print</span>(sum_ab(-<span class="number">3</span>,<span class="number">5</span>))</span><br><span class="line"><span class="comment"># 使用add变量</span></span><br><span class="line"><span class="built_in">print</span>(add(-<span class="number">3</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义两数和函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">a,b</span>):</span><br><span class="line">    <span class="keyword">return</span> a + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># add函数名称赋值给sum_ab变量</span></span><br><span class="line">sum_ab = add</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用sum_ab变量</span></span><br><span class="line"><span class="built_in">print</span>(sum_ab(-<span class="number">3</span>,<span class="number">5</span>))</span><br><span class="line"><span class="comment"># 使用add变量</span></span><br><span class="line"><span class="built_in">print</span>(add(-<span class="number">3</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<p>上面定义的全连接层函数中的activation参数就是这种用法。</p>
<p>下面以三隐层模型的建立作为示例：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建输入层</span></span><br><span class="line">x = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">784</span>],name=<span class="string">&#x27;X&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义标签数据占位符</span></span><br><span class="line">y = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">10</span>],name=<span class="string">&#x27;Y&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 隐藏层神经元数量</span></span><br><span class="line">H1_NN = <span class="number">256</span>    <span class="comment"># 第1隐藏层神经元为256个</span></span><br><span class="line">H2_NN = <span class="number">64</span>    <span class="comment"># 第2隐藏层神经元为64</span></span><br><span class="line">H3_NN = <span class="number">32</span>    <span class="comment"># 第3隐藏层神经元为32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建隐藏层1</span></span><br><span class="line">h1 = fcn_layer(inputs=x,input_dim=<span class="number">784</span>,output_dim=H1_NN,activation=tf.nn.relu)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建隐藏层2</span></span><br><span class="line">h2 = fcn_layer(inputs=h1,input_dim=H1_NN,output_dim=H2_NN,activation=tf.nn.relu)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建隐藏层3</span></span><br><span class="line">h3 = fcn_layer(inputs=h2,input_dim=H2_NN,output_dim=H3_NN,activation=tf.nn.relu)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建输出层</span></span><br><span class="line"><span class="comment"># 由于后面的损失函数要用到未经激活函数处理的输出，所以这里的activation定义为None</span></span><br><span class="line">forward = fcn_layer(inputs=h3,input_dim=H3_NN,output_dim=<span class="number">10</span>,activation=<span class="literal">None</span>)</span><br><span class="line">pred = tf.nn.softmax(forward)</span><br></pre></td></tr></table></figure>
<p>代码要比之前的简洁多了。</p>
<h3 id="6-2-4-模型保存"><a href="#6-2-4-模型保存" class="headerlink" title="6.2.4 模型保存"></a>6.2.4 模型保存</h3><p>初始化参数和模型目录：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 存储模型的粒度</span></span><br><span class="line">save_step = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建保存模型文件的目录</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">ckpt_dir = <span class="string">&#x27;ckpt_dir/&#x27;</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(ckpt_dir):</span><br><span class="line">    os.makedirs(ckpt_dir)</span><br></pre></td></tr></table></figure>

<p>训练并存储模型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 声明完所有变量后，调用tf.train.Saver</span></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 记录训练开始时间</span></span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line">start_time = time()</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(train_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> <span class="built_in">range</span>(total_batch):</span><br><span class="line">        xs,ys = mnist.train.next_batch(batch_size)</span><br><span class="line">        sess.run(optimizer,feed_dict=&#123;x:xs,y:ys&#125;)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># total_batch个批次训练完成后，使用验证数据计算误差与准确率</span></span><br><span class="line">    loss,acc = sess.run([loss_function,accuracy],</span><br><span class="line">                        feed_dict=&#123;x:mnist.validation.images,y:mnist.validation.labels&#125;)</span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Train Epoch: %02d,  Loss= %.9f,  Accurary= %.4f&#x27;</span></span><br><span class="line">             % (epoch + <span class="number">1</span>,loss,acc))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % save_step == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># 训练过程中存储模型</span></span><br><span class="line">        <span class="comment"># os.path.join()函数连接两个或更多的路径名</span></span><br><span class="line">        saver.save(sess,os.path.join(ckpt_dir,<span class="string">&#x27;mnist_model_&#123;:06d&#125;.ckpt&#x27;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>)))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;mnist_model_&#123;:06d&#125;.ckpt saved&#x27;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 存储训练完成后的模型        </span></span><br><span class="line">saver.save(sess,os.path.join(ckpt_dir,<span class="string">&#x27;mnist_model.ckpt&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Model saved!&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示运行总时间</span></span><br><span class="line">duration = time() - start_time</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Train Finished takes: %.2f&#x27;</span> % (duration))</span><br></pre></td></tr></table></figure>

<p>TensorFlow模型保存时有两个主要的文件：</p>
<ol>
<li>Meta graph：这是一个协议缓冲区，它保存了完整的TensorFlow图形，即所有变量、操作、集合等。该文件以.meta作为扩展名。</li>
<li>Checkpoint file：这是一个二进制文件，它包含了所有的权重、偏差、梯度和所有其他变量的值。这个文件有一个扩展名.ckpt。然而，Tensorflow从0.11版本中改变了这一点。现在，我们有两个文件，而不是单个.ckpt文件。</li>
</ol>
<p><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter6/%E5%9B%BE6-5.png"><br>.data文件是包含我们训练变量的文件。<br>与此同时，Tensorflow也有一个名为“checkpoint”的文件，它只保存最新保存的checkpoint文件的记录。<br>因此，TensorFlow模型如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter6/%E5%9B%BE6-6.png"><br>并且，TensorFlow默认最多保存最近5次的。因此此时ckpt_dir文件夹下应该有16个文件，其中包括1个名为checkpoint的文件和5个（每一个模型都由3个文件组成）TensorFlow模型。<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter6/%E5%9B%BE6-7.png"></p>
<p>下面介绍一下<strong>tf.train.saver.save()方法</strong>，该方法常用参数如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">save(</span><br><span class="line">    sess,</span><br><span class="line">    save_path)</span><br></pre></td></tr></table></figure>
<p>sess：会话对象。Tensorflow变量仅在会话中存在，因此必须在一个会话中保存模型。<br>save_path：要保存的文件路径+文件名的前缀，不需要加.ckpt扩展名。<br>示例：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在ckpt_dir文件夹下生成名为mnist_model的模型文件</span></span><br><span class="line">saver.save(sess,<span class="string">&#x27;ckpt_dir/mnist_model&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="6-2-5-模型还原"><a href="#6-2-5-模型还原" class="headerlink" title="6.2.5 模型还原"></a>6.2.5 模型还原</h3><p>TensorFlow中采用<strong>tf.train.saver.restore()方法</strong>实现已保存模型的载入。其参数如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">restore(</span><br><span class="line">    sess,</span><br><span class="line">    save_path)</span><br></pre></td></tr></table></figure>
<p>sess：用于还原变量的会话。<br>save_path：先前保存参数的路径。包括模型文件名。<br>示例：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取ckpt_dir文件夹下名为mnist_model的模型文件</span></span><br><span class="line">saver.restore(sess,<span class="string">&#x27;ckpt_dir/mnist_model&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>读取模型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 声明完所有变量后，调用tf.train.Saver</span></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 记录训练开始时间</span></span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line">start_time = time()</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取CheckpointState对象</span></span><br><span class="line">ckpt = tf.train.get_checkpoint_state(ckpt_dir)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:	<span class="comment"># 如果模型文件存在</span></span><br><span class="line">    <span class="comment"># 从已保存的模型中读取参数</span></span><br><span class="line">    saver.restore(sess,ckpt.model_checkpoint_path)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Restore model from&#x27;</span>,ckpt.model_checkpoint_path)</span><br></pre></td></tr></table></figure>

<p>上面的代码用到了<strong>tf.train.get_checkpoint_state()函数</strong>，下面对其进行介绍。<br>该函数的参数如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.train.get_checkpoint_state(</span><br><span class="line">    checkpoint_dir,</span><br><span class="line">    latest_filename=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>checkpoint_dir：模型文件的存放目录<br>该函数会读取那个名为“checkpoint”的文件，如果该文件内有合法的CheckpointState原型，将其返回。这是什么意思呢？<br>让我们看一下名为“checkpoint”的文件的内容：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model_checkpoint_path: <span class="string">&quot;mnist_model&quot;</span></span><br><span class="line">all_model_checkpoint_paths: <span class="string">&quot;mnist_model_000025&quot;</span></span><br><span class="line">all_model_checkpoint_paths: <span class="string">&quot;mnist_model_000030&quot;</span></span><br><span class="line">all_model_checkpoint_paths: <span class="string">&quot;mnist_model_000035&quot;</span></span><br><span class="line">all_model_checkpoint_paths: <span class="string">&quot;mnist_model_000040&quot;</span></span><br><span class="line">all_model_checkpoint_paths: <span class="string">&quot;mnist_model&quot;</span></span><br></pre></td></tr></table></figure>
<p>再运行如下代码：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ckpt = tf.train.get_checkpoint_state(ckpt_dir)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(ckpt))</span><br><span class="line"><span class="built_in">print</span>(ckpt)</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;tensorflow.python.training.checkpoint_state_pb2.CheckpointState&#x27;</span>&gt;</span><br><span class="line">model_checkpoint_path: <span class="string">&quot;ckpt_dir/mnist_model&quot;</span></span><br><span class="line">all_model_checkpoint_paths: <span class="string">&quot;ckpt_dir/mnist_model_000025&quot;</span></span><br><span class="line">all_model_checkpoint_paths: <span class="string">&quot;ckpt_dir/mnist_model_000030&quot;</span></span><br><span class="line">all_model_checkpoint_paths: <span class="string">&quot;ckpt_dir/mnist_model_000035&quot;</span></span><br><span class="line">all_model_checkpoint_paths: <span class="string">&quot;ckpt_dir/mnist_model_000040&quot;</span></span><br><span class="line">all_model_checkpoint_paths: <span class="string">&quot;ckpt_dir/mnist_model&quot;</span></span><br></pre></td></tr></table></figure>
<p>CheckpointState对象与“checkpoint”文件的内容相同！<br>CheckpointState对象有<strong>model_checkpoint_path</strong>和<strong>all_model_checkpoint_paths</strong>两个属性。其中model_checkpoint_path保存了最新的tensorflow模型文件的文件名（包含文件路径），all_model_checkpoint_paths则有未被删除的所有tensorflow模型文件的文件名（包含文件路径）。<br>因此我们可以通过tf.train.get_checkpoint_state()函数获得CheckpointState对象，并根据CheckpointState对象的model_checkpoint_path属性来载入最新的TensorFlow模型文件。</p>
<p>除了采用上面的代码外，我们还可以使用<strong>tf.train.latest_checkpoint()函数</strong>来找到最新的模型文件，从而实现模型的还原。<br>该函数的参数如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.train.latest_checkpoint(</span><br><span class="line">    checkpoint_dir,</span><br><span class="line">    latest_filename=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>checkpoint_dir：模型文件的存放目录<br>该函数会返回最新的tensorflow模型文件的文件名（包含文件路径）。<br>示例如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ckpt = tf.train.latest_checkpoint(ckpt_dir)</span><br><span class="line"><span class="built_in">print</span>(ckpt)</span><br></pre></td></tr></table></figure>
<p>输出为：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ckpt_dir/mnist_model</span><br></pre></td></tr></table></figure>

<p>下面我们试着用这种方式来实现模型的还原。<br>代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 声明完所有变量后，调用tf.train.Saver</span></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 记录训练开始时间</span></span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line">start_time = time()</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果有检查点文件，读取最新的检查点文件，恢复各种变量值</span></span><br><span class="line">ckpt = tf.train.latest_checkpoint(ckpt_dir)</span><br><span class="line"><span class="keyword">if</span> ckpt != <span class="literal">None</span>:</span><br><span class="line">    saver.restore(sess,ckpt)    <span class="comment"># 加载所有的参数</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Restore model from&#x27;</span>,ckpt)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Training from scratch&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>在介绍完了上面这些东西之后，我们可以考虑一下如何实现断点续训。<br>代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个记录训练轮数的epoch变量，使其能够被保存</span></span><br><span class="line"><span class="comment"># 指定trainable变量为False，表示该变量不参与训练</span></span><br><span class="line">epoch = tf.Variable(<span class="number">0</span>,name=<span class="string">&#x27;epoch&#x27;</span>,trainable=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------读取模型文件，还原变量，代码省略------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取续训参数</span></span><br><span class="line">start = sess.run(epoch)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Training starts from %d epoch&#x27;</span> % start + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ep <span class="keyword">in</span> <span class="built_in">range</span>(start,train_epochs):</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#------训练过程代码，略------</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将ep+1赋值给epoch，相当于每次循环epoch都自加1</span></span><br><span class="line">    <span class="comment"># 需要注意的是，调用赋值函数只是定义了一个操作，要想操作执行，必须要调用会话中的run函数</span></span><br><span class="line">    sess.run(epoch.assign(ep + <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>这样就可以实现断点重训了。</p>
<h1 id="第七章-图像识别问题：卷积神经网络及其应用"><a href="#第七章-图像识别问题：卷积神经网络及其应用" class="headerlink" title="第七章 图像识别问题：卷积神经网络及其应用"></a>第七章 图像识别问题：卷积神经网络及其应用</h1><h2 id="7-1-从全连接网络到卷积神经网络"><a href="#7-1-从全连接网络到卷积神经网络" class="headerlink" title="7.1 从全连接网络到卷积神经网络"></a>7.1 从全连接网络到卷积神经网络</h2><p>对于MNIST 手写数字识别，假如第一个隐层的节点数为500，那么一个全连接层的参数个数为：</p>
<blockquote>
<p>28×28×1×500+500 ≈ 40万</p>
</blockquote>
<p>其中的1为通道数，由于MNIST数据集中的图像都是黑白的，所以为色彩<strong>单通道</strong>。<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-1.png"><br>当图片更大时，比如CIFAR数据集中，图片大小为32×32×3，此时全连接层的参数有：</p>
<blockquote>
<p>32×32×3×500+500 ≈ 150万</p>
</blockquote>
<p>这里的3也为通道数，色彩<strong>三通道</strong>（RGB三色）。<br>当图片分辨率进一步提高时，当隐层数量增加时，例如:600 x 600 图像，各隐层节点数分别为300、200和100，则参数个数为：</p>
<blockquote>
<p>600 x 600 x 300 + 300 x 200 + 200 x 100 ≈ 1.08亿</p>
</blockquote>
<p>而参数增多会导致计算速度减慢，过拟合等问题。因此，我们需要更合理的结构来有效减少参数个数！<br>这就引出了卷积神经网络。<br>关于卷积神经网络的比较好的文章：<br><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/52668301">https://www.zhihu.com/question/52668301</a> 。这是知乎上的一个问题，强烈推荐看YJango的回答（第三个），机器之心的回答也可以看看。<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/v_july_v/article/details/51812459">https://blog.csdn.net/v_july_v&#x2F;article&#x2F;details&#x2F;51812459</a> </p>
<h2 id="7-2-卷积神经网络的介绍"><a href="#7-2-卷积神经网络的介绍" class="headerlink" title="7.2 卷积神经网络的介绍"></a>7.2 卷积神经网络的介绍</h2><h3 id="7-2-1-卷积神经网络概述"><a href="#7-2-1-卷积神经网络概述" class="headerlink" title="7.2.1 卷积神经网络概述"></a>7.2.1 卷积神经网络概述</h3><p>1962年Hubel和Wiesel通过对猫视觉皮层细胞的研究，提出了**感受野(receptive field)<strong>的概念。视觉皮层的神经元就是局部接受信息的，只受某些特定区域刺激的响应，而不是对全局图像进行感知。<br>1984年日本学者Fukushima基于感受野概念提出</strong>神经认知机(neocognitron)**。<br><strong>卷积神经网络（ConvolutionalNeuralNetwork,CNN）</strong>可看作是神经认知机的推广形式。</p>
<p>CNN是一个<strong>多层的神经网络</strong>，每层由<strong>多个二维平面</strong>组成，其中每个平面由<strong>多个独立神经元</strong>组成。<br>如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-2.png"><br>卷积神经网络的主要结构如下：</p>
<ol>
<li><strong>输入层</strong>：将每个像素代表一个特征节点输入到网络中。</li>
<li><strong>卷积层</strong>：卷积运算的主要目的是使原信号特征增强，并降低噪音。</li>
<li><strong>降采样层</strong>（也叫<strong>池化层</strong>）：降低网络训练参数及模型的过拟合程度。</li>
<li><strong>全连接层</strong>：对生成的特征进行加权。<br>下面对卷积神经网络中的卷积和池化运算进行介绍。</li>
</ol>
<h3 id="7-2-2-卷积"><a href="#7-2-2-卷积" class="headerlink" title="7.2.2 卷积"></a>7.2.2 卷积</h3><p>在介绍卷积神经网络的基本概念之前，我们先做一个矩阵运算：</p>
<ol>
<li><strong>求点积</strong>：将5×5输入矩阵中3×3深蓝色区域中每个元素分别与其对应位置的权值（红色数字）相乘，然后再相加，所得到的值作为3×3输出矩阵（绿色）的第一个元素。实际上，类似于前馈神经网络，这里所得到的值还应加上一个**偏移量b<sub>0</sub>**。<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-3.png"></li>
<li><strong>滑动窗口</strong>：将3×3权值矩阵向右移动一个格(即,步长为1)。</li>
<li><strong>重复操作</strong>：同样地，将此时深色区域内每个元素分别与对应的权值相乘然<br>后再相加，所得到的值作为输出矩阵的第二个元素；重复上述“求点积-滑动窗<br>口”操作，直至输出矩阵所有值被填满。<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-4.png"></li>
</ol>
<p><strong>卷积核</strong>在2 维输入数据上“滑动”，对当前输入部分的元素进行矩阵乘法，然后将结果汇为单个输出像素值，重复这个过程直到遍历整张图像，这个过程就叫做<strong>卷积</strong>。<br>这个权值矩阵就是<strong>卷积核</strong>。卷积操作后的图像称为<strong>特征图（feature map）</strong>。<br>卷积运算有以下两个主要特点：</p>
<ul>
<li><strong>局部连接</strong>：每个输出特性不用查看每个输入特征，而只需查看<strong>部分输入特征</strong>。</li>
<li><strong>权值共享</strong>：卷积核在图像上滑动过程中<strong>保持不变</strong>。即<strong>权值</strong>以及**偏移量b<sub>0</sub>**在图像上滑动过程中都是不变的。</li>
</ul>
<p>常用的卷积核有以下三种：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 提取垂直方向特征</span></span><br><span class="line"><span class="comment"># sobel_x</span></span><br><span class="line">kernel_1 = np.array([[-<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],</span><br><span class="line">                    [-<span class="number">2</span>,<span class="number">0</span>,<span class="number">2</span>],</span><br><span class="line">                    [-<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取水平方向特征</span></span><br><span class="line"><span class="comment"># sobel_y</span></span><br><span class="line">kernel_2 = np.array([[-<span class="number">1</span>,-<span class="number">2</span>,<span class="number">1</span>],</span><br><span class="line">                    [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">                    [<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Laplace扩展算子</span></span><br><span class="line"><span class="comment"># 二阶微分算子</span></span><br><span class="line">kernel_3 = np.array([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                    [<span class="number">1</span>,-<span class="number">8</span>,<span class="number">1</span>],</span><br><span class="line">                    [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br></pre></td></tr></table></figure>
<p>其中sobel_x主要提取垂直方向特征，sobel_y主要提取水平方向特征。Laplace则比较平均。<br>下面是同一张图片经过这几种卷积核进行运算后所产生的特征图：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-5.png"><br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-6.png"><br>可以看到，经不同卷积核所进行的卷积运算的差距还是蛮大的。<br>卷积运算的主要目的是<strong>使原信号特征增强，并降低噪音</strong>。<br>对图像用一个卷积核进行卷积运算，实际上是一个滤波的过程。每个<strong>卷积核</strong>都是一种<strong>特征提取方式</strong>，就像是一个筛子，将图像中符合条件的部分筛选出来。</p>
<p>观察卷积示例，我们会发现一个<strong>现象</strong>：在卷积核滑动的过程中图像的边缘会被裁剪掉，5×5特征矩阵将转换为 3×3的特征矩阵。<br>那么如何使得输出尺寸与输入保持一致呢?<br><strong>0填充（Zero padding）</strong>：用额外的“假”像素（通常值为 0）填充边缘。这样，在滑动时的卷积核可以允许原始边缘像素位于卷积核的中心，同时延伸到边缘之外的假像素，从而产生与输入（5×5蓝色）相同大小的输出（5×5绿色）。<br>如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-7.png"></p>
<p>下面介绍一下<strong>多通道卷积</strong>。<br>每个卷积核都会将图像生成为另一幅特征映射图，即：<strong>一个卷积核提取一种特征</strong>。为了使特征提取更充分，可以添加多个卷积核以提取不同的特征，也就是，<strong>多通道卷积</strong>。<br>多通道卷积主要有以下步骤：</p>
<ul>
<li>每个通道使用一个卷积核进行卷积操作<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-8.png"></li>
<li>然后将这些特征图相同位置上的值相加,生成一张特征图。<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-9.png"></li>
<li><strong>加偏置</strong>。偏置的作用是对每个feature map加一个偏置项以便产生最终的输出特征图。<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-10.png"></li>
</ul>
<p>和前馈神经网络一样，经过线性组合和偏移后，会<strong>加入非线性部分</strong>来增强模型的拟合能力。<br>如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-11.png"><br><strong>卷积层的输出应经过激活函数处理后再送入下一层</strong>。因此，卷积神经网络中间部分更细致一些的结构图应该如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-12.png"><br>其中CONV代表卷积层，RELU代表relu激活函数激活，POOL代表池化层。</p>
<h3 id="7-2-3-池化"><a href="#7-2-3-池化" class="headerlink" title="7.2.3 池化"></a>7.2.3 池化</h3><p>在卷积层之后常常紧接着一个降采样层，通过减小矩阵的长和宽，从而达到减少参数的目的。<br>降采样是降低特定信号的采样率的过程。最常用的降采样方法是<strong>池化（pooling）</strong>。</p>
<p>计算图像一个区域上的某个特征的平均值或最大值，这种聚合操作就叫做<strong>池化（pooling）</strong>。<br>卷积层的作用是探测上一层特征的局部连接，而池化的作用是<strong>在语义上把相似的特征合并起来</strong>，从而达到降维的目的。这些概要统计特征不仅具有低得多的维度（相比使用所有提取得到的特征），同时还会改善结果（不容易过拟合）。</p>
<p>常用的池化方法：</p>
<ol>
<li><strong>均值池化</strong>：对池化区域内的像素点<strong>取均值</strong>，这种方法得到的特征数据<strong>对背景信息更敏感</strong>。</li>
<li><strong>最大池化</strong>：对池化区域内所有像素点取<strong>最大值</strong>，这种方法得到的特征<strong>对纹理特征信息更加敏感</strong>。<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-13.png"><br>隐层与隐层之间空间分辨率递减，因此，为了检测更多的特征信息、形成更多不同通道特征的组合，从而形成更复杂的特征，需要逐渐增加每层所含的平面数（也就是特征图的数量）。</li>
</ol>
<p><strong>步长（stride）</strong>是卷积操作的重要概念，表示卷积核在图片上移动的格数。通过步长的变换，可以得到不同尺寸的卷积输出结果。<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-14.png"><br>步长大于1的卷积操作也是<strong>降维的一种方式</strong><br>卷积后图片尺寸：假如步长为S，原始图片尺寸为[N1,N1]，卷积核大小为<br>[N2,N2]，那么卷积之后图像大小：<br>[(N1-N2)&#x2F;S+1, (N1-N2)&#x2F;S+1]</p>
<p>卷积神经网络过程概括：输入图像通过若干个“卷积→降采样”后，连接成一个向量输入到传统的分类器层中，最终得到输出。<br>可用如下的正则表达式表示：<br><strong>输入层→(卷积层+→池化层?)+→全连接层+</strong><br>+表示一个或多个，?表示0个或一个，()表示一组。</p>
<h3 id="7-2-4-TensorFlow中CNN的相关函数"><a href="#7-2-4-TensorFlow中CNN的相关函数" class="headerlink" title="7.2.4 TensorFlow中CNN的相关函数"></a>7.2.4 TensorFlow中CNN的相关函数</h3><p>卷积函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.conv2d(<span class="built_in">input</span>, <span class="built_in">filter</span>, strides, padding, use_cudnn_on_gpu=<span class="literal">None</span>,name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>input：需要做卷积的输入数据。注意：这是一个<strong>4维张量</strong>（[batch,in_height,in_width,in_channels]），要求类型为float32或float64其中之一。batch为图片的数量，in_height 为图片高度，in_weight 为图片宽度，in_channel 为图片的通道数，灰度图该值为1，彩色图为3。</li>
<li>filter：卷积核。[filter_height, filter_width, in_channels, out_channels]，其中filter_height为卷积核高度，filter_width为卷积核宽度，in_channels是图像通道数 ，和input 的in_channels要保持一致，out_channels是卷积核数量。</li>
<li>strides：图像每一维的步长，是一个一维向量，长度为4。一般为[1,stride,stride,1]，其中第一个1和最后一个1是固定值，需要改变的是中间的两个数，即在x轴和y轴的移动步长。</li>
<li>padding：定义元素边框与元素内容之间的空间。”SAME”或”VALID”（<strong>注意要大写！</strong>），这个值决定了不同的卷积方式。当为”SAME”时，表示边缘填充，适用于全尺寸操作；当为”VALID”时，表示边缘不填充。使用”SAME”时卷积结果与卷积前图像大小相同，使用”VALID”时卷积结果要小于卷积前图像。**对卷积操作来说，一般是选”SAME”**。</li>
<li>use_cudnn_on_gpu：bool类型，是否使用cudnn加速，默认为True。</li>
<li>name：该操作的名称</li>
<li>返回值：返回一个tensor，即feature map，shape仍然是[batch, height, width, channels]这种形式</li>
</ul>
<p>对上面filter参数中out_channels的中的疑问：为什么颜色通道为1或3的图像，经过卷积后，它的通道可以变成 128 或者其它呢？<br>答案在于卷积核的数目。一个卷积核得到的特征提取是不充分的，我们可以添加多个卷积核，比如32个卷积核，可以学习32种特征，此时输出为32个feature map，即输出的图像的通道数为32。所以上面的out_channels为输出图像的通道数，也就是卷积核的数目。<br>卷积过程对一个通道的图像进行卷积，比如10个卷积核，得到10个feature map，那么那么<strong>输入图像为RGB三个通道</strong>呢？输出为30个feature map吗？答案肯定不是的。<strong>输出的个数依然是卷积核的个数10，只不过输出的图像是对RGB三个通道的加和</strong>。<br>这一部分，其实在我前面推荐的那篇YJango的回答也有所提及，可以看一下。</p>
<p>示例代码：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">10</span>,<span class="number">9</span>,<span class="number">9</span>,<span class="number">4</span>),dtype=tf.float32)</span><br><span class="line">filter_data = tf.Variable(np.random.rand(<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">2</span>),dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卷积</span></span><br><span class="line">y1 = tf.nn.conv2d(input_data,filter_data,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line">y2 = tf.nn.conv2d(input_data,filter_data,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">&#x27;VALID&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(input_data)</span><br><span class="line"><span class="built_in">print</span>(y1)</span><br><span class="line"><span class="built_in">print</span>(y2)</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Variable <span class="string">&#x27;Variable_12:0&#x27;</span> shape=(<span class="number">10</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">4</span>) dtype=float32_ref&gt;</span><br><span class="line">Tensor(<span class="string">&quot;Conv2D_8:0&quot;</span>, shape=(<span class="number">10</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">2</span>), dtype=float32)</span><br><span class="line">Tensor(<span class="string">&quot;Conv2D_9:0&quot;</span>, shape=(<span class="number">10</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">2</span>), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>可以看到在指定padding参数为”VAILD”后，输出的特征图的大小相较于原始图像的大小是减小了的。</p>
<p>池化函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最大池化</span></span><br><span class="line">tf.nn.max_pool(value, ksize, strides, padding, name=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 平均池化</span></span><br><span class="line">tf.nn.avg_pool(value, ksize, strides, padding, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>value：需要池化的输入。一般池化层接在卷积层后面，所以输入通常是conv2d所输出的feature map，依然是4维的张量([batch, height, width,channels])。</li>
<li>ksize：池化窗口的大小，由于一般不在batch和channel上做池化，所以ksize一般是[1,height, width,1]。</li>
<li>strides：跟卷积函数中strides含义一样。图像每一维的步长，是一个一维向量，长度为4，一般为[1,stride,stride,1]。</li>
<li>padding：和卷积函数中padding含义一样，”SAME”或”VALID”。</li>
<li>name：该操作的名称。</li>
<li>返回值：返回一个Tensor，类型不变，shape仍然是[batch, height, width, channels]这种形式。</li>
</ul>
<p>示例如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>),dtype=tf.float32)</span><br><span class="line">filter_data = tf.Variable(np.random.rand(<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>),dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卷积</span></span><br><span class="line">y = tf.nn.conv2d(input_data,filter_data,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最大池化</span></span><br><span class="line">output1 = tf.nn.max_pool(y,ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line">output2 = tf.nn.max_pool(y,ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">&#x27;VALID&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(output1)</span><br><span class="line"><span class="built_in">print</span>(output2)</span><br></pre></td></tr></table></figure>
<p>输出为：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Tensor(<span class="string">&quot;Conv2D_11:0&quot;</span>, shape=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>), dtype=float32)</span><br><span class="line">Tensor(<span class="string">&quot;MaxPool_2:0&quot;</span>, shape=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>), dtype=float32)</span><br><span class="line">Tensor(<span class="string">&quot;MaxPool_3:0&quot;</span>, shape=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>可以看到，使用不同的padding参数，结果还是不一样的。<br>因为模板在滑动时，可能存在覆盖不完全的地方，就比如用2*2的模板，对于VALID模式和SAME模式就不一样，SAME模式会补全橙色部分，而VALID模式就不会补全了。<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-15.png"></p>
<h2 id="7-3-卷积神经网络实现MNIST手写数据识别"><a href="#7-3-卷积神经网络实现MNIST手写数据识别" class="headerlink" title="7.3 卷积神经网络实现MNIST手写数据识别"></a>7.3 卷积神经网络实现MNIST手写数据识别</h2><p>采用的卷积神经网络结构如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-16.png"></p>
<table>
<thead>
<tr>
<th>输入层</th>
<th>卷积层1</th>
<th>降采样层1</th>
<th>卷积层2</th>
<th>降采样层2</th>
<th>全连接层</th>
<th>输出层</th>
</tr>
</thead>
<tbody><tr>
<td>28x28图像，<br>通道为1（黑白图片）</td>
<td>第一次卷积：<br>输入通道：1<br>输出通道：32<br>卷积后图像尺寸不变，依然是28x28</td>
<td>第一次降采样：<br>将28x28图像缩小为14x14；<br>池化不改变通道数量，因此依然是32个</td>
<td>第二次卷积：<br>输入通道：32<br>输出通道：64<br>卷积后图像尺寸不变依然是14x14</td>
<td>第二次降采样：<br>将14x14图像缩小为7x7；<br>池化不改变通道数量，因此依然是64个</td>
<td>将64个7x7的图像转换成长度是64x7x7&#x3D;3136的一维向量，该层有128个神经元</td>
<td>输出层共有10个神经元，对应0-9这10个类别</td>
</tr>
</tbody></table>
<p>导入数据：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取mnist数据</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">&#x27;MNIST_data/&#x27;</span>,one_hot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>参数设置：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数设置</span></span><br><span class="line">train_epochs = <span class="number">150</span>    <span class="comment"># 训练轮次</span></span><br><span class="line">learning_rate = <span class="number">0.0001</span>    <span class="comment"># 学习率</span></span><br><span class="line">batch_size = <span class="number">50</span>    <span class="comment"># 单次训练样本数（批次大小）</span></span><br><span class="line">total_batch = <span class="built_in">int</span>(mnist.train.num_examples / batch_size)    <span class="comment"># 一轮训练有多少批次</span></span><br><span class="line">display_step = <span class="number">1</span>    <span class="comment"># 显示粒度</span></span><br><span class="line">save_step = <span class="number">1</span>    <span class="comment"># 存储模型的粒度</span></span><br></pre></td></tr></table></figure>

<p>定义共享函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义权值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">weight</span>(<span class="params">shape</span>):</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(tf.truncated_normal(shape,stddev=<span class="number">0.1</span>),name=<span class="string">&#x27;W&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义偏置</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bias</span>(<span class="params">shape</span>):</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(tf.zeros(shape),name=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义卷积操作</span></span><br><span class="line"><span class="comment"># 步长为1,padding为&#x27;SAME&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv2d</span>(<span class="params">x,W</span>):</span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x,W,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义池化操作</span></span><br><span class="line"><span class="comment"># 步长为2,即原尺寸的长和宽各除以2</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">max_pool_2x2</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x,ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>定义网络结构：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入层</span></span><br><span class="line"><span class="comment"># 28x28图像，通道为1</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;input_layer&#x27;</span>):</span><br><span class="line">    x = tf.placeholder(tf.float32,shape=[<span class="literal">None</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>],name=<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 第一个卷积层</span></span><br><span class="line"><span class="comment"># 输入通道：1,输出通道：32,卷积后图像尺寸不变，依然是28x28</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;conv_1&#x27;</span>):</span><br><span class="line">    W1 = weight([<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">32</span>])    <span class="comment"># [k_width,k_height,input_chn,output_chn]</span></span><br><span class="line">    b1 = bias([<span class="number">32</span>])    <span class="comment"># 与output_chn一致</span></span><br><span class="line">    conv_1 = conv2d(x,W1) + b1</span><br><span class="line">    conv_1 = tf.nn.relu(conv_1)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 第一个池化层</span></span><br><span class="line"><span class="comment"># 将28x28图像缩小为14x14,池化不改变通道数量，因此依然是32个</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;pool_1&#x27;</span>):</span><br><span class="line">    pool_1 = max_pool_2x2(conv_1)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 第2个卷积层</span></span><br><span class="line"><span class="comment"># 输入通道：32，输出通道：64，卷积后图像尺寸不变，依然是14x14</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;conv_2&#x27;</span>):</span><br><span class="line">    W2 = weight([<span class="number">3</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">64</span>])</span><br><span class="line">    b2 = bias([<span class="number">64</span>])</span><br><span class="line">    conv_2 = conv2d(pool_1,W2) + b2</span><br><span class="line">    conv_2 = tf.nn.relu(conv_2)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 第2个池化层</span></span><br><span class="line"><span class="comment"># 将14x14图像缩小为7x7，池化不改变通道数量，因此依然是64个</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;pool_2&#x27;</span>):</span><br><span class="line">    pool_2 = max_pool_2x2(conv_2)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 全连接层</span></span><br><span class="line"><span class="comment"># 将第2个池化层的64个7x7的图像转换为一维的向量，长度是64×7×7=3136</span></span><br><span class="line"><span class="comment"># 128个神经元</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;fc&#x27;</span>):</span><br><span class="line">    W3 = weight([<span class="number">3136</span>,<span class="number">128</span>])    <span class="comment"># 有128个神经元</span></span><br><span class="line">    b3 = bias([<span class="number">128</span>])</span><br><span class="line">    <span class="comment"># -1是指未设定行数，程序随机分配，所以这里的含义为(任意行，3136列)</span></span><br><span class="line">    flat = tf.reshape(pool_2,[-<span class="number">1</span>,<span class="number">3136</span>])</span><br><span class="line">    h = tf.nn.relu(tf.matmul(flat,W3) + b3)</span><br><span class="line">    <span class="comment"># 采用dropout</span></span><br><span class="line">    h_dropout = tf.nn.dropout(h,keep_prob=<span class="number">0.8</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 输出层</span></span><br><span class="line"><span class="comment"># 输出层共有10个神经元，对应到0-9这10个类别</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;output_layer&#x27;</span>):</span><br><span class="line">    W4 = weight([<span class="number">128</span>,<span class="number">10</span>])</span><br><span class="line">    b4 = bias([<span class="number">10</span>])</span><br><span class="line">    <span class="comment"># 训练时用pred_dropout，测试时用pred</span></span><br><span class="line">    pred = tf.nn.softmax(tf.matmul(h,W4) + b4)</span><br><span class="line">    pred_dropout = tf.nn.softmax(tf.matmul(h_dropout,W4) + b4)</span><br></pre></td></tr></table></figure>
<p>上面的代码中使用了<strong>dropout</strong>，它是神经网络训练中防止过拟合的一种技术。<br>dropout是一种正则化方法，通过对网络某层的节点都设置一个被消除的概率，之后在训练中按照概率随机将某些节点消除掉，以达到正则化，降低方差的目的。<br>示例如下：<br><img src="/2020/01/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91-tensorflow%E5%AE%9E%E8%B7%B5/chapter7/%E5%9B%BE7-17.png"><br>tensorflow中提供了实现dropout的函数，该函数为**tf.nn.dropout()**。参数如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.dropout(</span><br><span class="line">    x,</span><br><span class="line">    keep_prob,</span><br><span class="line">    noise_shape=<span class="literal">None</span>,</span><br><span class="line">     seed=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>x：指输入，浮点型Tensor。</li>
<li>keep_prob：float类型，每个元素被保留下来的概率。</li>
</ul>
<p>就这两个参数比较常用，其他的就不做介绍了。值得一提的是，<strong>Dropout 层只能在训练中使用，而不能用于测试过程</strong>，这是很重要的一点。</p>
<p>构建模型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;optimizer&#x27;</span>):</span><br><span class="line">    <span class="comment"># 定义占位符</span></span><br><span class="line">    y = tf.placeholder(tf.float32,shape=[<span class="literal">None</span>,<span class="number">10</span>],name=<span class="string">&#x27;labels&#x27;</span>)</span><br><span class="line">    <span class="comment"># 定义使用dropout的损失函数</span></span><br><span class="line">    loss_function_dropout = tf.reduce_mean(</span><br><span class="line">        tf.nn.softmax_cross_entropy_with_logits(logits=pred_dropout,labels=y))</span><br><span class="line">    <span class="comment"># 定义无dropout的损失函数</span></span><br><span class="line">    loss_function = tf.reduce_mean(</span><br><span class="line">        tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y)) </span><br><span class="line">    <span class="comment"># 选择优化器</span></span><br><span class="line">    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss_function_dropout)</span><br></pre></td></tr></table></figure>

<p>定义准确率：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;evaluation&#x27;</span>):</span><br><span class="line">    <span class="comment"># 记录预测正确与否的张量</span></span><br><span class="line">    <span class="comment"># 这里用的是未经dropout的prep</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(pred,axis=<span class="number">1</span>),tf.argmax(y,axis=<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 正确率</span></span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br></pre></td></tr></table></figure>

<p>启动会话：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练轮数（由0开始）</span></span><br><span class="line">epoch = tf.Variable(<span class="number">0</span>,name=<span class="string">&#x27;epoch&#x27;</span>,trainable=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 记录训练开始时间</span></span><br><span class="line">start_time = time()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义会话及变量初始化</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure>

<p>断点续训：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置检查点存储目录</span></span><br><span class="line">ckpt_dir = <span class="string">&#x27;mnist_ckpt/&#x27;</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(ckpt_dir):</span><br><span class="line">    os.makedirs(ckpt_dir)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 生成saver</span></span><br><span class="line"><span class="comment"># max_to_keep表示要保留的最近文件的最大数量，默认为5</span></span><br><span class="line">saver = tf.train.Saver(max_to_keep=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果有检查点文件，读取最新的检查点文件，恢复各种变量值</span></span><br><span class="line">ckpt = tf.train.latest_checkpoint(ckpt_dir)</span><br><span class="line"><span class="keyword">if</span> ckpt != <span class="literal">None</span>:</span><br><span class="line">    saver.restore(sess,ckpt)    <span class="comment"># 加载所有参数</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Training from scratch&#x27;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 获取续训参数</span></span><br><span class="line">start = sess.run(epoch)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Training starts from %d epoch.&#x27;</span> % (start + <span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<p>迭代训练：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 迭代训练</span></span><br><span class="line"><span class="keyword">for</span> ep <span class="keyword">in</span> <span class="built_in">range</span>(start,train_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> <span class="built_in">range</span>(total_batch):</span><br><span class="line">        xs,ys = mnist.train.next_batch(batch_size)</span><br><span class="line">        <span class="comment"># 将xs转变为4维数组</span></span><br><span class="line">        xs = xs.reshape([batch_size,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])</span><br><span class="line">        sess.run(optimizer,feed_dict=&#123;x:xs,y:ys&#125;)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># total_batch个批次训练完成后，使用验证数据计算误差与准确率</span></span><br><span class="line">    <span class="comment"># 别忘了将喂给x的数据reshape</span></span><br><span class="line">    loss,acc = sess.run([loss_function,accuracy],</span><br><span class="line">                       feed_dict=&#123;x:mnist.validation.images.reshape([mnist.validation.num_examples,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>]),</span><br><span class="line">                                  y:mnist.validation.labels&#125;)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新epoch变量的值</span></span><br><span class="line">    sess.run(epoch.assign(ep + <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (ep + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># 打印损失值与准确率</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Train Epoch: %02d,  Loss= %.9f,  Accurary= %.4f&#x27;</span></span><br><span class="line">             % (ep + <span class="number">1</span>,loss,acc))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> (ep + <span class="number">1</span>) % save_step == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># 训练过程中存储模型</span></span><br><span class="line">        saver.save(sess,os.path.join(ckpt_dir,<span class="string">&#x27;mnist_model_&#123;:06d&#125;&#x27;</span>.<span class="built_in">format</span>(ep + <span class="number">1</span>)))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;mnist_model_&#123;:06d&#125; saved&#x27;</span>.<span class="built_in">format</span>(ep + <span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 显示运行总时间</span></span><br><span class="line">duration = time() - start_time</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Train Finished takes: %.2f&#x27;</span> % (duration))</span><br></pre></td></tr></table></figure>

<p>计算测试集上的准确率：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试集上的正确率</span></span><br><span class="line">acc_test = sess.run(accuracy,</span><br><span class="line">                    feed_dict=&#123;x:mnist.test.images.reshape([mnist.test.num_examples,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>]),</span><br><span class="line">                               y:mnist.test.labels&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test accuracy: %.4f&#x27;</span> % acc_test)</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/tensorflow/" rel="tag"># tensorflow</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/01/03/%E6%95%B0%E5%AD%97%E7%94%B5%E8%B7%AF/%E6%95%B0%E5%AD%97%E7%94%B5%E8%B7%AF%E4%B8%8E%E9%80%BB%E8%BE%91%E8%AE%BE%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="prev" title="数字电路与逻辑设计学习笔记">
      <i class="fa fa-chevron-left"></i> 数字电路与逻辑设计学习笔记
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/02/05/%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A/ccf%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A/" rel="next" title="ccf解题报告">
      ccf解题报告 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E7%AB%A0-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B%E5%8F%8A%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA"><span class="nav-text">第一章 深度学习简介及开发环境搭建</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="nav-text">1.1 人工智能与机器学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B"><span class="nav-text">1.2 机器学习分类与算法简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="nav-text">1.3 神经网络与深度学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-Anaconda%E5%92%8CTensorFlow%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA"><span class="nav-text">1.4 Anaconda和TensorFlow开发环境搭建</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0-TensorFlow%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80"><span class="nav-text">第二章 TensorFlow编程基础</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-TensorFlow%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-text">2.1 TensorFlow的基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-1-TensorFlow%E7%9A%84%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B-%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="nav-text">2.1.1 TensorFlow的计算模型:计算图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-2-%E5%BC%A0%E9%87%8F"><span class="nav-text">2.1.2 张量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-3-%E6%93%8D%E4%BD%9C"><span class="nav-text">2.2.3 操作</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-TensorFlow%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%9A%84%E8%BF%90%E7%AE%97"><span class="nav-text">2.2 TensorFlow的基本的运算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-%E4%BC%9A%E8%AF%9D"><span class="nav-text">2.2.1 会话</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-%E5%B8%B8%E9%87%8F%E5%92%8C%E5%8F%98%E9%87%8F"><span class="nav-text">2.2.2 常量和变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-3-%E5%8F%98%E9%87%8F%E7%9A%84%E8%B5%8B%E5%80%BC"><span class="nav-text">2.2.3 变量的赋值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-4-%E5%8D%A0%E4%BD%8D%E7%AC%A6%E3%80%81Feed%E6%95%B0%E6%8D%AE%E5%A1%AB%E5%85%85%E5%92%8CFetch%E6%95%B0%E6%8D%AE%E8%8E%B7%E5%8F%96"><span class="nav-text">2.2.4 占位符、Feed数据填充和Fetch数据获取</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%E5%88%9D%E6%AD%A5"><span class="nav-text">2.3 TensorBoard可视化初步</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0-%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-TensorFlow%E5%AE%9E%E6%88%98"><span class="nav-text">第三章 单变量线性回归:TensorFlow实战</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-%E7%9B%91%E7%9D%A3%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%9C%AF%E8%AF%AD"><span class="nav-text">3.1 监督式机器学习的基本术语</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-1-%E6%A0%B7%E6%9C%AC%E3%80%81%E7%89%B9%E5%BE%81%E3%80%81%E6%A0%87%E7%AD%BE%E4%B8%8E%E6%A8%A1%E5%9E%8B"><span class="nav-text">3.1.1 样本、特征、标签与模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-2-%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%8D%9F%E5%A4%B1"><span class="nav-text">3.1.2 训练与损失</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-3-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%99%8D%E4%BD%8E%E6%8D%9F%E5%A4%B1"><span class="nav-text">3.1.3 模型训练与降低损失</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-4-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E4%B8%8E%E8%B6%85%E5%8F%82%E6%95%B0"><span class="nav-text">3.1.4 梯度下降法与超参数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98TensorFlow%E5%AE%9E%E6%88%98"><span class="nav-text">3.2 线性回归问题TensorFlow实战</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1-%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4"><span class="nav-text">3.2.1 基本步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-%E6%98%BE%E7%A4%BA%E6%8D%9F%E5%A4%B1Loss"><span class="nav-text">3.2.2 显示损失Loss</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%9A%E6%B3%A2%E5%A3%AB%E9%A1%BF%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%97%AE%E9%A2%98"><span class="nav-text">第四章 多元线性回归：波士顿房价预测问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%9F%BA%E7%A1%80"><span class="nav-text">4.1 机器学习中的线性代数基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-1-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%AF%B9%E8%B1%A1"><span class="nav-text">4.1.1 线性代数的数学对象</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-2-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%AE%97%E8%A7%84%E5%88%99"><span class="nav-text">4.1.2 线性代数基本运算规则</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-%E6%95%B0%E6%8D%AE%E4%B8%8E%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90"><span class="nav-text">4.2 数据与问题分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-1-%E6%B3%A2%E5%A3%AB%E9%A1%BF%E6%88%BF%E4%BB%B7%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0"><span class="nav-text">4.2.1 波士顿房价问题描述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-2-%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6%E8%AF%BB%E5%8F%96"><span class="nav-text">4.2.2 数据文件读取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-3-%E5%87%86%E5%A4%87%E5%BB%BA%E6%A8%A1"><span class="nav-text">4.2.3 准备建模</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%89%88%E6%9C%AC%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA"><span class="nav-text">4.3 第一个版本的模型构建</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-1-%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89"><span class="nav-text">4.3.1 数据准备与模型定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-2-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="nav-text">4.3.2 模型训练</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-4-%E5%90%8E%E7%BB%AD%E7%89%88%E6%9C%AC%E7%9A%84%E6%8C%81%E7%BB%AD%E6%94%B9%E8%BF%9B"><span class="nav-text">4.4 后续版本的持续改进</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-1-%E7%89%88%E6%9C%AC2%EF%BC%9A%E7%89%B9%E5%BE%81%E6%95%B0%E6%8D%AE%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-text">4.4.1 版本2：特征数据归一化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-2-%E7%89%88%E6%9C%AC2%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%80%BC"><span class="nav-text">4.4.2 版本2：可视化训练过程中的损失值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-3-%E7%89%88%E6%9C%AC4%EF%BC%9A%E5%8A%A0%E4%B8%8ATensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%E4%BB%A3%E7%A0%81"><span class="nav-text">4.4.3 版本4：加上TensorBoard可视化代码</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%BA%94%E7%AB%A0-MNIST%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%EF%BC%9A%E5%88%86%E7%B1%BB%E5%BA%94%E7%94%A8%E5%85%A5%E9%97%A8"><span class="nav-text">第五章 MNIST手写数字识别：分类应用入门</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-MNIST%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%E6%95%B0%E6%8D%AE%E8%A7%A3%E8%AF%BB"><span class="nav-text">5.1 MNIST手写数字识别数据解读</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-1-%E6%95%B0%E6%8D%AE%E9%9B%86%E7%AE%80%E4%BB%8B"><span class="nav-text">5.1.1 数据集简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-2-%E6%95%B0%E6%8D%AE%E6%A0%87%E7%AD%BE%E4%B8%8E%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81"><span class="nav-text">5.1.2 数据标签与独热编码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-3-%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%88%92%E5%88%86"><span class="nav-text">5.1.3 数据集的划分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-4-%E6%95%B0%E6%8D%AE%E7%9A%84%E6%89%B9%E9%87%8F%E8%AF%BB%E5%8F%96"><span class="nav-text">5.1.4 数据的批量读取</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA%E4%B8%8E%E8%AE%AD%E7%BB%83"><span class="nav-text">5.2 分类模型构建与训练</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-1-%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA"><span class="nav-text">5.2.1 模型构建</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-2-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="nav-text">5.2.2 逻辑回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-3-%E5%A4%9A%E5%85%83%E5%88%86%E7%B1%BB%E5%92%8CSoftmax"><span class="nav-text">5.2.3 多元分类和Softmax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-4-%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA%E4%B8%8E%E8%AE%AD%E7%BB%83%E5%AE%9E%E8%B7%B5"><span class="nav-text">5.2.4 分类模型构建与训练实践</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-5-%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="nav-text">5.2.5 预测结果的可视化</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E5%85%AD%E7%AB%A0-MNIST%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%E8%BF%9B%E9%98%B6%EF%BC%9A%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E5%BA%94%E7%94%A8"><span class="nav-text">第六章 MNIST手写数字识别进阶：多层神经网络与应用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#6-1-%E5%8D%95%E9%9A%90%E8%97%8F%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%84%E5%BB%BA%E4%B8%8E%E5%BA%94%E7%94%A8"><span class="nav-text">6.1 单隐藏层神经网络构建与应用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-1-%E4%BB%8E%E5%8D%95%E7%A5%9E%E7%BB%8F%E5%85%83%E5%88%B0%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-text">6.1.1 从单神经元到全连接神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-2-%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8"><span class="nav-text">6.1.2 模型应用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-2-%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E8%BF%98%E5%8E%9F"><span class="nav-text">6.2 多层神经网络建模与模型的保存还原</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-1-2%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9E%84%E5%BB%BA"><span class="nav-text">6.2.1 2层神经网络的构建</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-2-3%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9E%84%E5%BB%BA"><span class="nav-text">6.2.2 3层神经网络的构建</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-3-%E6%A8%A1%E5%9E%8B%E9%87%8D%E6%9E%84"><span class="nav-text">6.2.3 模型重构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-4-%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98"><span class="nav-text">6.2.4 模型保存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-5-%E6%A8%A1%E5%9E%8B%E8%BF%98%E5%8E%9F"><span class="nav-text">6.2.5 模型还原</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%B8%83%E7%AB%A0-%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB%E9%97%AE%E9%A2%98%EF%BC%9A%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8"><span class="nav-text">第七章 图像识别问题：卷积神经网络及其应用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#7-1-%E4%BB%8E%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9C%E5%88%B0%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-text">7.1 从全连接网络到卷积神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-2-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%8B%E7%BB%8D"><span class="nav-text">7.2 卷积神经网络的介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-1-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0"><span class="nav-text">7.2.1 卷积神经网络概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-2-%E5%8D%B7%E7%A7%AF"><span class="nav-text">7.2.2 卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-3-%E6%B1%A0%E5%8C%96"><span class="nav-text">7.2.3 池化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-4-TensorFlow%E4%B8%ADCNN%E7%9A%84%E7%9B%B8%E5%85%B3%E5%87%BD%E6%95%B0"><span class="nav-text">7.2.4 TensorFlow中CNN的相关函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-3-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0MNIST%E6%89%8B%E5%86%99%E6%95%B0%E6%8D%AE%E8%AF%86%E5%88%AB"><span class="nav-text">7.3 卷积神经网络实现MNIST手写数据识别</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="李澳"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">李澳</p>
  <div class="site-description" itemprop="description">自娱自乐</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">31</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">32</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">李澳</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">199k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">11:02</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
